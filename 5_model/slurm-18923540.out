fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 16s - loss: 0.3184 - acc: 0.8645 - auc: 0.9365 - val_loss: 0.2978 - val_acc: 0.8776 - val_auc: 0.9447 - 16s/epoch - 55ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2878 - acc: 0.8790 - auc: 0.9474 - val_loss: 0.2765 - val_acc: 0.8844 - val_auc: 0.9505 - 14s/epoch - 47ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2712 - acc: 0.8917 - auc: 0.9527 - val_loss: 0.2586 - val_acc: 0.8979 - val_auc: 0.9554 - 14s/epoch - 48ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2691 - acc: 0.8896 - auc: 0.9534 - val_loss: 0.2512 - val_acc: 0.8988 - val_auc: 0.9582 - 14s/epoch - 48ms/step
Epoch 5/100
292/292 - 14s - loss: 0.2631 - acc: 0.8943 - auc: 0.9557 - val_loss: 0.2713 - val_acc: 0.8911 - val_auc: 0.9513 - 14s/epoch - 48ms/step
Epoch 6/100
292/292 - 14s - loss: 0.2595 - acc: 0.8974 - auc: 0.9577 - val_loss: 0.2529 - val_acc: 0.9008 - val_auc: 0.9582 - 14s/epoch - 49ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2551 - acc: 0.8977 - auc: 0.9592 - val_loss: 0.2616 - val_acc: 0.8960 - val_auc: 0.9587 - 14s/epoch - 48ms/step
Epoch 8/100
292/292 - 14s - loss: 0.2562 - acc: 0.8986 - auc: 0.9589 - val_loss: 0.2464 - val_acc: 0.8988 - val_auc: 0.9611 - 14s/epoch - 49ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2535 - acc: 0.8964 - auc: 0.9597 - val_loss: 0.2524 - val_acc: 0.8988 - val_auc: 0.9587 - 14s/epoch - 48ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2527 - acc: 0.8955 - auc: 0.9608 - val_loss: 0.2492 - val_acc: 0.8940 - val_auc: 0.9598 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2500 - acc: 0.8989 - auc: 0.9607 - val_loss: 0.2521 - val_acc: 0.9046 - val_auc: 0.9590 - 14s/epoch - 48ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2471 - acc: 0.8992 - auc: 0.9623 - val_loss: 0.2456 - val_acc: 0.9046 - val_auc: 0.9602 - 14s/epoch - 48ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2475 - acc: 0.8991 - auc: 0.9621 - val_loss: 0.2405 - val_acc: 0.9027 - val_auc: 0.9614 - 14s/epoch - 48ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2456 - acc: 0.9009 - auc: 0.9626 - val_loss: 0.2566 - val_acc: 0.9008 - val_auc: 0.9583 - 14s/epoch - 48ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2424 - acc: 0.9017 - auc: 0.9636 - val_loss: 0.2411 - val_acc: 0.9094 - val_auc: 0.9615 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2408 - acc: 0.9044 - auc: 0.9637 - val_loss: 0.2582 - val_acc: 0.9008 - val_auc: 0.9581 - 14s/epoch - 48ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2408 - acc: 0.9024 - auc: 0.9637 - val_loss: 0.2446 - val_acc: 0.9066 - val_auc: 0.9606 - 14s/epoch - 48ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2371 - acc: 0.9021 - auc: 0.9646 - val_loss: 0.2402 - val_acc: 0.9085 - val_auc: 0.9634 - 14s/epoch - 49ms/step
Epoch 19/100
292/292 - 14s - loss: 0.2333 - acc: 0.9067 - auc: 0.9663 - val_loss: 0.2392 - val_acc: 0.9056 - val_auc: 0.9626 - 14s/epoch - 49ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2337 - acc: 0.9061 - auc: 0.9662 - val_loss: 0.2331 - val_acc: 0.9114 - val_auc: 0.9645 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2296 - acc: 0.9070 - auc: 0.9676 - val_loss: 0.2371 - val_acc: 0.9114 - val_auc: 0.9609 - 14s/epoch - 48ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2286 - acc: 0.9074 - auc: 0.9671 - val_loss: 0.2431 - val_acc: 0.8998 - val_auc: 0.9630 - 14s/epoch - 48ms/step
Epoch 23/100
292/292 - 14s - loss: 0.2249 - acc: 0.9064 - auc: 0.9690 - val_loss: 0.2369 - val_acc: 0.9123 - val_auc: 0.9635 - 14s/epoch - 49ms/step
Epoch 24/100
292/292 - 14s - loss: 0.2234 - acc: 0.9096 - auc: 0.9691 - val_loss: 0.2339 - val_acc: 0.9123 - val_auc: 0.9651 - 14s/epoch - 49ms/step
Epoch 25/100
292/292 - 14s - loss: 0.2194 - acc: 0.9108 - auc: 0.9702 - val_loss: 0.2458 - val_acc: 0.9085 - val_auc: 0.9604 - 14s/epoch - 49ms/step
Epoch 26/100
292/292 - 14s - loss: 0.2160 - acc: 0.9141 - auc: 0.9714 - val_loss: 0.2506 - val_acc: 0.9046 - val_auc: 0.9618 - 14s/epoch - 48ms/step
Epoch 27/100
292/292 - 14s - loss: 0.2132 - acc: 0.9135 - auc: 0.9720 - val_loss: 0.2369 - val_acc: 0.9075 - val_auc: 0.9643 - 14s/epoch - 49ms/step
Epoch 28/100
292/292 - 14s - loss: 0.2098 - acc: 0.9153 - auc: 0.9729 - val_loss: 0.2465 - val_acc: 0.9066 - val_auc: 0.9616 - 14s/epoch - 49ms/step
Epoch 29/100
292/292 - 14s - loss: 0.2020 - acc: 0.9171 - auc: 0.9752 - val_loss: 0.2486 - val_acc: 0.9027 - val_auc: 0.9621 - 14s/epoch - 48ms/step
Epoch 30/100
292/292 - 14s - loss: 0.1980 - acc: 0.9211 - auc: 0.9759 - val_loss: 0.2490 - val_acc: 0.9056 - val_auc: 0.9605 - 14s/epoch - 48ms/step
Epoch 31/100
292/292 - 14s - loss: 0.1970 - acc: 0.9200 - auc: 0.9762 - val_loss: 0.2512 - val_acc: 0.9094 - val_auc: 0.9592 - 14s/epoch - 49ms/step
Epoch 32/100
292/292 - 14s - loss: 0.1875 - acc: 0.9251 - auc: 0.9786 - val_loss: 0.2602 - val_acc: 0.9143 - val_auc: 0.9586 - 14s/epoch - 49ms/step
Epoch 33/100
292/292 - 14s - loss: 0.1832 - acc: 0.9263 - auc: 0.9789 - val_loss: 0.2748 - val_acc: 0.8969 - val_auc: 0.9528 - 14s/epoch - 48ms/step
Epoch 34/100
292/292 - 14s - loss: 0.1791 - acc: 0.9275 - auc: 0.9802 - val_loss: 0.2597 - val_acc: 0.8950 - val_auc: 0.9585 - 14s/epoch - 48ms/step
Early stopping epoch: 33
******Evaluating TEST set*********
33/33 - 1s - 702ms/epoch - 21ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.88       403
           1       0.91      0.95      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.91      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.45      0.39       403
           1       0.58      0.48      0.52       635

    accuracy                           0.47      1038
   macro avg       0.46      0.46      0.46      1038
weighted avg       0.49      0.47      0.47      1038

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 16s - loss: 0.3096 - acc: 0.8682 - auc: 0.9392 - val_loss: 0.3018 - val_acc: 0.8757 - val_auc: 0.9441 - 16s/epoch - 56ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2818 - acc: 0.8868 - auc: 0.9484 - val_loss: 0.2960 - val_acc: 0.8805 - val_auc: 0.9464 - 14s/epoch - 49ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2683 - acc: 0.8919 - auc: 0.9529 - val_loss: 0.2811 - val_acc: 0.8873 - val_auc: 0.9523 - 14s/epoch - 49ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2613 - acc: 0.8955 - auc: 0.9557 - val_loss: 0.2909 - val_acc: 0.8748 - val_auc: 0.9504 - 14s/epoch - 49ms/step
Epoch 5/100
292/292 - 14s - loss: 0.2586 - acc: 0.8955 - auc: 0.9570 - val_loss: 0.2783 - val_acc: 0.8892 - val_auc: 0.9546 - 14s/epoch - 49ms/step
Epoch 6/100
292/292 - 14s - loss: 0.2556 - acc: 0.8973 - auc: 0.9585 - val_loss: 0.2827 - val_acc: 0.8873 - val_auc: 0.9536 - 14s/epoch - 50ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2527 - acc: 0.8965 - auc: 0.9594 - val_loss: 0.2966 - val_acc: 0.8844 - val_auc: 0.9502 - 14s/epoch - 49ms/step
Epoch 8/100
292/292 - 14s - loss: 0.2535 - acc: 0.8988 - auc: 0.9590 - val_loss: 0.2772 - val_acc: 0.8834 - val_auc: 0.9556 - 14s/epoch - 49ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2489 - acc: 0.9013 - auc: 0.9610 - val_loss: 0.2734 - val_acc: 0.8902 - val_auc: 0.9562 - 14s/epoch - 49ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2455 - acc: 0.9015 - auc: 0.9617 - val_loss: 0.2854 - val_acc: 0.8776 - val_auc: 0.9532 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2450 - acc: 0.9033 - auc: 0.9614 - val_loss: 0.2748 - val_acc: 0.8796 - val_auc: 0.9554 - 14s/epoch - 49ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2426 - acc: 0.9063 - auc: 0.9627 - val_loss: 0.2959 - val_acc: 0.8786 - val_auc: 0.9499 - 14s/epoch - 48ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2426 - acc: 0.9004 - auc: 0.9630 - val_loss: 0.2808 - val_acc: 0.8834 - val_auc: 0.9537 - 14s/epoch - 49ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2375 - acc: 0.9045 - auc: 0.9643 - val_loss: 0.2685 - val_acc: 0.8902 - val_auc: 0.9584 - 14s/epoch - 49ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2367 - acc: 0.9065 - auc: 0.9646 - val_loss: 0.2687 - val_acc: 0.8834 - val_auc: 0.9573 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2336 - acc: 0.9047 - auc: 0.9658 - val_loss: 0.2739 - val_acc: 0.8892 - val_auc: 0.9560 - 14s/epoch - 49ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2332 - acc: 0.9053 - auc: 0.9665 - val_loss: 0.2688 - val_acc: 0.8921 - val_auc: 0.9585 - 14s/epoch - 49ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2307 - acc: 0.9071 - auc: 0.9665 - val_loss: 0.2720 - val_acc: 0.8873 - val_auc: 0.9568 - 14s/epoch - 49ms/step
Epoch 19/100
292/292 - 14s - loss: 0.2267 - acc: 0.9073 - auc: 0.9681 - val_loss: 0.2713 - val_acc: 0.8854 - val_auc: 0.9570 - 14s/epoch - 49ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2241 - acc: 0.9105 - auc: 0.9686 - val_loss: 0.2793 - val_acc: 0.8854 - val_auc: 0.9546 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2212 - acc: 0.9119 - auc: 0.9689 - val_loss: 0.2867 - val_acc: 0.8786 - val_auc: 0.9538 - 14s/epoch - 49ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2188 - acc: 0.9137 - auc: 0.9705 - val_loss: 0.2709 - val_acc: 0.8931 - val_auc: 0.9566 - 14s/epoch - 49ms/step
Epoch 23/100
292/292 - 14s - loss: 0.2145 - acc: 0.9140 - auc: 0.9716 - val_loss: 0.2697 - val_acc: 0.8873 - val_auc: 0.9570 - 14s/epoch - 49ms/step
Epoch 24/100
292/292 - 14s - loss: 0.2093 - acc: 0.9159 - auc: 0.9726 - val_loss: 0.2792 - val_acc: 0.8854 - val_auc: 0.9578 - 14s/epoch - 49ms/step
Epoch 25/100
292/292 - 14s - loss: 0.2050 - acc: 0.9210 - auc: 0.9742 - val_loss: 0.2986 - val_acc: 0.8815 - val_auc: 0.9493 - 14s/epoch - 49ms/step
Epoch 26/100
292/292 - 14s - loss: 0.2021 - acc: 0.9222 - auc: 0.9743 - val_loss: 0.2983 - val_acc: 0.8815 - val_auc: 0.9493 - 14s/epoch - 49ms/step
Epoch 27/100
292/292 - 14s - loss: 0.1969 - acc: 0.9225 - auc: 0.9755 - val_loss: 0.2676 - val_acc: 0.8979 - val_auc: 0.9571 - 14s/epoch - 49ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
33/33 - 1s - 722ms/epoch - 22ms/step
              precision    recall  f1-score   support

           0       0.89      0.83      0.86       403
           1       0.89      0.93      0.91       635

    accuracy                           0.89      1038
   macro avg       0.89      0.88      0.88      1038
weighted avg       0.89      0.89      0.89      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.51      0.43       403
           1       0.60      0.46      0.52       635

    accuracy                           0.48      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.48      0.49      1038

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3209 - acc: 0.8599 - auc: 0.9360 - val_loss: 0.2777 - val_acc: 0.8931 - val_auc: 0.9493 - 17s/epoch - 57ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2820 - acc: 0.8852 - auc: 0.9494 - val_loss: 0.2668 - val_acc: 0.8998 - val_auc: 0.9539 - 14s/epoch - 49ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2733 - acc: 0.8897 - auc: 0.9527 - val_loss: 0.2553 - val_acc: 0.9027 - val_auc: 0.9576 - 14s/epoch - 49ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2662 - acc: 0.8935 - auc: 0.9548 - val_loss: 0.2515 - val_acc: 0.9017 - val_auc: 0.9584 - 14s/epoch - 49ms/step
Epoch 5/100
292/292 - 14s - loss: 0.2639 - acc: 0.8969 - auc: 0.9553 - val_loss: 0.2572 - val_acc: 0.9008 - val_auc: 0.9584 - 14s/epoch - 49ms/step
Epoch 6/100
292/292 - 15s - loss: 0.2600 - acc: 0.8980 - auc: 0.9573 - val_loss: 0.2548 - val_acc: 0.9066 - val_auc: 0.9571 - 15s/epoch - 50ms/step
Epoch 7/100
292/292 - 15s - loss: 0.2619 - acc: 0.8970 - auc: 0.9564 - val_loss: 0.2482 - val_acc: 0.9056 - val_auc: 0.9601 - 15s/epoch - 50ms/step
Epoch 8/100
292/292 - 15s - loss: 0.2587 - acc: 0.8979 - auc: 0.9578 - val_loss: 0.2482 - val_acc: 0.9017 - val_auc: 0.9602 - 15s/epoch - 50ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2576 - acc: 0.8940 - auc: 0.9578 - val_loss: 0.2529 - val_acc: 0.9085 - val_auc: 0.9596 - 14s/epoch - 49ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2542 - acc: 0.8971 - auc: 0.9594 - val_loss: 0.2505 - val_acc: 0.9046 - val_auc: 0.9593 - 14s/epoch - 50ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2515 - acc: 0.8963 - auc: 0.9603 - val_loss: 0.2501 - val_acc: 0.9046 - val_auc: 0.9599 - 14s/epoch - 49ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2523 - acc: 0.8971 - auc: 0.9603 - val_loss: 0.2381 - val_acc: 0.9037 - val_auc: 0.9632 - 14s/epoch - 50ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2470 - acc: 0.8986 - auc: 0.9617 - val_loss: 0.2450 - val_acc: 0.9027 - val_auc: 0.9607 - 14s/epoch - 49ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2436 - acc: 0.9004 - auc: 0.9632 - val_loss: 0.2413 - val_acc: 0.9056 - val_auc: 0.9629 - 14s/epoch - 50ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2428 - acc: 0.9021 - auc: 0.9632 - val_loss: 0.2533 - val_acc: 0.9075 - val_auc: 0.9590 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2421 - acc: 0.8986 - auc: 0.9636 - val_loss: 0.2488 - val_acc: 0.9085 - val_auc: 0.9616 - 14s/epoch - 50ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2385 - acc: 0.9015 - auc: 0.9650 - val_loss: 0.2529 - val_acc: 0.9037 - val_auc: 0.9590 - 14s/epoch - 50ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2349 - acc: 0.9038 - auc: 0.9654 - val_loss: 0.2481 - val_acc: 0.9094 - val_auc: 0.9573 - 14s/epoch - 49ms/step
Epoch 19/100
292/292 - 14s - loss: 0.2328 - acc: 0.9047 - auc: 0.9668 - val_loss: 0.2446 - val_acc: 0.9104 - val_auc: 0.9601 - 14s/epoch - 49ms/step
Epoch 20/100
292/292 - 15s - loss: 0.2313 - acc: 0.9064 - auc: 0.9669 - val_loss: 0.2405 - val_acc: 0.9056 - val_auc: 0.9626 - 15s/epoch - 50ms/step
Epoch 21/100
292/292 - 15s - loss: 0.2277 - acc: 0.9090 - auc: 0.9678 - val_loss: 0.2450 - val_acc: 0.9114 - val_auc: 0.9601 - 15s/epoch - 50ms/step
Epoch 22/100
292/292 - 15s - loss: 0.2243 - acc: 0.9094 - auc: 0.9686 - val_loss: 0.2398 - val_acc: 0.9094 - val_auc: 0.9637 - 15s/epoch - 50ms/step
Epoch 23/100
292/292 - 15s - loss: 0.2201 - acc: 0.9129 - auc: 0.9699 - val_loss: 0.2417 - val_acc: 0.9056 - val_auc: 0.9632 - 15s/epoch - 50ms/step
Epoch 24/100
292/292 - 14s - loss: 0.2155 - acc: 0.9111 - auc: 0.9715 - val_loss: 0.2460 - val_acc: 0.9075 - val_auc: 0.9616 - 14s/epoch - 50ms/step
Epoch 25/100
292/292 - 15s - loss: 0.2149 - acc: 0.9132 - auc: 0.9710 - val_loss: 0.2493 - val_acc: 0.9027 - val_auc: 0.9605 - 15s/epoch - 50ms/step
Epoch 26/100
292/292 - 15s - loss: 0.2071 - acc: 0.9172 - auc: 0.9732 - val_loss: 0.2509 - val_acc: 0.9017 - val_auc: 0.9620 - 15s/epoch - 50ms/step
Epoch 27/100
292/292 - 15s - loss: 0.2047 - acc: 0.9205 - auc: 0.9738 - val_loss: 0.2604 - val_acc: 0.9027 - val_auc: 0.9586 - 15s/epoch - 50ms/step
Epoch 28/100
292/292 - 15s - loss: 0.2016 - acc: 0.9189 - auc: 0.9750 - val_loss: 0.2563 - val_acc: 0.9046 - val_auc: 0.9614 - 15s/epoch - 50ms/step
Epoch 29/100
292/292 - 15s - loss: 0.1945 - acc: 0.9200 - auc: 0.9762 - val_loss: 0.2737 - val_acc: 0.8960 - val_auc: 0.9537 - 15s/epoch - 50ms/step
Epoch 30/100
292/292 - 15s - loss: 0.1897 - acc: 0.9262 - auc: 0.9778 - val_loss: 0.2618 - val_acc: 0.9085 - val_auc: 0.9595 - 15s/epoch - 50ms/step
Epoch 31/100
292/292 - 14s - loss: 0.1849 - acc: 0.9285 - auc: 0.9785 - val_loss: 0.2708 - val_acc: 0.8979 - val_auc: 0.9566 - 14s/epoch - 50ms/step
Epoch 32/100
292/292 - 14s - loss: 0.1768 - acc: 0.9294 - auc: 0.9812 - val_loss: 0.2805 - val_acc: 0.9066 - val_auc: 0.9527 - 14s/epoch - 49ms/step
Early stopping epoch: 31
******Evaluating TEST set*********
33/33 - 1s - 717ms/epoch - 22ms/step
              precision    recall  f1-score   support

           0       0.91      0.85      0.88       403
           1       0.91      0.95      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.60      0.49      0.54       635

    accuracy                           0.49      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.49      0.49      1038

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3227 - acc: 0.8610 - auc: 0.9342 - val_loss: 0.2570 - val_acc: 0.9008 - val_auc: 0.9581 - 17s/epoch - 57ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2871 - acc: 0.8840 - auc: 0.9470 - val_loss: 0.2568 - val_acc: 0.8931 - val_auc: 0.9607 - 14s/epoch - 50ms/step
Epoch 3/100
292/292 - 15s - loss: 0.2790 - acc: 0.8881 - auc: 0.9498 - val_loss: 0.2489 - val_acc: 0.9017 - val_auc: 0.9628 - 15s/epoch - 50ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2714 - acc: 0.8900 - auc: 0.9527 - val_loss: 0.2600 - val_acc: 0.8854 - val_auc: 0.9627 - 14s/epoch - 49ms/step
Epoch 5/100
292/292 - 15s - loss: 0.2652 - acc: 0.8932 - auc: 0.9552 - val_loss: 0.2264 - val_acc: 0.9114 - val_auc: 0.9702 - 15s/epoch - 50ms/step
Epoch 6/100
292/292 - 14s - loss: 0.2691 - acc: 0.8902 - auc: 0.9544 - val_loss: 0.2271 - val_acc: 0.9066 - val_auc: 0.9677 - 14s/epoch - 49ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2608 - acc: 0.8966 - auc: 0.9567 - val_loss: 0.2222 - val_acc: 0.9104 - val_auc: 0.9712 - 14s/epoch - 49ms/step
Epoch 8/100
292/292 - 14s - loss: 0.2586 - acc: 0.8956 - auc: 0.9577 - val_loss: 0.2266 - val_acc: 0.9123 - val_auc: 0.9694 - 14s/epoch - 49ms/step
Epoch 9/100
292/292 - 15s - loss: 0.2589 - acc: 0.8939 - auc: 0.9579 - val_loss: 0.2243 - val_acc: 0.9085 - val_auc: 0.9708 - 15s/epoch - 50ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2570 - acc: 0.8993 - auc: 0.9580 - val_loss: 0.2385 - val_acc: 0.8960 - val_auc: 0.9682 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 15s - loss: 0.2530 - acc: 0.8983 - auc: 0.9594 - val_loss: 0.2236 - val_acc: 0.9133 - val_auc: 0.9712 - 15s/epoch - 50ms/step
Epoch 12/100
292/292 - 15s - loss: 0.2514 - acc: 0.8983 - auc: 0.9606 - val_loss: 0.2195 - val_acc: 0.9094 - val_auc: 0.9729 - 15s/epoch - 50ms/step
Epoch 13/100
292/292 - 15s - loss: 0.2477 - acc: 0.8987 - auc: 0.9615 - val_loss: 0.2241 - val_acc: 0.9114 - val_auc: 0.9718 - 15s/epoch - 50ms/step
Epoch 14/100
292/292 - 15s - loss: 0.2502 - acc: 0.8986 - auc: 0.9610 - val_loss: 0.2170 - val_acc: 0.9037 - val_auc: 0.9727 - 15s/epoch - 50ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2467 - acc: 0.9003 - auc: 0.9621 - val_loss: 0.2174 - val_acc: 0.9075 - val_auc: 0.9734 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2407 - acc: 0.9021 - auc: 0.9636 - val_loss: 0.2387 - val_acc: 0.8969 - val_auc: 0.9679 - 14s/epoch - 49ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2412 - acc: 0.9003 - auc: 0.9641 - val_loss: 0.2172 - val_acc: 0.9056 - val_auc: 0.9725 - 14s/epoch - 49ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2373 - acc: 0.9024 - auc: 0.9649 - val_loss: 0.2235 - val_acc: 0.9085 - val_auc: 0.9706 - 14s/epoch - 50ms/step
Epoch 19/100
292/292 - 15s - loss: 0.2350 - acc: 0.9047 - auc: 0.9657 - val_loss: 0.2205 - val_acc: 0.9046 - val_auc: 0.9713 - 15s/epoch - 50ms/step
Epoch 20/100
292/292 - 15s - loss: 0.2319 - acc: 0.9066 - auc: 0.9668 - val_loss: 0.2158 - val_acc: 0.9056 - val_auc: 0.9721 - 15s/epoch - 50ms/step
Epoch 21/100
292/292 - 15s - loss: 0.2284 - acc: 0.9067 - auc: 0.9675 - val_loss: 0.2205 - val_acc: 0.9075 - val_auc: 0.9715 - 15s/epoch - 50ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2249 - acc: 0.9112 - auc: 0.9681 - val_loss: 0.2486 - val_acc: 0.9017 - val_auc: 0.9647 - 14s/epoch - 49ms/step
Epoch 23/100
292/292 - 15s - loss: 0.2250 - acc: 0.9097 - auc: 0.9686 - val_loss: 0.2164 - val_acc: 0.9114 - val_auc: 0.9710 - 15s/epoch - 50ms/step
Epoch 24/100
292/292 - 14s - loss: 0.2187 - acc: 0.9139 - auc: 0.9704 - val_loss: 0.2368 - val_acc: 0.9037 - val_auc: 0.9661 - 14s/epoch - 49ms/step
Epoch 25/100
292/292 - 14s - loss: 0.2153 - acc: 0.9122 - auc: 0.9713 - val_loss: 0.2181 - val_acc: 0.9123 - val_auc: 0.9721 - 14s/epoch - 49ms/step
Early stopping epoch: 24
******Evaluating TEST set*********
33/33 - 1s - 702ms/epoch - 21ms/step
              precision    recall  f1-score   support

           0       0.92      0.83      0.87       403
           1       0.90      0.96      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.89      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.46      0.42       403
           1       0.60      0.52      0.56       635

    accuracy                           0.50      1038
   macro avg       0.49      0.49      0.49      1038
weighted avg       0.52      0.50      0.50      1038

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3182 - acc: 0.8641 - auc: 0.9364 - val_loss: 0.2713 - val_acc: 0.8863 - val_auc: 0.9539 - 17s/epoch - 58ms/step
Epoch 2/100
292/292 - 15s - loss: 0.2856 - acc: 0.8856 - auc: 0.9472 - val_loss: 0.2516 - val_acc: 0.8931 - val_auc: 0.9611 - 15s/epoch - 50ms/step
Epoch 3/100
292/292 - 15s - loss: 0.2779 - acc: 0.8884 - auc: 0.9505 - val_loss: 0.2383 - val_acc: 0.9037 - val_auc: 0.9663 - 15s/epoch - 50ms/step
Epoch 4/100
292/292 - 15s - loss: 0.2743 - acc: 0.8900 - auc: 0.9516 - val_loss: 0.2347 - val_acc: 0.8960 - val_auc: 0.9651 - 15s/epoch - 50ms/step
Epoch 5/100
292/292 - 15s - loss: 0.2680 - acc: 0.8926 - auc: 0.9539 - val_loss: 0.2337 - val_acc: 0.8979 - val_auc: 0.9663 - 15s/epoch - 50ms/step
Epoch 6/100
292/292 - 15s - loss: 0.2638 - acc: 0.8921 - auc: 0.9551 - val_loss: 0.2377 - val_acc: 0.9104 - val_auc: 0.9673 - 15s/epoch - 50ms/step
Epoch 7/100
292/292 - 15s - loss: 0.2638 - acc: 0.8955 - auc: 0.9556 - val_loss: 0.2477 - val_acc: 0.8921 - val_auc: 0.9636 - 15s/epoch - 50ms/step
Epoch 8/100
292/292 - 15s - loss: 0.2603 - acc: 0.8957 - auc: 0.9574 - val_loss: 0.2281 - val_acc: 0.9046 - val_auc: 0.9668 - 15s/epoch - 50ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2591 - acc: 0.8947 - auc: 0.9573 - val_loss: 0.2322 - val_acc: 0.9114 - val_auc: 0.9660 - 14s/epoch - 49ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2571 - acc: 0.8968 - auc: 0.9587 - val_loss: 0.2226 - val_acc: 0.9133 - val_auc: 0.9691 - 14s/epoch - 50ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2570 - acc: 0.8954 - auc: 0.9587 - val_loss: 0.2235 - val_acc: 0.9075 - val_auc: 0.9680 - 14s/epoch - 49ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2556 - acc: 0.8971 - auc: 0.9588 - val_loss: 0.2265 - val_acc: 0.9152 - val_auc: 0.9685 - 14s/epoch - 50ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2507 - acc: 0.8990 - auc: 0.9608 - val_loss: 0.2226 - val_acc: 0.9123 - val_auc: 0.9707 - 14s/epoch - 50ms/step
Epoch 14/100
292/292 - 15s - loss: 0.2492 - acc: 0.8995 - auc: 0.9611 - val_loss: 0.2165 - val_acc: 0.9162 - val_auc: 0.9701 - 15s/epoch - 50ms/step
Epoch 15/100
292/292 - 15s - loss: 0.2455 - acc: 0.9007 - auc: 0.9626 - val_loss: 0.2197 - val_acc: 0.9133 - val_auc: 0.9713 - 15s/epoch - 50ms/step
Epoch 16/100
292/292 - 15s - loss: 0.2466 - acc: 0.9008 - auc: 0.9625 - val_loss: 0.2178 - val_acc: 0.9143 - val_auc: 0.9705 - 15s/epoch - 50ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2418 - acc: 0.9030 - auc: 0.9630 - val_loss: 0.2238 - val_acc: 0.9094 - val_auc: 0.9684 - 14s/epoch - 49ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2420 - acc: 0.9035 - auc: 0.9634 - val_loss: 0.2126 - val_acc: 0.9133 - val_auc: 0.9719 - 14s/epoch - 50ms/step
Epoch 19/100
292/292 - 15s - loss: 0.2410 - acc: 0.9020 - auc: 0.9640 - val_loss: 0.2198 - val_acc: 0.9075 - val_auc: 0.9704 - 15s/epoch - 50ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2343 - acc: 0.9036 - auc: 0.9659 - val_loss: 0.2263 - val_acc: 0.9094 - val_auc: 0.9680 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2313 - acc: 0.9076 - auc: 0.9666 - val_loss: 0.2162 - val_acc: 0.9123 - val_auc: 0.9716 - 14s/epoch - 49ms/step
Epoch 22/100
292/292 - 15s - loss: 0.2296 - acc: 0.9090 - auc: 0.9670 - val_loss: 0.2169 - val_acc: 0.9114 - val_auc: 0.9707 - 15s/epoch - 50ms/step
Epoch 23/100
292/292 - 15s - loss: 0.2290 - acc: 0.9108 - auc: 0.9671 - val_loss: 0.2235 - val_acc: 0.9133 - val_auc: 0.9689 - 15s/epoch - 50ms/step
Epoch 24/100
292/292 - 15s - loss: 0.2258 - acc: 0.9096 - auc: 0.9682 - val_loss: 0.2212 - val_acc: 0.9191 - val_auc: 0.9685 - 15s/epoch - 50ms/step
Epoch 25/100
292/292 - 14s - loss: 0.2232 - acc: 0.9104 - auc: 0.9689 - val_loss: 0.2416 - val_acc: 0.9037 - val_auc: 0.9655 - 14s/epoch - 49ms/step
Epoch 26/100
292/292 - 14s - loss: 0.2192 - acc: 0.9128 - auc: 0.9697 - val_loss: 0.2254 - val_acc: 0.9094 - val_auc: 0.9688 - 14s/epoch - 49ms/step
Epoch 27/100
292/292 - 16s - loss: 0.2153 - acc: 0.9141 - auc: 0.9713 - val_loss: 0.2298 - val_acc: 0.9094 - val_auc: 0.9659 - 16s/epoch - 55ms/step
Epoch 28/100
292/292 - 15s - loss: 0.2153 - acc: 0.9125 - auc: 0.9707 - val_loss: 0.2424 - val_acc: 0.9008 - val_auc: 0.9643 - 15s/epoch - 50ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
33/33 - 1s - 727ms/epoch - 22ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.89       403
           1       0.92      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.91      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.52      0.44       403
           1       0.61      0.48      0.54       635

    accuracy                           0.50      1038
   macro avg       0.50      0.50      0.49      1038
weighted avg       0.53      0.50      0.50      1038

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3168 - acc: 0.8688 - auc: 0.9367 - val_loss: 0.2870 - val_acc: 0.8804 - val_auc: 0.9544 - 17s/epoch - 57ms/step
Epoch 2/100
292/292 - 15s - loss: 0.2853 - acc: 0.8839 - auc: 0.9476 - val_loss: 0.2726 - val_acc: 0.8824 - val_auc: 0.9557 - 15s/epoch - 50ms/step
Epoch 3/100
292/292 - 15s - loss: 0.2716 - acc: 0.8917 - auc: 0.9523 - val_loss: 0.2674 - val_acc: 0.8872 - val_auc: 0.9602 - 15s/epoch - 50ms/step
Epoch 4/100
292/292 - 15s - loss: 0.2673 - acc: 0.8930 - auc: 0.9541 - val_loss: 0.2549 - val_acc: 0.8920 - val_auc: 0.9615 - 15s/epoch - 50ms/step
Epoch 5/100
292/292 - 15s - loss: 0.2614 - acc: 0.8927 - auc: 0.9564 - val_loss: 0.2527 - val_acc: 0.8920 - val_auc: 0.9623 - 15s/epoch - 50ms/step
Epoch 6/100
292/292 - 15s - loss: 0.2621 - acc: 0.8948 - auc: 0.9559 - val_loss: 0.2674 - val_acc: 0.8881 - val_auc: 0.9580 - 15s/epoch - 50ms/step
Epoch 7/100
292/292 - 15s - loss: 0.2584 - acc: 0.8966 - auc: 0.9575 - val_loss: 0.2489 - val_acc: 0.8959 - val_auc: 0.9646 - 15s/epoch - 50ms/step
Epoch 8/100
292/292 - 15s - loss: 0.2554 - acc: 0.8967 - auc: 0.9587 - val_loss: 0.2491 - val_acc: 0.8978 - val_auc: 0.9642 - 15s/epoch - 50ms/step
Epoch 9/100
292/292 - 15s - loss: 0.2520 - acc: 0.8976 - auc: 0.9597 - val_loss: 0.2562 - val_acc: 0.8959 - val_auc: 0.9628 - 15s/epoch - 50ms/step
Epoch 10/100
292/292 - 15s - loss: 0.2487 - acc: 0.8999 - auc: 0.9612 - val_loss: 0.2546 - val_acc: 0.8987 - val_auc: 0.9633 - 15s/epoch - 50ms/step
Epoch 11/100
292/292 - 15s - loss: 0.2477 - acc: 0.8993 - auc: 0.9616 - val_loss: 0.2644 - val_acc: 0.8901 - val_auc: 0.9602 - 15s/epoch - 50ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2470 - acc: 0.8988 - auc: 0.9615 - val_loss: 0.2508 - val_acc: 0.8949 - val_auc: 0.9630 - 14s/epoch - 49ms/step
Epoch 13/100
292/292 - 15s - loss: 0.2439 - acc: 0.9021 - auc: 0.9626 - val_loss: 0.2475 - val_acc: 0.8910 - val_auc: 0.9646 - 15s/epoch - 50ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2403 - acc: 0.9024 - auc: 0.9634 - val_loss: 0.2493 - val_acc: 0.9007 - val_auc: 0.9640 - 14s/epoch - 50ms/step
Epoch 15/100
292/292 - 15s - loss: 0.2380 - acc: 0.9028 - auc: 0.9649 - val_loss: 0.2659 - val_acc: 0.8872 - val_auc: 0.9597 - 15s/epoch - 50ms/step
Epoch 16/100
292/292 - 15s - loss: 0.2358 - acc: 0.9063 - auc: 0.9650 - val_loss: 0.2454 - val_acc: 0.8987 - val_auc: 0.9638 - 15s/epoch - 50ms/step
Epoch 17/100
292/292 - 15s - loss: 0.2324 - acc: 0.9057 - auc: 0.9664 - val_loss: 0.2562 - val_acc: 0.8920 - val_auc: 0.9642 - 15s/epoch - 50ms/step
Epoch 18/100
292/292 - 15s - loss: 0.2295 - acc: 0.9061 - auc: 0.9678 - val_loss: 0.2522 - val_acc: 0.8987 - val_auc: 0.9635 - 15s/epoch - 51ms/step
Epoch 19/100
292/292 - 15s - loss: 0.2288 - acc: 0.9072 - auc: 0.9676 - val_loss: 0.2473 - val_acc: 0.8910 - val_auc: 0.9635 - 15s/epoch - 50ms/step
Epoch 20/100
292/292 - 15s - loss: 0.2260 - acc: 0.9091 - auc: 0.9685 - val_loss: 0.2607 - val_acc: 0.8901 - val_auc: 0.9623 - 15s/epoch - 50ms/step
Epoch 21/100
292/292 - 15s - loss: 0.2242 - acc: 0.9076 - auc: 0.9691 - val_loss: 0.2492 - val_acc: 0.8968 - val_auc: 0.9637 - 15s/epoch - 50ms/step
Epoch 22/100
292/292 - 15s - loss: 0.2170 - acc: 0.9106 - auc: 0.9710 - val_loss: 0.2486 - val_acc: 0.8881 - val_auc: 0.9623 - 15s/epoch - 50ms/step
Epoch 23/100
292/292 - 15s - loss: 0.2131 - acc: 0.9138 - auc: 0.9718 - val_loss: 0.2604 - val_acc: 0.8920 - val_auc: 0.9631 - 15s/epoch - 50ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
33/33 - 1s - 696ms/epoch - 21ms/step
              precision    recall  f1-score   support

           0       0.85      0.87      0.86       403
           1       0.92      0.91      0.91       634

    accuracy                           0.89      1037
   macro avg       0.88      0.89      0.89      1037
weighted avg       0.89      0.89      0.89      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.61      0.48      0.54       634

    accuracy                           0.49      1037
   macro avg       0.50      0.50      0.49      1037
weighted avg       0.52      0.49      0.50      1037

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 18s - loss: 0.3122 - acc: 0.8666 - auc: 0.9388 - val_loss: 0.3066 - val_acc: 0.8775 - val_auc: 0.9387 - 18s/epoch - 60ms/step
Epoch 2/100
292/292 - 15s - loss: 0.2769 - acc: 0.8908 - auc: 0.9512 - val_loss: 0.3106 - val_acc: 0.8833 - val_auc: 0.9409 - 15s/epoch - 50ms/step
Epoch 3/100
292/292 - 15s - loss: 0.2666 - acc: 0.8921 - auc: 0.9545 - val_loss: 0.2946 - val_acc: 0.8852 - val_auc: 0.9441 - 15s/epoch - 50ms/step
Epoch 4/100
292/292 - 15s - loss: 0.2609 - acc: 0.8968 - auc: 0.9566 - val_loss: 0.2925 - val_acc: 0.8852 - val_auc: 0.9458 - 15s/epoch - 50ms/step
Epoch 5/100
292/292 - 15s - loss: 0.2590 - acc: 0.8972 - auc: 0.9575 - val_loss: 0.2982 - val_acc: 0.8852 - val_auc: 0.9448 - 15s/epoch - 50ms/step
Epoch 6/100
292/292 - 15s - loss: 0.2579 - acc: 0.8984 - auc: 0.9580 - val_loss: 0.2855 - val_acc: 0.8920 - val_auc: 0.9454 - 15s/epoch - 50ms/step
Epoch 7/100
292/292 - 15s - loss: 0.2552 - acc: 0.8975 - auc: 0.9590 - val_loss: 0.2847 - val_acc: 0.8891 - val_auc: 0.9480 - 15s/epoch - 50ms/step
Epoch 8/100
292/292 - 15s - loss: 0.2538 - acc: 0.8975 - auc: 0.9597 - val_loss: 0.2816 - val_acc: 0.8920 - val_auc: 0.9492 - 15s/epoch - 50ms/step
Epoch 9/100
292/292 - 15s - loss: 0.2506 - acc: 0.8991 - auc: 0.9611 - val_loss: 0.3099 - val_acc: 0.8862 - val_auc: 0.9392 - 15s/epoch - 50ms/step
Epoch 10/100
292/292 - 15s - loss: 0.2512 - acc: 0.8988 - auc: 0.9607 - val_loss: 0.2780 - val_acc: 0.8881 - val_auc: 0.9519 - 15s/epoch - 50ms/step
Epoch 11/100
292/292 - 15s - loss: 0.2464 - acc: 0.8989 - auc: 0.9619 - val_loss: 0.2768 - val_acc: 0.8843 - val_auc: 0.9528 - 15s/epoch - 50ms/step
Epoch 12/100
292/292 - 15s - loss: 0.2460 - acc: 0.9003 - auc: 0.9626 - val_loss: 0.2841 - val_acc: 0.8862 - val_auc: 0.9513 - 15s/epoch - 50ms/step
Epoch 13/100
292/292 - 15s - loss: 0.2409 - acc: 0.9021 - auc: 0.9642 - val_loss: 0.2832 - val_acc: 0.8862 - val_auc: 0.9499 - 15s/epoch - 50ms/step
Epoch 14/100
292/292 - 15s - loss: 0.2405 - acc: 0.9027 - auc: 0.9643 - val_loss: 0.2754 - val_acc: 0.8852 - val_auc: 0.9533 - 15s/epoch - 50ms/step
Epoch 15/100
292/292 - 15s - loss: 0.2375 - acc: 0.9036 - auc: 0.9651 - val_loss: 0.2769 - val_acc: 0.8939 - val_auc: 0.9502 - 15s/epoch - 50ms/step
Epoch 16/100
292/292 - 15s - loss: 0.2385 - acc: 0.9030 - auc: 0.9652 - val_loss: 0.2874 - val_acc: 0.8824 - val_auc: 0.9496 - 15s/epoch - 50ms/step
Epoch 17/100
292/292 - 15s - loss: 0.2320 - acc: 0.9079 - auc: 0.9665 - val_loss: 0.2813 - val_acc: 0.8862 - val_auc: 0.9509 - 15s/epoch - 50ms/step
Epoch 18/100
292/292 - 15s - loss: 0.2328 - acc: 0.9088 - auc: 0.9663 - val_loss: 0.2685 - val_acc: 0.8862 - val_auc: 0.9555 - 15s/epoch - 51ms/step
Epoch 19/100
292/292 - 20s - loss: 0.2284 - acc: 0.9068 - auc: 0.9680 - val_loss: 0.2963 - val_acc: 0.8833 - val_auc: 0.9452 - 20s/epoch - 67ms/step
Epoch 20/100
292/292 - 18s - loss: 0.2289 - acc: 0.9106 - auc: 0.9675 - val_loss: 0.2949 - val_acc: 0.8930 - val_auc: 0.9457 - 18s/epoch - 62ms/step
Epoch 21/100
292/292 - 15s - loss: 0.2217 - acc: 0.9104 - auc: 0.9692 - val_loss: 0.2946 - val_acc: 0.8910 - val_auc: 0.9481 - 15s/epoch - 50ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2223 - acc: 0.9070 - auc: 0.9696 - val_loss: 0.2754 - val_acc: 0.8987 - val_auc: 0.9531 - 14s/epoch - 50ms/step
Epoch 23/100
292/292 - 15s - loss: 0.2176 - acc: 0.9128 - auc: 0.9708 - val_loss: 0.2965 - val_acc: 0.8939 - val_auc: 0.9470 - 15s/epoch - 51ms/step
Epoch 24/100
292/292 - 18s - loss: 0.2131 - acc: 0.9140 - auc: 0.9714 - val_loss: 0.3136 - val_acc: 0.8775 - val_auc: 0.9430 - 18s/epoch - 63ms/step
Epoch 25/100
292/292 - 14s - loss: 0.2110 - acc: 0.9160 - auc: 0.9725 - val_loss: 0.2788 - val_acc: 0.8891 - val_auc: 0.9528 - 14s/epoch - 49ms/step
Epoch 26/100
292/292 - 15s - loss: 0.2031 - acc: 0.9197 - auc: 0.9746 - val_loss: 0.2993 - val_acc: 0.8872 - val_auc: 0.9476 - 15s/epoch - 50ms/step
Epoch 27/100
292/292 - 14s - loss: 0.2011 - acc: 0.9201 - auc: 0.9749 - val_loss: 0.2785 - val_acc: 0.9007 - val_auc: 0.9523 - 14s/epoch - 50ms/step
Epoch 28/100
292/292 - 14s - loss: 0.1964 - acc: 0.9221 - auc: 0.9760 - val_loss: 0.2905 - val_acc: 0.8891 - val_auc: 0.9509 - 14s/epoch - 50ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
33/33 - 1s - 825ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.88      0.82      0.85       403
           1       0.89      0.93      0.91       634

    accuracy                           0.89      1037
   macro avg       0.88      0.87      0.88      1037
weighted avg       0.89      0.89      0.89      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.62      0.50      0.56       634

    accuracy                           0.51      1037
   macro avg       0.51      0.51      0.50      1037
weighted avg       0.53      0.51      0.51      1037

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3155 - acc: 0.8645 - auc: 0.9372 - val_loss: 0.2651 - val_acc: 0.9007 - val_auc: 0.9522 - 17s/epoch - 57ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2822 - acc: 0.8839 - auc: 0.9496 - val_loss: 0.2532 - val_acc: 0.9026 - val_auc: 0.9551 - 14s/epoch - 49ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2724 - acc: 0.8902 - auc: 0.9520 - val_loss: 0.2681 - val_acc: 0.8997 - val_auc: 0.9549 - 14s/epoch - 49ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2669 - acc: 0.8915 - auc: 0.9549 - val_loss: 0.2675 - val_acc: 0.8930 - val_auc: 0.9570 - 14s/epoch - 50ms/step
Epoch 5/100
292/292 - 15s - loss: 0.2641 - acc: 0.8939 - auc: 0.9556 - val_loss: 0.2483 - val_acc: 0.9007 - val_auc: 0.9589 - 15s/epoch - 50ms/step
Epoch 6/100
292/292 - 15s - loss: 0.2628 - acc: 0.8951 - auc: 0.9560 - val_loss: 0.2478 - val_acc: 0.9036 - val_auc: 0.9616 - 15s/epoch - 50ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2583 - acc: 0.8941 - auc: 0.9579 - val_loss: 0.2570 - val_acc: 0.8959 - val_auc: 0.9570 - 14s/epoch - 50ms/step
Epoch 8/100
292/292 - 15s - loss: 0.2568 - acc: 0.8963 - auc: 0.9582 - val_loss: 0.2503 - val_acc: 0.9016 - val_auc: 0.9603 - 15s/epoch - 50ms/step
Epoch 9/100
292/292 - 15s - loss: 0.2549 - acc: 0.8974 - auc: 0.9588 - val_loss: 0.2594 - val_acc: 0.8997 - val_auc: 0.9569 - 15s/epoch - 50ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2487 - acc: 0.8984 - auc: 0.9613 - val_loss: 0.2478 - val_acc: 0.8997 - val_auc: 0.9603 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2477 - acc: 0.9001 - auc: 0.9616 - val_loss: 0.2852 - val_acc: 0.8920 - val_auc: 0.9514 - 14s/epoch - 49ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2448 - acc: 0.8985 - auc: 0.9626 - val_loss: 0.2506 - val_acc: 0.8949 - val_auc: 0.9608 - 14s/epoch - 50ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2471 - acc: 0.8988 - auc: 0.9623 - val_loss: 0.2480 - val_acc: 0.8968 - val_auc: 0.9621 - 14s/epoch - 49ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2392 - acc: 0.9029 - auc: 0.9649 - val_loss: 0.2534 - val_acc: 0.8978 - val_auc: 0.9610 - 14s/epoch - 50ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2376 - acc: 0.9028 - auc: 0.9650 - val_loss: 0.2570 - val_acc: 0.8968 - val_auc: 0.9593 - 14s/epoch - 50ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2353 - acc: 0.9025 - auc: 0.9653 - val_loss: 0.2613 - val_acc: 0.8978 - val_auc: 0.9589 - 14s/epoch - 49ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2350 - acc: 0.9044 - auc: 0.9662 - val_loss: 0.2670 - val_acc: 0.9026 - val_auc: 0.9560 - 14s/epoch - 50ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2295 - acc: 0.9065 - auc: 0.9676 - val_loss: 0.2697 - val_acc: 0.8978 - val_auc: 0.9553 - 14s/epoch - 50ms/step
Epoch 19/100
292/292 - 15s - loss: 0.2295 - acc: 0.9100 - auc: 0.9668 - val_loss: 0.2692 - val_acc: 0.8968 - val_auc: 0.9595 - 15s/epoch - 50ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2259 - acc: 0.9105 - auc: 0.9687 - val_loss: 0.2569 - val_acc: 0.9007 - val_auc: 0.9574 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2225 - acc: 0.9099 - auc: 0.9689 - val_loss: 0.2629 - val_acc: 0.8997 - val_auc: 0.9567 - 14s/epoch - 49ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2199 - acc: 0.9118 - auc: 0.9702 - val_loss: 0.2767 - val_acc: 0.8939 - val_auc: 0.9523 - 14s/epoch - 49ms/step
Epoch 23/100
292/292 - 14s - loss: 0.2161 - acc: 0.9139 - auc: 0.9712 - val_loss: 0.2766 - val_acc: 0.8939 - val_auc: 0.9518 - 14s/epoch - 49ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
33/33 - 1s - 710ms/epoch - 22ms/step
              precision    recall  f1-score   support

           0       0.91      0.82      0.86       403
           1       0.89      0.95      0.92       634

    accuracy                           0.90      1037
   macro avg       0.90      0.88      0.89      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.61      0.51      0.55       634

    accuracy                           0.50      1037
   macro avg       0.49      0.49      0.49      1037
weighted avg       0.52      0.50      0.50      1037

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3123 - acc: 0.8670 - auc: 0.9384 - val_loss: 0.3068 - val_acc: 0.8746 - val_auc: 0.9413 - 17s/epoch - 60ms/step
Epoch 2/100
292/292 - 15s - loss: 0.2818 - acc: 0.8868 - auc: 0.9493 - val_loss: 0.3048 - val_acc: 0.8660 - val_auc: 0.9442 - 15s/epoch - 50ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2731 - acc: 0.8906 - auc: 0.9521 - val_loss: 0.2938 - val_acc: 0.8766 - val_auc: 0.9486 - 14s/epoch - 50ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2631 - acc: 0.8967 - auc: 0.9555 - val_loss: 0.2884 - val_acc: 0.8862 - val_auc: 0.9487 - 14s/epoch - 49ms/step
Epoch 5/100
292/292 - 14s - loss: 0.2613 - acc: 0.8952 - auc: 0.9566 - val_loss: 0.2910 - val_acc: 0.8746 - val_auc: 0.9485 - 14s/epoch - 49ms/step
Epoch 6/100
292/292 - 14s - loss: 0.2570 - acc: 0.8973 - auc: 0.9583 - val_loss: 0.2893 - val_acc: 0.8756 - val_auc: 0.9499 - 14s/epoch - 49ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2572 - acc: 0.8989 - auc: 0.9578 - val_loss: 0.2902 - val_acc: 0.8814 - val_auc: 0.9481 - 14s/epoch - 49ms/step
Epoch 8/100
292/292 - 14s - loss: 0.2537 - acc: 0.8979 - auc: 0.9589 - val_loss: 0.2952 - val_acc: 0.8804 - val_auc: 0.9512 - 14s/epoch - 49ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2504 - acc: 0.8996 - auc: 0.9609 - val_loss: 0.2862 - val_acc: 0.8795 - val_auc: 0.9505 - 14s/epoch - 49ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2489 - acc: 0.8990 - auc: 0.9609 - val_loss: 0.2894 - val_acc: 0.8775 - val_auc: 0.9499 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 14s - loss: 0.2462 - acc: 0.8999 - auc: 0.9619 - val_loss: 0.2958 - val_acc: 0.8756 - val_auc: 0.9508 - 14s/epoch - 49ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2450 - acc: 0.8998 - auc: 0.9626 - val_loss: 0.2926 - val_acc: 0.8795 - val_auc: 0.9500 - 14s/epoch - 49ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2411 - acc: 0.9003 - auc: 0.9638 - val_loss: 0.2894 - val_acc: 0.8804 - val_auc: 0.9506 - 14s/epoch - 49ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2393 - acc: 0.9032 - auc: 0.9644 - val_loss: 0.2817 - val_acc: 0.8746 - val_auc: 0.9528 - 14s/epoch - 49ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2334 - acc: 0.9042 - auc: 0.9664 - val_loss: 0.2958 - val_acc: 0.8679 - val_auc: 0.9473 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2319 - acc: 0.9047 - auc: 0.9670 - val_loss: 0.2867 - val_acc: 0.8785 - val_auc: 0.9521 - 14s/epoch - 49ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2312 - acc: 0.9060 - auc: 0.9672 - val_loss: 0.2935 - val_acc: 0.8766 - val_auc: 0.9503 - 14s/epoch - 49ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2292 - acc: 0.9046 - auc: 0.9679 - val_loss: 0.2895 - val_acc: 0.8785 - val_auc: 0.9491 - 14s/epoch - 49ms/step
Epoch 19/100
292/292 - 14s - loss: 0.2262 - acc: 0.9102 - auc: 0.9687 - val_loss: 0.2975 - val_acc: 0.8708 - val_auc: 0.9492 - 14s/epoch - 49ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2208 - acc: 0.9103 - auc: 0.9700 - val_loss: 0.2946 - val_acc: 0.8785 - val_auc: 0.9495 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2180 - acc: 0.9125 - auc: 0.9711 - val_loss: 0.2967 - val_acc: 0.8746 - val_auc: 0.9464 - 14s/epoch - 49ms/step
Epoch 22/100
292/292 - 14s - loss: 0.2141 - acc: 0.9139 - auc: 0.9719 - val_loss: 0.3043 - val_acc: 0.8756 - val_auc: 0.9454 - 14s/epoch - 49ms/step
Epoch 23/100
292/292 - 14s - loss: 0.2128 - acc: 0.9127 - auc: 0.9725 - val_loss: 0.3068 - val_acc: 0.8698 - val_auc: 0.9444 - 14s/epoch - 49ms/step
Epoch 24/100
292/292 - 14s - loss: 0.2068 - acc: 0.9176 - auc: 0.9737 - val_loss: 0.3207 - val_acc: 0.8795 - val_auc: 0.9403 - 14s/epoch - 49ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
33/33 - 1s - 705ms/epoch - 21ms/step
              precision    recall  f1-score   support

           0       0.85      0.82      0.84       403
           1       0.89      0.91      0.90       634

    accuracy                           0.87      1037
   macro avg       0.87      0.86      0.87      1037
weighted avg       0.87      0.87      0.87      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.55      0.47       403
           1       0.63      0.49      0.55       634

    accuracy                           0.51      1037
   macro avg       0.52      0.52      0.51      1037
weighted avg       0.54      0.51      0.52      1037

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 17s - loss: 0.3155 - acc: 0.8654 - auc: 0.9376 - val_loss: 0.3161 - val_acc: 0.8679 - val_auc: 0.9375 - 17s/epoch - 57ms/step
Epoch 2/100
292/292 - 14s - loss: 0.2801 - acc: 0.8877 - auc: 0.9499 - val_loss: 0.2918 - val_acc: 0.8910 - val_auc: 0.9425 - 14s/epoch - 49ms/step
Epoch 3/100
292/292 - 14s - loss: 0.2708 - acc: 0.8918 - auc: 0.9530 - val_loss: 0.2746 - val_acc: 0.8930 - val_auc: 0.9503 - 14s/epoch - 50ms/step
Epoch 4/100
292/292 - 14s - loss: 0.2658 - acc: 0.8943 - auc: 0.9547 - val_loss: 0.2790 - val_acc: 0.8949 - val_auc: 0.9478 - 14s/epoch - 50ms/step
Epoch 5/100
292/292 - 14s - loss: 0.2604 - acc: 0.8937 - auc: 0.9570 - val_loss: 0.2734 - val_acc: 0.9045 - val_auc: 0.9503 - 14s/epoch - 50ms/step
Epoch 6/100
292/292 - 14s - loss: 0.2567 - acc: 0.8973 - auc: 0.9583 - val_loss: 0.2754 - val_acc: 0.8959 - val_auc: 0.9491 - 14s/epoch - 49ms/step
Epoch 7/100
292/292 - 14s - loss: 0.2554 - acc: 0.8969 - auc: 0.9591 - val_loss: 0.2647 - val_acc: 0.9026 - val_auc: 0.9533 - 14s/epoch - 49ms/step
Epoch 8/100
292/292 - 14s - loss: 0.2540 - acc: 0.8959 - auc: 0.9595 - val_loss: 0.2649 - val_acc: 0.8978 - val_auc: 0.9534 - 14s/epoch - 49ms/step
Epoch 9/100
292/292 - 14s - loss: 0.2531 - acc: 0.8969 - auc: 0.9602 - val_loss: 0.2650 - val_acc: 0.9084 - val_auc: 0.9527 - 14s/epoch - 49ms/step
Epoch 10/100
292/292 - 14s - loss: 0.2470 - acc: 0.9013 - auc: 0.9618 - val_loss: 0.2652 - val_acc: 0.9026 - val_auc: 0.9528 - 14s/epoch - 49ms/step
Epoch 11/100
292/292 - 15s - loss: 0.2455 - acc: 0.9021 - auc: 0.9623 - val_loss: 0.2642 - val_acc: 0.8968 - val_auc: 0.9570 - 15s/epoch - 50ms/step
Epoch 12/100
292/292 - 14s - loss: 0.2437 - acc: 0.9005 - auc: 0.9632 - val_loss: 0.2615 - val_acc: 0.9016 - val_auc: 0.9549 - 14s/epoch - 49ms/step
Epoch 13/100
292/292 - 14s - loss: 0.2440 - acc: 0.8981 - auc: 0.9634 - val_loss: 0.2724 - val_acc: 0.8930 - val_auc: 0.9501 - 14s/epoch - 49ms/step
Epoch 14/100
292/292 - 14s - loss: 0.2406 - acc: 0.9016 - auc: 0.9639 - val_loss: 0.2535 - val_acc: 0.9065 - val_auc: 0.9564 - 14s/epoch - 49ms/step
Epoch 15/100
292/292 - 14s - loss: 0.2385 - acc: 0.9029 - auc: 0.9648 - val_loss: 0.2580 - val_acc: 0.9007 - val_auc: 0.9562 - 14s/epoch - 49ms/step
Epoch 16/100
292/292 - 14s - loss: 0.2387 - acc: 0.9017 - auc: 0.9647 - val_loss: 0.2574 - val_acc: 0.9016 - val_auc: 0.9564 - 14s/epoch - 49ms/step
Epoch 17/100
292/292 - 14s - loss: 0.2319 - acc: 0.9046 - auc: 0.9667 - val_loss: 0.2554 - val_acc: 0.9065 - val_auc: 0.9554 - 14s/epoch - 49ms/step
Epoch 18/100
292/292 - 14s - loss: 0.2313 - acc: 0.9051 - auc: 0.9669 - val_loss: 0.2700 - val_acc: 0.9007 - val_auc: 0.9538 - 14s/epoch - 49ms/step
Epoch 19/100
292/292 - 14s - loss: 0.2304 - acc: 0.9072 - auc: 0.9671 - val_loss: 0.2613 - val_acc: 0.8978 - val_auc: 0.9553 - 14s/epoch - 49ms/step
Epoch 20/100
292/292 - 14s - loss: 0.2244 - acc: 0.9110 - auc: 0.9685 - val_loss: 0.2734 - val_acc: 0.9036 - val_auc: 0.9514 - 14s/epoch - 49ms/step
Epoch 21/100
292/292 - 14s - loss: 0.2245 - acc: 0.9094 - auc: 0.9689 - val_loss: 0.2705 - val_acc: 0.8959 - val_auc: 0.9541 - 14s/epoch - 49ms/step
Early stopping epoch: 20
******Evaluating TEST set*********
33/33 - 1s - 717ms/epoch - 22ms/step
              precision    recall  f1-score   support

           0       0.88      0.85      0.87       403
           1       0.91      0.92      0.92       634

    accuracy                           0.90      1037
   macro avg       0.89      0.89      0.89      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.45      0.41       403
           1       0.60      0.52      0.56       634

    accuracy                           0.49      1037
   macro avg       0.49      0.49      0.48      1037
weighted avg       0.51      0.49      0.50      1037

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8980] IC [0.8907, 0.9053]
Mean Recall[0.8876] IC [0.8803, 0.8949]
Mean F1[0.8915] IC [0.8839, 0.8992]
Median Accuracy[0.8968]
Median Recall[0.8879]
Median F1[0.8901]
