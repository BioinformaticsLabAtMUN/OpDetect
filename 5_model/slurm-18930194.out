fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3180 - acc: 0.8666 - auc: 0.9366 - val_loss: 0.3008 - val_acc: 0.8767 - val_auc: 0.9441 - 22s/epoch - 76ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2878 - acc: 0.8807 - auc: 0.9474 - val_loss: 0.2757 - val_acc: 0.8863 - val_auc: 0.9512 - 19s/epoch - 66ms/step
Epoch 3/100
292/292 - 20s - loss: 0.2714 - acc: 0.8905 - auc: 0.9528 - val_loss: 0.2600 - val_acc: 0.8950 - val_auc: 0.9543 - 20s/epoch - 67ms/step
Epoch 4/100
292/292 - 20s - loss: 0.2692 - acc: 0.8900 - auc: 0.9533 - val_loss: 0.2501 - val_acc: 0.8998 - val_auc: 0.9581 - 20s/epoch - 68ms/step
Epoch 5/100
292/292 - 17s - loss: 0.2633 - acc: 0.8945 - auc: 0.9557 - val_loss: 0.2707 - val_acc: 0.8931 - val_auc: 0.9515 - 17s/epoch - 57ms/step
Epoch 6/100
292/292 - 16s - loss: 0.2598 - acc: 0.8981 - auc: 0.9574 - val_loss: 0.2502 - val_acc: 0.8979 - val_auc: 0.9590 - 16s/epoch - 55ms/step
Epoch 7/100
292/292 - 16s - loss: 0.2566 - acc: 0.8959 - auc: 0.9586 - val_loss: 0.2596 - val_acc: 0.8988 - val_auc: 0.9593 - 16s/epoch - 54ms/step
Epoch 8/100
292/292 - 16s - loss: 0.2573 - acc: 0.8975 - auc: 0.9584 - val_loss: 0.2473 - val_acc: 0.8998 - val_auc: 0.9604 - 16s/epoch - 55ms/step
Epoch 9/100
292/292 - 16s - loss: 0.2541 - acc: 0.8958 - auc: 0.9595 - val_loss: 0.2552 - val_acc: 0.8969 - val_auc: 0.9590 - 16s/epoch - 54ms/step
Epoch 10/100
292/292 - 16s - loss: 0.2532 - acc: 0.8957 - auc: 0.9604 - val_loss: 0.2512 - val_acc: 0.8988 - val_auc: 0.9604 - 16s/epoch - 54ms/step
Epoch 11/100
292/292 - 16s - loss: 0.2540 - acc: 0.8958 - auc: 0.9595 - val_loss: 0.2556 - val_acc: 0.9008 - val_auc: 0.9579 - 16s/epoch - 54ms/step
Epoch 12/100
292/292 - 16s - loss: 0.2484 - acc: 0.8981 - auc: 0.9618 - val_loss: 0.2504 - val_acc: 0.9027 - val_auc: 0.9598 - 16s/epoch - 55ms/step
Epoch 13/100
292/292 - 16s - loss: 0.2479 - acc: 0.8994 - auc: 0.9620 - val_loss: 0.2429 - val_acc: 0.9008 - val_auc: 0.9610 - 16s/epoch - 54ms/step
Epoch 14/100
292/292 - 16s - loss: 0.2473 - acc: 0.9001 - auc: 0.9622 - val_loss: 0.2616 - val_acc: 0.8940 - val_auc: 0.9567 - 16s/epoch - 54ms/step
Epoch 15/100
292/292 - 16s - loss: 0.2445 - acc: 0.9010 - auc: 0.9630 - val_loss: 0.2436 - val_acc: 0.9046 - val_auc: 0.9615 - 16s/epoch - 54ms/step
Epoch 16/100
292/292 - 20s - loss: 0.2429 - acc: 0.9036 - auc: 0.9632 - val_loss: 0.2530 - val_acc: 0.8940 - val_auc: 0.9610 - 20s/epoch - 69ms/step
Epoch 17/100
292/292 - 20s - loss: 0.2412 - acc: 0.9003 - auc: 0.9638 - val_loss: 0.2438 - val_acc: 0.9066 - val_auc: 0.9607 - 20s/epoch - 68ms/step
Epoch 18/100
292/292 - 20s - loss: 0.2388 - acc: 0.9023 - auc: 0.9647 - val_loss: 0.2447 - val_acc: 0.8960 - val_auc: 0.9630 - 20s/epoch - 70ms/step
Epoch 19/100
292/292 - 20s - loss: 0.2340 - acc: 0.9047 - auc: 0.9664 - val_loss: 0.2493 - val_acc: 0.9008 - val_auc: 0.9609 - 20s/epoch - 70ms/step
Epoch 20/100
292/292 - 20s - loss: 0.2341 - acc: 0.9067 - auc: 0.9660 - val_loss: 0.2419 - val_acc: 0.9075 - val_auc: 0.9623 - 20s/epoch - 70ms/step
Epoch 21/100
292/292 - 19s - loss: 0.2305 - acc: 0.9062 - auc: 0.9676 - val_loss: 0.2379 - val_acc: 0.9143 - val_auc: 0.9619 - 19s/epoch - 64ms/step
Epoch 22/100
292/292 - 17s - loss: 0.2280 - acc: 0.9074 - auc: 0.9677 - val_loss: 0.2507 - val_acc: 0.9017 - val_auc: 0.9585 - 17s/epoch - 57ms/step
Epoch 23/100
292/292 - 21s - loss: 0.2270 - acc: 0.9082 - auc: 0.9686 - val_loss: 0.2375 - val_acc: 0.9133 - val_auc: 0.9638 - 21s/epoch - 71ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2210 - acc: 0.9089 - auc: 0.9700 - val_loss: 0.2462 - val_acc: 0.9075 - val_auc: 0.9609 - 21s/epoch - 70ms/step
Epoch 25/100
292/292 - 20s - loss: 0.2183 - acc: 0.9102 - auc: 0.9709 - val_loss: 0.2510 - val_acc: 0.9094 - val_auc: 0.9594 - 20s/epoch - 69ms/step
Epoch 26/100
292/292 - 20s - loss: 0.2151 - acc: 0.9138 - auc: 0.9718 - val_loss: 0.2606 - val_acc: 0.8998 - val_auc: 0.9586 - 20s/epoch - 69ms/step
Epoch 27/100
292/292 - 19s - loss: 0.2141 - acc: 0.9131 - auc: 0.9716 - val_loss: 0.2607 - val_acc: 0.9046 - val_auc: 0.9576 - 19s/epoch - 66ms/step
Epoch 28/100
292/292 - 21s - loss: 0.2084 - acc: 0.9159 - auc: 0.9739 - val_loss: 0.2607 - val_acc: 0.9037 - val_auc: 0.9554 - 21s/epoch - 71ms/step
Epoch 29/100
292/292 - 20s - loss: 0.1979 - acc: 0.9200 - auc: 0.9762 - val_loss: 0.2564 - val_acc: 0.9075 - val_auc: 0.9601 - 20s/epoch - 69ms/step
Epoch 30/100
292/292 - 20s - loss: 0.1981 - acc: 0.9211 - auc: 0.9763 - val_loss: 0.2561 - val_acc: 0.9094 - val_auc: 0.9584 - 20s/epoch - 69ms/step
Epoch 31/100
292/292 - 20s - loss: 0.1925 - acc: 0.9217 - auc: 0.9776 - val_loss: 0.2625 - val_acc: 0.8998 - val_auc: 0.9574 - 20s/epoch - 69ms/step
Epoch 32/100
292/292 - 20s - loss: 0.1837 - acc: 0.9236 - auc: 0.9799 - val_loss: 0.2808 - val_acc: 0.9027 - val_auc: 0.9565 - 20s/epoch - 69ms/step
Epoch 33/100
292/292 - 20s - loss: 0.1832 - acc: 0.9258 - auc: 0.9796 - val_loss: 0.2668 - val_acc: 0.9037 - val_auc: 0.9562 - 20s/epoch - 68ms/step
Early stopping epoch: 32
******Evaluating TEST set*********
33/33 - 1s - 834ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.90      0.88      0.89       403
           1       0.92      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.91      0.91      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.45      0.39       403
           1       0.58      0.48      0.52       635

    accuracy                           0.47      1038
   macro avg       0.46      0.46      0.46      1038
weighted avg       0.49      0.47      0.47      1038

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3101 - acc: 0.8694 - auc: 0.9394 - val_loss: 0.3024 - val_acc: 0.8786 - val_auc: 0.9435 - 22s/epoch - 74ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2815 - acc: 0.8872 - auc: 0.9487 - val_loss: 0.2931 - val_acc: 0.8805 - val_auc: 0.9480 - 19s/epoch - 65ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2668 - acc: 0.8931 - auc: 0.9532 - val_loss: 0.2814 - val_acc: 0.8892 - val_auc: 0.9525 - 19s/epoch - 66ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2606 - acc: 0.8956 - auc: 0.9560 - val_loss: 0.2906 - val_acc: 0.8767 - val_auc: 0.9503 - 19s/epoch - 66ms/step
Epoch 5/100
292/292 - 19s - loss: 0.2580 - acc: 0.8974 - auc: 0.9572 - val_loss: 0.2780 - val_acc: 0.8863 - val_auc: 0.9544 - 19s/epoch - 65ms/step
Epoch 6/100
292/292 - 19s - loss: 0.2559 - acc: 0.8956 - auc: 0.9584 - val_loss: 0.2853 - val_acc: 0.8825 - val_auc: 0.9528 - 19s/epoch - 64ms/step
Epoch 7/100
292/292 - 19s - loss: 0.2519 - acc: 0.9003 - auc: 0.9597 - val_loss: 0.2958 - val_acc: 0.8805 - val_auc: 0.9521 - 19s/epoch - 64ms/step
Epoch 8/100
292/292 - 19s - loss: 0.2536 - acc: 0.8974 - auc: 0.9592 - val_loss: 0.2771 - val_acc: 0.8805 - val_auc: 0.9557 - 19s/epoch - 64ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2475 - acc: 0.9007 - auc: 0.9613 - val_loss: 0.2728 - val_acc: 0.8844 - val_auc: 0.9559 - 19s/epoch - 64ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2445 - acc: 0.9025 - auc: 0.9622 - val_loss: 0.2912 - val_acc: 0.8699 - val_auc: 0.9517 - 19s/epoch - 64ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2437 - acc: 0.9047 - auc: 0.9622 - val_loss: 0.2739 - val_acc: 0.8815 - val_auc: 0.9566 - 19s/epoch - 64ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2409 - acc: 0.9035 - auc: 0.9636 - val_loss: 0.2970 - val_acc: 0.8776 - val_auc: 0.9492 - 19s/epoch - 64ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2413 - acc: 0.9037 - auc: 0.9633 - val_loss: 0.2851 - val_acc: 0.8776 - val_auc: 0.9522 - 19s/epoch - 64ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2369 - acc: 0.9052 - auc: 0.9643 - val_loss: 0.2692 - val_acc: 0.8844 - val_auc: 0.9586 - 19s/epoch - 65ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2367 - acc: 0.9048 - auc: 0.9647 - val_loss: 0.2772 - val_acc: 0.8786 - val_auc: 0.9559 - 19s/epoch - 64ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2341 - acc: 0.9061 - auc: 0.9661 - val_loss: 0.2739 - val_acc: 0.8940 - val_auc: 0.9566 - 19s/epoch - 65ms/step
Epoch 17/100
292/292 - 19s - loss: 0.2314 - acc: 0.9074 - auc: 0.9672 - val_loss: 0.2700 - val_acc: 0.8844 - val_auc: 0.9588 - 19s/epoch - 65ms/step
Epoch 18/100
292/292 - 20s - loss: 0.2305 - acc: 0.9062 - auc: 0.9670 - val_loss: 0.2710 - val_acc: 0.8882 - val_auc: 0.9576 - 20s/epoch - 68ms/step
Epoch 19/100
292/292 - 19s - loss: 0.2255 - acc: 0.9083 - auc: 0.9688 - val_loss: 0.2714 - val_acc: 0.8786 - val_auc: 0.9576 - 19s/epoch - 64ms/step
Epoch 20/100
292/292 - 19s - loss: 0.2240 - acc: 0.9099 - auc: 0.9688 - val_loss: 0.2717 - val_acc: 0.8815 - val_auc: 0.9573 - 19s/epoch - 65ms/step
Epoch 21/100
292/292 - 19s - loss: 0.2211 - acc: 0.9121 - auc: 0.9695 - val_loss: 0.2830 - val_acc: 0.8748 - val_auc: 0.9545 - 19s/epoch - 64ms/step
Epoch 22/100
292/292 - 19s - loss: 0.2204 - acc: 0.9131 - auc: 0.9702 - val_loss: 0.2748 - val_acc: 0.8757 - val_auc: 0.9566 - 19s/epoch - 64ms/step
Epoch 23/100
292/292 - 19s - loss: 0.2157 - acc: 0.9136 - auc: 0.9716 - val_loss: 0.2684 - val_acc: 0.8776 - val_auc: 0.9585 - 19s/epoch - 64ms/step
Epoch 24/100
292/292 - 19s - loss: 0.2104 - acc: 0.9177 - auc: 0.9727 - val_loss: 0.2780 - val_acc: 0.8834 - val_auc: 0.9578 - 19s/epoch - 64ms/step
Epoch 25/100
292/292 - 19s - loss: 0.2078 - acc: 0.9175 - auc: 0.9737 - val_loss: 0.2861 - val_acc: 0.8738 - val_auc: 0.9528 - 19s/epoch - 64ms/step
Epoch 26/100
292/292 - 19s - loss: 0.2034 - acc: 0.9204 - auc: 0.9743 - val_loss: 0.2962 - val_acc: 0.8757 - val_auc: 0.9501 - 19s/epoch - 64ms/step
Epoch 27/100
292/292 - 19s - loss: 0.2006 - acc: 0.9174 - auc: 0.9753 - val_loss: 0.2872 - val_acc: 0.8796 - val_auc: 0.9517 - 19s/epoch - 64ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
33/33 - 1s - 831ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.87      0.82      0.85       403
           1       0.89      0.92      0.91       635

    accuracy                           0.88      1038
   macro avg       0.88      0.87      0.88      1038
weighted avg       0.88      0.88      0.88      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.51      0.43       403
           1       0.60      0.46      0.52       635

    accuracy                           0.48      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.48      0.49      1038

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 21s - loss: 0.3196 - acc: 0.8602 - auc: 0.9359 - val_loss: 0.2779 - val_acc: 0.8911 - val_auc: 0.9489 - 21s/epoch - 72ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2808 - acc: 0.8856 - auc: 0.9496 - val_loss: 0.2680 - val_acc: 0.9017 - val_auc: 0.9529 - 19s/epoch - 64ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2734 - acc: 0.8904 - auc: 0.9527 - val_loss: 0.2579 - val_acc: 0.9008 - val_auc: 0.9567 - 19s/epoch - 64ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2659 - acc: 0.8928 - auc: 0.9547 - val_loss: 0.2522 - val_acc: 0.9027 - val_auc: 0.9572 - 19s/epoch - 64ms/step
Epoch 5/100
292/292 - 19s - loss: 0.2643 - acc: 0.8959 - auc: 0.9554 - val_loss: 0.2536 - val_acc: 0.9046 - val_auc: 0.9589 - 19s/epoch - 64ms/step
Epoch 6/100
292/292 - 19s - loss: 0.2600 - acc: 0.8964 - auc: 0.9574 - val_loss: 0.2498 - val_acc: 0.9094 - val_auc: 0.9602 - 19s/epoch - 65ms/step
Epoch 7/100
292/292 - 19s - loss: 0.2605 - acc: 0.8985 - auc: 0.9571 - val_loss: 0.2442 - val_acc: 0.9123 - val_auc: 0.9611 - 19s/epoch - 65ms/step
Epoch 8/100
292/292 - 19s - loss: 0.2574 - acc: 0.8959 - auc: 0.9585 - val_loss: 0.2480 - val_acc: 0.9008 - val_auc: 0.9597 - 19s/epoch - 65ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2556 - acc: 0.8947 - auc: 0.9586 - val_loss: 0.2585 - val_acc: 0.9008 - val_auc: 0.9588 - 19s/epoch - 65ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2516 - acc: 0.8999 - auc: 0.9603 - val_loss: 0.2489 - val_acc: 0.9123 - val_auc: 0.9595 - 19s/epoch - 65ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2492 - acc: 0.8988 - auc: 0.9613 - val_loss: 0.2525 - val_acc: 0.8988 - val_auc: 0.9599 - 19s/epoch - 65ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2487 - acc: 0.9004 - auc: 0.9618 - val_loss: 0.2391 - val_acc: 0.9075 - val_auc: 0.9636 - 19s/epoch - 64ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2431 - acc: 0.9018 - auc: 0.9633 - val_loss: 0.2460 - val_acc: 0.9104 - val_auc: 0.9610 - 19s/epoch - 64ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2395 - acc: 0.9029 - auc: 0.9645 - val_loss: 0.2420 - val_acc: 0.9123 - val_auc: 0.9624 - 19s/epoch - 64ms/step
Epoch 15/100
292/292 - 20s - loss: 0.2394 - acc: 0.9013 - auc: 0.9646 - val_loss: 0.2523 - val_acc: 0.9104 - val_auc: 0.9609 - 20s/epoch - 68ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2384 - acc: 0.9000 - auc: 0.9651 - val_loss: 0.2443 - val_acc: 0.9085 - val_auc: 0.9620 - 19s/epoch - 66ms/step
Epoch 17/100
292/292 - 17s - loss: 0.2356 - acc: 0.9023 - auc: 0.9661 - val_loss: 0.2541 - val_acc: 0.9046 - val_auc: 0.9588 - 17s/epoch - 59ms/step
Epoch 18/100
292/292 - 17s - loss: 0.2332 - acc: 0.9048 - auc: 0.9662 - val_loss: 0.2470 - val_acc: 0.9094 - val_auc: 0.9596 - 17s/epoch - 58ms/step
Epoch 19/100
292/292 - 18s - loss: 0.2282 - acc: 0.9069 - auc: 0.9681 - val_loss: 0.2465 - val_acc: 0.9027 - val_auc: 0.9587 - 18s/epoch - 61ms/step
Epoch 20/100
292/292 - 16s - loss: 0.2301 - acc: 0.9069 - auc: 0.9676 - val_loss: 0.2342 - val_acc: 0.9066 - val_auc: 0.9636 - 16s/epoch - 56ms/step
Epoch 21/100
292/292 - 17s - loss: 0.2253 - acc: 0.9094 - auc: 0.9690 - val_loss: 0.2397 - val_acc: 0.9104 - val_auc: 0.9615 - 17s/epoch - 57ms/step
Epoch 22/100
292/292 - 18s - loss: 0.2230 - acc: 0.9085 - auc: 0.9696 - val_loss: 0.2466 - val_acc: 0.9094 - val_auc: 0.9587 - 18s/epoch - 62ms/step
Early stopping epoch: 21
******Evaluating TEST set*********
33/33 - 1s - 882ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.90      0.86      0.88       403
           1       0.91      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.60      0.49      0.54       635

    accuracy                           0.49      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.49      0.49      1038

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3213 - acc: 0.8629 - auc: 0.9350 - val_loss: 0.2570 - val_acc: 0.9017 - val_auc: 0.9585 - 23s/epoch - 78ms/step
Epoch 2/100
292/292 - 17s - loss: 0.2869 - acc: 0.8839 - auc: 0.9472 - val_loss: 0.2602 - val_acc: 0.8854 - val_auc: 0.9601 - 17s/epoch - 59ms/step
Epoch 3/100
292/292 - 17s - loss: 0.2788 - acc: 0.8886 - auc: 0.9499 - val_loss: 0.2446 - val_acc: 0.9085 - val_auc: 0.9639 - 17s/epoch - 58ms/step
Epoch 4/100
292/292 - 17s - loss: 0.2706 - acc: 0.8918 - auc: 0.9529 - val_loss: 0.2598 - val_acc: 0.8844 - val_auc: 0.9622 - 17s/epoch - 59ms/step
Epoch 5/100
292/292 - 16s - loss: 0.2649 - acc: 0.8921 - auc: 0.9555 - val_loss: 0.2269 - val_acc: 0.9104 - val_auc: 0.9683 - 16s/epoch - 56ms/step
Epoch 6/100
292/292 - 17s - loss: 0.2682 - acc: 0.8909 - auc: 0.9545 - val_loss: 0.2278 - val_acc: 0.9075 - val_auc: 0.9678 - 17s/epoch - 57ms/step
Epoch 7/100
292/292 - 16s - loss: 0.2615 - acc: 0.8959 - auc: 0.9564 - val_loss: 0.2272 - val_acc: 0.9104 - val_auc: 0.9690 - 16s/epoch - 56ms/step
Epoch 8/100
292/292 - 17s - loss: 0.2582 - acc: 0.8955 - auc: 0.9578 - val_loss: 0.2284 - val_acc: 0.9133 - val_auc: 0.9689 - 17s/epoch - 57ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2590 - acc: 0.8949 - auc: 0.9576 - val_loss: 0.2250 - val_acc: 0.9056 - val_auc: 0.9701 - 19s/epoch - 63ms/step
Epoch 10/100
292/292 - 16s - loss: 0.2574 - acc: 0.8977 - auc: 0.9579 - val_loss: 0.2381 - val_acc: 0.8979 - val_auc: 0.9678 - 16s/epoch - 56ms/step
Epoch 11/100
292/292 - 16s - loss: 0.2534 - acc: 0.8979 - auc: 0.9596 - val_loss: 0.2229 - val_acc: 0.9056 - val_auc: 0.9710 - 16s/epoch - 56ms/step
Epoch 12/100
292/292 - 16s - loss: 0.2528 - acc: 0.8973 - auc: 0.9601 - val_loss: 0.2245 - val_acc: 0.9046 - val_auc: 0.9709 - 16s/epoch - 56ms/step
Epoch 13/100
292/292 - 16s - loss: 0.2483 - acc: 0.8985 - auc: 0.9614 - val_loss: 0.2236 - val_acc: 0.9104 - val_auc: 0.9706 - 16s/epoch - 56ms/step
Epoch 14/100
292/292 - 16s - loss: 0.2502 - acc: 0.8981 - auc: 0.9610 - val_loss: 0.2187 - val_acc: 0.9017 - val_auc: 0.9718 - 16s/epoch - 56ms/step
Epoch 15/100
292/292 - 16s - loss: 0.2460 - acc: 0.9007 - auc: 0.9625 - val_loss: 0.2210 - val_acc: 0.9056 - val_auc: 0.9723 - 16s/epoch - 56ms/step
Epoch 16/100
292/292 - 16s - loss: 0.2423 - acc: 0.9022 - auc: 0.9636 - val_loss: 0.2489 - val_acc: 0.8911 - val_auc: 0.9650 - 16s/epoch - 56ms/step
Epoch 17/100
292/292 - 16s - loss: 0.2407 - acc: 0.9019 - auc: 0.9643 - val_loss: 0.2180 - val_acc: 0.9114 - val_auc: 0.9722 - 16s/epoch - 56ms/step
Epoch 18/100
292/292 - 16s - loss: 0.2384 - acc: 0.9028 - auc: 0.9645 - val_loss: 0.2265 - val_acc: 0.9037 - val_auc: 0.9694 - 16s/epoch - 56ms/step
Epoch 19/100
292/292 - 16s - loss: 0.2353 - acc: 0.9055 - auc: 0.9656 - val_loss: 0.2298 - val_acc: 0.9085 - val_auc: 0.9688 - 16s/epoch - 56ms/step
Epoch 20/100
292/292 - 16s - loss: 0.2326 - acc: 0.9063 - auc: 0.9663 - val_loss: 0.2237 - val_acc: 0.9085 - val_auc: 0.9708 - 16s/epoch - 56ms/step
Epoch 21/100
292/292 - 17s - loss: 0.2270 - acc: 0.9090 - auc: 0.9680 - val_loss: 0.2311 - val_acc: 0.9066 - val_auc: 0.9691 - 17s/epoch - 58ms/step
Epoch 22/100
292/292 - 16s - loss: 0.2216 - acc: 0.9112 - auc: 0.9696 - val_loss: 0.2773 - val_acc: 0.8873 - val_auc: 0.9574 - 16s/epoch - 56ms/step
Epoch 23/100
292/292 - 16s - loss: 0.2217 - acc: 0.9101 - auc: 0.9700 - val_loss: 0.2264 - val_acc: 0.9114 - val_auc: 0.9693 - 16s/epoch - 56ms/step
Epoch 24/100
292/292 - 19s - loss: 0.2155 - acc: 0.9160 - auc: 0.9710 - val_loss: 0.2463 - val_acc: 0.9046 - val_auc: 0.9650 - 19s/epoch - 64ms/step
Epoch 25/100
292/292 - 16s - loss: 0.2088 - acc: 0.9164 - auc: 0.9728 - val_loss: 0.2456 - val_acc: 0.8969 - val_auc: 0.9664 - 16s/epoch - 56ms/step
Early stopping epoch: 24
******Evaluating TEST set*********
33/33 - 1s - 765ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.92      0.82      0.87       403
           1       0.90      0.96      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.89      0.90      1038
weighted avg       0.91      0.91      0.90      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.46      0.42       403
           1       0.60      0.52      0.56       635

    accuracy                           0.50      1038
   macro avg       0.49      0.49      0.49      1038
weighted avg       0.52      0.50      0.50      1038

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 21s - loss: 0.3177 - acc: 0.8638 - auc: 0.9368 - val_loss: 0.2722 - val_acc: 0.8834 - val_auc: 0.9538 - 21s/epoch - 72ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2860 - acc: 0.8863 - auc: 0.9476 - val_loss: 0.2529 - val_acc: 0.8940 - val_auc: 0.9603 - 21s/epoch - 73ms/step
Epoch 3/100
292/292 - 21s - loss: 0.2783 - acc: 0.8869 - auc: 0.9504 - val_loss: 0.2372 - val_acc: 0.9027 - val_auc: 0.9665 - 21s/epoch - 71ms/step
Epoch 4/100
292/292 - 17s - loss: 0.2749 - acc: 0.8901 - auc: 0.9515 - val_loss: 0.2367 - val_acc: 0.8998 - val_auc: 0.9646 - 17s/epoch - 59ms/step
Epoch 5/100
292/292 - 18s - loss: 0.2690 - acc: 0.8925 - auc: 0.9538 - val_loss: 0.2348 - val_acc: 0.8988 - val_auc: 0.9663 - 18s/epoch - 61ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2643 - acc: 0.8935 - auc: 0.9551 - val_loss: 0.2373 - val_acc: 0.9123 - val_auc: 0.9671 - 21s/epoch - 73ms/step
Epoch 7/100
292/292 - 22s - loss: 0.2649 - acc: 0.8955 - auc: 0.9554 - val_loss: 0.2489 - val_acc: 0.8921 - val_auc: 0.9636 - 22s/epoch - 74ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2603 - acc: 0.8948 - auc: 0.9576 - val_loss: 0.2265 - val_acc: 0.9037 - val_auc: 0.9681 - 21s/epoch - 71ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2595 - acc: 0.8940 - auc: 0.9573 - val_loss: 0.2317 - val_acc: 0.9075 - val_auc: 0.9667 - 21s/epoch - 71ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2583 - acc: 0.8955 - auc: 0.9579 - val_loss: 0.2209 - val_acc: 0.9133 - val_auc: 0.9699 - 21s/epoch - 70ms/step
Epoch 11/100
292/292 - 20s - loss: 0.2551 - acc: 0.8955 - auc: 0.9590 - val_loss: 0.2211 - val_acc: 0.9094 - val_auc: 0.9696 - 20s/epoch - 67ms/step
Epoch 12/100
292/292 - 20s - loss: 0.2551 - acc: 0.8977 - auc: 0.9590 - val_loss: 0.2255 - val_acc: 0.9123 - val_auc: 0.9693 - 20s/epoch - 67ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2510 - acc: 0.8986 - auc: 0.9605 - val_loss: 0.2199 - val_acc: 0.9133 - val_auc: 0.9713 - 19s/epoch - 65ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2506 - acc: 0.8966 - auc: 0.9607 - val_loss: 0.2162 - val_acc: 0.9210 - val_auc: 0.9714 - 19s/epoch - 64ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2473 - acc: 0.9011 - auc: 0.9618 - val_loss: 0.2148 - val_acc: 0.9162 - val_auc: 0.9727 - 19s/epoch - 66ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2467 - acc: 0.9005 - auc: 0.9623 - val_loss: 0.2185 - val_acc: 0.9075 - val_auc: 0.9711 - 19s/epoch - 65ms/step
Epoch 17/100
292/292 - 19s - loss: 0.2437 - acc: 0.8994 - auc: 0.9630 - val_loss: 0.2220 - val_acc: 0.9123 - val_auc: 0.9698 - 19s/epoch - 67ms/step
Epoch 18/100
292/292 - 20s - loss: 0.2433 - acc: 0.9018 - auc: 0.9629 - val_loss: 0.2103 - val_acc: 0.9191 - val_auc: 0.9732 - 20s/epoch - 69ms/step
Epoch 19/100
292/292 - 20s - loss: 0.2407 - acc: 0.9025 - auc: 0.9641 - val_loss: 0.2148 - val_acc: 0.9171 - val_auc: 0.9725 - 20s/epoch - 69ms/step
Epoch 20/100
292/292 - 18s - loss: 0.2362 - acc: 0.9045 - auc: 0.9651 - val_loss: 0.2182 - val_acc: 0.9104 - val_auc: 0.9712 - 18s/epoch - 63ms/step
Epoch 21/100
292/292 - 17s - loss: 0.2337 - acc: 0.9056 - auc: 0.9663 - val_loss: 0.2094 - val_acc: 0.9123 - val_auc: 0.9743 - 17s/epoch - 57ms/step
Epoch 22/100
292/292 - 16s - loss: 0.2301 - acc: 0.9060 - auc: 0.9670 - val_loss: 0.2139 - val_acc: 0.9162 - val_auc: 0.9735 - 16s/epoch - 55ms/step
Epoch 23/100
292/292 - 16s - loss: 0.2287 - acc: 0.9066 - auc: 0.9678 - val_loss: 0.2121 - val_acc: 0.9133 - val_auc: 0.9731 - 16s/epoch - 56ms/step
Epoch 24/100
292/292 - 16s - loss: 0.2273 - acc: 0.9083 - auc: 0.9681 - val_loss: 0.2160 - val_acc: 0.9104 - val_auc: 0.9717 - 16s/epoch - 56ms/step
Epoch 25/100
292/292 - 16s - loss: 0.2201 - acc: 0.9137 - auc: 0.9696 - val_loss: 0.2299 - val_acc: 0.9046 - val_auc: 0.9701 - 16s/epoch - 55ms/step
Epoch 26/100
292/292 - 16s - loss: 0.2186 - acc: 0.9121 - auc: 0.9700 - val_loss: 0.2216 - val_acc: 0.9027 - val_auc: 0.9714 - 16s/epoch - 55ms/step
Epoch 27/100
292/292 - 17s - loss: 0.2139 - acc: 0.9164 - auc: 0.9717 - val_loss: 0.2254 - val_acc: 0.9094 - val_auc: 0.9716 - 17s/epoch - 59ms/step
Epoch 28/100
292/292 - 21s - loss: 0.2098 - acc: 0.9174 - auc: 0.9726 - val_loss: 0.2274 - val_acc: 0.8931 - val_auc: 0.9686 - 21s/epoch - 72ms/step
Epoch 29/100
292/292 - 20s - loss: 0.2033 - acc: 0.9191 - auc: 0.9742 - val_loss: 0.2205 - val_acc: 0.9085 - val_auc: 0.9705 - 20s/epoch - 70ms/step
Epoch 30/100
292/292 - 20s - loss: 0.1999 - acc: 0.9229 - auc: 0.9749 - val_loss: 0.2372 - val_acc: 0.9037 - val_auc: 0.9673 - 20s/epoch - 69ms/step
Epoch 31/100
292/292 - 20s - loss: 0.1935 - acc: 0.9222 - auc: 0.9767 - val_loss: 0.2291 - val_acc: 0.9143 - val_auc: 0.9696 - 20s/epoch - 70ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
33/33 - 1s - 837ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.90      0.87      0.89       403
           1       0.92      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.91      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.52      0.44       403
           1       0.61      0.48      0.54       635

    accuracy                           0.50      1038
   macro avg       0.50      0.50      0.49      1038
weighted avg       0.53      0.50      0.50      1038

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3183 - acc: 0.8674 - auc: 0.9362 - val_loss: 0.2886 - val_acc: 0.8795 - val_auc: 0.9542 - 22s/epoch - 76ms/step
Epoch 2/100
292/292 - 20s - loss: 0.2840 - acc: 0.8840 - auc: 0.9480 - val_loss: 0.2745 - val_acc: 0.8814 - val_auc: 0.9554 - 20s/epoch - 69ms/step
Epoch 3/100
292/292 - 20s - loss: 0.2698 - acc: 0.8921 - auc: 0.9530 - val_loss: 0.2684 - val_acc: 0.8891 - val_auc: 0.9605 - 20s/epoch - 69ms/step
Epoch 4/100
292/292 - 20s - loss: 0.2655 - acc: 0.8937 - auc: 0.9549 - val_loss: 0.2556 - val_acc: 0.8920 - val_auc: 0.9614 - 20s/epoch - 68ms/step
Epoch 5/100
292/292 - 19s - loss: 0.2611 - acc: 0.8944 - auc: 0.9565 - val_loss: 0.2521 - val_acc: 0.8949 - val_auc: 0.9629 - 19s/epoch - 64ms/step
Epoch 6/100
292/292 - 19s - loss: 0.2617 - acc: 0.8949 - auc: 0.9558 - val_loss: 0.2675 - val_acc: 0.8833 - val_auc: 0.9587 - 19s/epoch - 64ms/step
Epoch 7/100
292/292 - 19s - loss: 0.2574 - acc: 0.8972 - auc: 0.9581 - val_loss: 0.2494 - val_acc: 0.8959 - val_auc: 0.9653 - 19s/epoch - 64ms/step
Epoch 8/100
292/292 - 19s - loss: 0.2542 - acc: 0.8976 - auc: 0.9591 - val_loss: 0.2513 - val_acc: 0.8959 - val_auc: 0.9643 - 19s/epoch - 64ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2513 - acc: 0.8992 - auc: 0.9599 - val_loss: 0.2553 - val_acc: 0.8959 - val_auc: 0.9631 - 19s/epoch - 64ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2485 - acc: 0.8990 - auc: 0.9612 - val_loss: 0.2543 - val_acc: 0.8930 - val_auc: 0.9639 - 19s/epoch - 64ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2475 - acc: 0.9010 - auc: 0.9617 - val_loss: 0.2641 - val_acc: 0.8901 - val_auc: 0.9612 - 19s/epoch - 64ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2468 - acc: 0.8998 - auc: 0.9617 - val_loss: 0.2555 - val_acc: 0.8901 - val_auc: 0.9625 - 19s/epoch - 64ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2439 - acc: 0.9020 - auc: 0.9623 - val_loss: 0.2485 - val_acc: 0.8939 - val_auc: 0.9647 - 19s/epoch - 64ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2404 - acc: 0.9016 - auc: 0.9637 - val_loss: 0.2489 - val_acc: 0.8968 - val_auc: 0.9644 - 19s/epoch - 65ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2365 - acc: 0.9025 - auc: 0.9653 - val_loss: 0.2582 - val_acc: 0.8833 - val_auc: 0.9625 - 19s/epoch - 64ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2337 - acc: 0.9047 - auc: 0.9659 - val_loss: 0.2473 - val_acc: 0.8920 - val_auc: 0.9636 - 19s/epoch - 64ms/step
Epoch 17/100
292/292 - 19s - loss: 0.2320 - acc: 0.9068 - auc: 0.9665 - val_loss: 0.2481 - val_acc: 0.8959 - val_auc: 0.9664 - 19s/epoch - 65ms/step
Epoch 18/100
292/292 - 19s - loss: 0.2304 - acc: 0.9075 - auc: 0.9671 - val_loss: 0.2542 - val_acc: 0.8901 - val_auc: 0.9636 - 19s/epoch - 64ms/step
Epoch 19/100
292/292 - 19s - loss: 0.2286 - acc: 0.9074 - auc: 0.9675 - val_loss: 0.2517 - val_acc: 0.8910 - val_auc: 0.9632 - 19s/epoch - 65ms/step
Epoch 20/100
292/292 - 19s - loss: 0.2258 - acc: 0.9090 - auc: 0.9687 - val_loss: 0.2635 - val_acc: 0.8949 - val_auc: 0.9625 - 19s/epoch - 66ms/step
Epoch 21/100
292/292 - 19s - loss: 0.2255 - acc: 0.9076 - auc: 0.9688 - val_loss: 0.2485 - val_acc: 0.8930 - val_auc: 0.9635 - 19s/epoch - 65ms/step
Epoch 22/100
292/292 - 19s - loss: 0.2168 - acc: 0.9126 - auc: 0.9709 - val_loss: 0.2542 - val_acc: 0.8852 - val_auc: 0.9609 - 19s/epoch - 65ms/step
Epoch 23/100
292/292 - 19s - loss: 0.2141 - acc: 0.9141 - auc: 0.9717 - val_loss: 0.2764 - val_acc: 0.8939 - val_auc: 0.9608 - 19s/epoch - 65ms/step
Epoch 24/100
292/292 - 19s - loss: 0.2113 - acc: 0.9150 - auc: 0.9725 - val_loss: 0.2647 - val_acc: 0.8862 - val_auc: 0.9611 - 19s/epoch - 66ms/step
Epoch 25/100
292/292 - 19s - loss: 0.2058 - acc: 0.9149 - auc: 0.9734 - val_loss: 0.2602 - val_acc: 0.8833 - val_auc: 0.9603 - 19s/epoch - 64ms/step
Epoch 26/100
292/292 - 19s - loss: 0.2052 - acc: 0.9171 - auc: 0.9743 - val_loss: 0.2713 - val_acc: 0.8862 - val_auc: 0.9584 - 19s/epoch - 65ms/step
Epoch 27/100
292/292 - 19s - loss: 0.2044 - acc: 0.9184 - auc: 0.9745 - val_loss: 0.2751 - val_acc: 0.8891 - val_auc: 0.9580 - 19s/epoch - 64ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
33/33 - 1s - 771ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.88      0.85      0.86       403
           1       0.91      0.92      0.92       634

    accuracy                           0.90      1037
   macro avg       0.89      0.89      0.89      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.61      0.48      0.54       634

    accuracy                           0.49      1037
   macro avg       0.50      0.50      0.49      1037
weighted avg       0.52      0.49      0.50      1037

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 21s - loss: 0.3112 - acc: 0.8662 - auc: 0.9392 - val_loss: 0.3129 - val_acc: 0.8727 - val_auc: 0.9369 - 21s/epoch - 73ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2754 - acc: 0.8918 - auc: 0.9514 - val_loss: 0.3118 - val_acc: 0.8852 - val_auc: 0.9398 - 19s/epoch - 65ms/step
Epoch 3/100
292/292 - 20s - loss: 0.2656 - acc: 0.8927 - auc: 0.9550 - val_loss: 0.2956 - val_acc: 0.8852 - val_auc: 0.9440 - 20s/epoch - 68ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2611 - acc: 0.8960 - auc: 0.9564 - val_loss: 0.2979 - val_acc: 0.8814 - val_auc: 0.9448 - 21s/epoch - 71ms/step
Epoch 5/100
292/292 - 17s - loss: 0.2592 - acc: 0.8959 - auc: 0.9574 - val_loss: 0.2998 - val_acc: 0.8910 - val_auc: 0.9446 - 17s/epoch - 57ms/step
Epoch 6/100
292/292 - 20s - loss: 0.2576 - acc: 0.8974 - auc: 0.9582 - val_loss: 0.2863 - val_acc: 0.8891 - val_auc: 0.9472 - 20s/epoch - 70ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2537 - acc: 0.8979 - auc: 0.9594 - val_loss: 0.2890 - val_acc: 0.8824 - val_auc: 0.9460 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 20s - loss: 0.2532 - acc: 0.8982 - auc: 0.9599 - val_loss: 0.2842 - val_acc: 0.8852 - val_auc: 0.9504 - 20s/epoch - 67ms/step
Epoch 9/100
292/292 - 17s - loss: 0.2499 - acc: 0.8997 - auc: 0.9611 - val_loss: 0.3200 - val_acc: 0.8862 - val_auc: 0.9356 - 17s/epoch - 57ms/step
Epoch 10/100
292/292 - 17s - loss: 0.2522 - acc: 0.8970 - auc: 0.9601 - val_loss: 0.2827 - val_acc: 0.8862 - val_auc: 0.9500 - 17s/epoch - 57ms/step
Epoch 11/100
292/292 - 16s - loss: 0.2451 - acc: 0.8985 - auc: 0.9624 - val_loss: 0.2763 - val_acc: 0.8833 - val_auc: 0.9516 - 16s/epoch - 56ms/step
Epoch 12/100
292/292 - 16s - loss: 0.2456 - acc: 0.8991 - auc: 0.9627 - val_loss: 0.2874 - val_acc: 0.8872 - val_auc: 0.9492 - 16s/epoch - 56ms/step
Epoch 13/100
292/292 - 17s - loss: 0.2393 - acc: 0.9030 - auc: 0.9645 - val_loss: 0.2820 - val_acc: 0.8881 - val_auc: 0.9493 - 17s/epoch - 57ms/step
Epoch 14/100
292/292 - 16s - loss: 0.2388 - acc: 0.9049 - auc: 0.9646 - val_loss: 0.2842 - val_acc: 0.8785 - val_auc: 0.9503 - 16s/epoch - 56ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2353 - acc: 0.9042 - auc: 0.9659 - val_loss: 0.2820 - val_acc: 0.8901 - val_auc: 0.9477 - 17s/epoch - 57ms/step
Epoch 16/100
292/292 - 17s - loss: 0.2364 - acc: 0.9035 - auc: 0.9654 - val_loss: 0.2784 - val_acc: 0.8862 - val_auc: 0.9513 - 17s/epoch - 57ms/step
Epoch 17/100
292/292 - 21s - loss: 0.2283 - acc: 0.9065 - auc: 0.9674 - val_loss: 0.2864 - val_acc: 0.8852 - val_auc: 0.9490 - 21s/epoch - 72ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2285 - acc: 0.9089 - auc: 0.9675 - val_loss: 0.2775 - val_acc: 0.8872 - val_auc: 0.9516 - 21s/epoch - 71ms/step
Epoch 19/100
292/292 - 21s - loss: 0.2242 - acc: 0.9068 - auc: 0.9690 - val_loss: 0.3095 - val_acc: 0.8746 - val_auc: 0.9410 - 21s/epoch - 71ms/step
Epoch 20/100
292/292 - 21s - loss: 0.2252 - acc: 0.9085 - auc: 0.9685 - val_loss: 0.2996 - val_acc: 0.8872 - val_auc: 0.9450 - 21s/epoch - 71ms/step
Epoch 21/100
292/292 - 20s - loss: 0.2185 - acc: 0.9096 - auc: 0.9703 - val_loss: 0.2961 - val_acc: 0.8881 - val_auc: 0.9457 - 20s/epoch - 70ms/step
Epoch 22/100
292/292 - 22s - loss: 0.2175 - acc: 0.9108 - auc: 0.9711 - val_loss: 0.2787 - val_acc: 0.8939 - val_auc: 0.9494 - 22s/epoch - 74ms/step
Epoch 23/100
292/292 - 21s - loss: 0.2138 - acc: 0.9134 - auc: 0.9720 - val_loss: 0.3180 - val_acc: 0.8843 - val_auc: 0.9385 - 21s/epoch - 74ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2122 - acc: 0.9135 - auc: 0.9721 - val_loss: 0.3222 - val_acc: 0.8679 - val_auc: 0.9393 - 21s/epoch - 70ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2087 - acc: 0.9156 - auc: 0.9731 - val_loss: 0.2920 - val_acc: 0.8910 - val_auc: 0.9466 - 21s/epoch - 71ms/step
Epoch 26/100
292/292 - 20s - loss: 0.2016 - acc: 0.9195 - auc: 0.9746 - val_loss: 0.3167 - val_acc: 0.8824 - val_auc: 0.9398 - 20s/epoch - 68ms/step
Epoch 27/100
292/292 - 20s - loss: 0.1988 - acc: 0.9199 - auc: 0.9753 - val_loss: 0.3002 - val_acc: 0.8824 - val_auc: 0.9436 - 20s/epoch - 67ms/step
Epoch 28/100
292/292 - 20s - loss: 0.1933 - acc: 0.9227 - auc: 0.9766 - val_loss: 0.2892 - val_acc: 0.8939 - val_auc: 0.9454 - 20s/epoch - 67ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
33/33 - 1s - 823ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.87      0.83      0.85       403
           1       0.90      0.92      0.91       634

    accuracy                           0.89      1037
   macro avg       0.88      0.88      0.88      1037
weighted avg       0.89      0.89      0.89      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.62      0.50      0.56       634

    accuracy                           0.51      1037
   macro avg       0.51      0.51      0.50      1037
weighted avg       0.53      0.51      0.51      1037

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3221 - acc: 0.8627 - auc: 0.9348 - val_loss: 0.2672 - val_acc: 0.8978 - val_auc: 0.9521 - 22s/epoch - 75ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2848 - acc: 0.8826 - auc: 0.9486 - val_loss: 0.2539 - val_acc: 0.9007 - val_auc: 0.9560 - 19s/epoch - 64ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2747 - acc: 0.8899 - auc: 0.9512 - val_loss: 0.2648 - val_acc: 0.9007 - val_auc: 0.9554 - 19s/epoch - 64ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2670 - acc: 0.8931 - auc: 0.9545 - val_loss: 0.2674 - val_acc: 0.8881 - val_auc: 0.9562 - 19s/epoch - 64ms/step
Epoch 5/100
292/292 - 19s - loss: 0.2641 - acc: 0.8934 - auc: 0.9555 - val_loss: 0.2491 - val_acc: 0.9016 - val_auc: 0.9584 - 19s/epoch - 64ms/step
Epoch 6/100
292/292 - 19s - loss: 0.2621 - acc: 0.8960 - auc: 0.9566 - val_loss: 0.2474 - val_acc: 0.9055 - val_auc: 0.9613 - 19s/epoch - 64ms/step
Epoch 7/100
292/292 - 19s - loss: 0.2571 - acc: 0.8962 - auc: 0.9583 - val_loss: 0.2552 - val_acc: 0.9016 - val_auc: 0.9573 - 19s/epoch - 64ms/step
Epoch 8/100
292/292 - 19s - loss: 0.2552 - acc: 0.8985 - auc: 0.9586 - val_loss: 0.2501 - val_acc: 0.9045 - val_auc: 0.9612 - 19s/epoch - 64ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2532 - acc: 0.8986 - auc: 0.9596 - val_loss: 0.2584 - val_acc: 0.9007 - val_auc: 0.9566 - 19s/epoch - 64ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2475 - acc: 0.8983 - auc: 0.9618 - val_loss: 0.2512 - val_acc: 0.9007 - val_auc: 0.9599 - 19s/epoch - 64ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2482 - acc: 0.8992 - auc: 0.9612 - val_loss: 0.2674 - val_acc: 0.9007 - val_auc: 0.9549 - 19s/epoch - 64ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2441 - acc: 0.8998 - auc: 0.9630 - val_loss: 0.2553 - val_acc: 0.8901 - val_auc: 0.9600 - 19s/epoch - 64ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2465 - acc: 0.8976 - auc: 0.9622 - val_loss: 0.2525 - val_acc: 0.8968 - val_auc: 0.9594 - 19s/epoch - 64ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2397 - acc: 0.9027 - auc: 0.9643 - val_loss: 0.2538 - val_acc: 0.9007 - val_auc: 0.9598 - 19s/epoch - 65ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2391 - acc: 0.9039 - auc: 0.9647 - val_loss: 0.2608 - val_acc: 0.8959 - val_auc: 0.9577 - 19s/epoch - 65ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2361 - acc: 0.9053 - auc: 0.9650 - val_loss: 0.2545 - val_acc: 0.9036 - val_auc: 0.9594 - 19s/epoch - 64ms/step
Early stopping epoch: 15
******Evaluating TEST set*********
33/33 - 1s - 830ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.89      0.87      0.88       403
           1       0.92      0.93      0.92       634

    accuracy                           0.91      1037
   macro avg       0.90      0.90      0.90      1037
weighted avg       0.91      0.91      0.91      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.61      0.51      0.55       634

    accuracy                           0.50      1037
   macro avg       0.49      0.49      0.49      1037
weighted avg       0.52      0.50      0.50      1037

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3100 - acc: 0.8703 - auc: 0.9390 - val_loss: 0.3038 - val_acc: 0.8746 - val_auc: 0.9429 - 22s/epoch - 75ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2824 - acc: 0.8852 - auc: 0.9489 - val_loss: 0.3017 - val_acc: 0.8640 - val_auc: 0.9449 - 19s/epoch - 65ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2736 - acc: 0.8894 - auc: 0.9521 - val_loss: 0.2898 - val_acc: 0.8775 - val_auc: 0.9489 - 19s/epoch - 64ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2628 - acc: 0.8961 - auc: 0.9558 - val_loss: 0.2931 - val_acc: 0.8717 - val_auc: 0.9478 - 19s/epoch - 66ms/step
Epoch 5/100
292/292 - 20s - loss: 0.2594 - acc: 0.8981 - auc: 0.9571 - val_loss: 0.2893 - val_acc: 0.8795 - val_auc: 0.9487 - 20s/epoch - 68ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2563 - acc: 0.8974 - auc: 0.9587 - val_loss: 0.2936 - val_acc: 0.8775 - val_auc: 0.9510 - 21s/epoch - 71ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2549 - acc: 0.8988 - auc: 0.9588 - val_loss: 0.2893 - val_acc: 0.8814 - val_auc: 0.9483 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 20s - loss: 0.2520 - acc: 0.8985 - auc: 0.9594 - val_loss: 0.2944 - val_acc: 0.8766 - val_auc: 0.9513 - 20s/epoch - 69ms/step
Epoch 9/100
292/292 - 20s - loss: 0.2495 - acc: 0.8990 - auc: 0.9611 - val_loss: 0.2842 - val_acc: 0.8824 - val_auc: 0.9515 - 20s/epoch - 67ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2477 - acc: 0.9002 - auc: 0.9615 - val_loss: 0.2881 - val_acc: 0.8785 - val_auc: 0.9505 - 19s/epoch - 65ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2447 - acc: 0.9014 - auc: 0.9621 - val_loss: 0.3038 - val_acc: 0.8737 - val_auc: 0.9477 - 19s/epoch - 65ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2430 - acc: 0.9013 - auc: 0.9633 - val_loss: 0.2952 - val_acc: 0.8746 - val_auc: 0.9491 - 19s/epoch - 65ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2396 - acc: 0.9023 - auc: 0.9645 - val_loss: 0.2964 - val_acc: 0.8727 - val_auc: 0.9481 - 19s/epoch - 65ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2393 - acc: 0.9031 - auc: 0.9642 - val_loss: 0.2868 - val_acc: 0.8795 - val_auc: 0.9514 - 19s/epoch - 66ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2337 - acc: 0.9035 - auc: 0.9661 - val_loss: 0.2908 - val_acc: 0.8669 - val_auc: 0.9501 - 19s/epoch - 64ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2303 - acc: 0.9059 - auc: 0.9672 - val_loss: 0.2915 - val_acc: 0.8708 - val_auc: 0.9504 - 19s/epoch - 66ms/step
Epoch 17/100
292/292 - 19s - loss: 0.2308 - acc: 0.9077 - auc: 0.9670 - val_loss: 0.2939 - val_acc: 0.8717 - val_auc: 0.9504 - 19s/epoch - 65ms/step
Epoch 18/100
292/292 - 19s - loss: 0.2296 - acc: 0.9070 - auc: 0.9671 - val_loss: 0.3040 - val_acc: 0.8582 - val_auc: 0.9451 - 19s/epoch - 65ms/step
Epoch 19/100
292/292 - 19s - loss: 0.2264 - acc: 0.9094 - auc: 0.9683 - val_loss: 0.3011 - val_acc: 0.8640 - val_auc: 0.9480 - 19s/epoch - 66ms/step
Early stopping epoch: 18
******Evaluating TEST set*********
33/33 - 1s - 802ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.86      0.83      0.85       403
           1       0.89      0.92      0.91       634

    accuracy                           0.88      1037
   macro avg       0.88      0.87      0.88      1037
weighted avg       0.88      0.88      0.88      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.55      0.47       403
           1       0.63      0.49      0.55       634

    accuracy                           0.51      1037
   macro avg       0.52      0.52      0.51      1037
weighted avg       0.54      0.51      0.52      1037

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 21s - loss: 0.3153 - acc: 0.8667 - auc: 0.9378 - val_loss: 0.3165 - val_acc: 0.8679 - val_auc: 0.9374 - 21s/epoch - 73ms/step
Epoch 2/100
292/292 - 19s - loss: 0.2786 - acc: 0.8877 - auc: 0.9506 - val_loss: 0.2881 - val_acc: 0.8901 - val_auc: 0.9443 - 19s/epoch - 66ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2707 - acc: 0.8916 - auc: 0.9530 - val_loss: 0.2765 - val_acc: 0.8920 - val_auc: 0.9497 - 19s/epoch - 65ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2658 - acc: 0.8955 - auc: 0.9551 - val_loss: 0.2785 - val_acc: 0.8978 - val_auc: 0.9479 - 19s/epoch - 65ms/step
Epoch 5/100
292/292 - 19s - loss: 0.2606 - acc: 0.8925 - auc: 0.9573 - val_loss: 0.2733 - val_acc: 0.9026 - val_auc: 0.9509 - 19s/epoch - 65ms/step
Epoch 6/100
292/292 - 19s - loss: 0.2566 - acc: 0.8976 - auc: 0.9588 - val_loss: 0.2789 - val_acc: 0.8949 - val_auc: 0.9486 - 19s/epoch - 65ms/step
Epoch 7/100
292/292 - 19s - loss: 0.2569 - acc: 0.8967 - auc: 0.9589 - val_loss: 0.2681 - val_acc: 0.8968 - val_auc: 0.9522 - 19s/epoch - 66ms/step
Epoch 8/100
292/292 - 19s - loss: 0.2547 - acc: 0.8956 - auc: 0.9595 - val_loss: 0.2661 - val_acc: 0.8968 - val_auc: 0.9513 - 19s/epoch - 66ms/step
Epoch 9/100
292/292 - 19s - loss: 0.2538 - acc: 0.8948 - auc: 0.9604 - val_loss: 0.2656 - val_acc: 0.9045 - val_auc: 0.9518 - 19s/epoch - 66ms/step
Epoch 10/100
292/292 - 19s - loss: 0.2480 - acc: 0.9005 - auc: 0.9619 - val_loss: 0.2688 - val_acc: 0.9007 - val_auc: 0.9510 - 19s/epoch - 64ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2461 - acc: 0.8991 - auc: 0.9625 - val_loss: 0.2671 - val_acc: 0.8968 - val_auc: 0.9555 - 19s/epoch - 65ms/step
Epoch 12/100
292/292 - 19s - loss: 0.2428 - acc: 0.8999 - auc: 0.9638 - val_loss: 0.2619 - val_acc: 0.9036 - val_auc: 0.9546 - 19s/epoch - 66ms/step
Epoch 13/100
292/292 - 19s - loss: 0.2446 - acc: 0.8989 - auc: 0.9631 - val_loss: 0.2707 - val_acc: 0.8959 - val_auc: 0.9506 - 19s/epoch - 65ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2409 - acc: 0.9013 - auc: 0.9641 - val_loss: 0.2588 - val_acc: 0.9016 - val_auc: 0.9559 - 19s/epoch - 65ms/step
Epoch 15/100
292/292 - 19s - loss: 0.2373 - acc: 0.9032 - auc: 0.9651 - val_loss: 0.2587 - val_acc: 0.8997 - val_auc: 0.9555 - 19s/epoch - 65ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2400 - acc: 0.9010 - auc: 0.9643 - val_loss: 0.2603 - val_acc: 0.8987 - val_auc: 0.9553 - 19s/epoch - 65ms/step
Epoch 17/100
292/292 - 19s - loss: 0.2324 - acc: 0.9047 - auc: 0.9669 - val_loss: 0.2551 - val_acc: 0.9026 - val_auc: 0.9564 - 19s/epoch - 65ms/step
Epoch 18/100
292/292 - 19s - loss: 0.2338 - acc: 0.9049 - auc: 0.9663 - val_loss: 0.2651 - val_acc: 0.9084 - val_auc: 0.9550 - 19s/epoch - 66ms/step
Epoch 19/100
292/292 - 19s - loss: 0.2304 - acc: 0.9089 - auc: 0.9668 - val_loss: 0.2585 - val_acc: 0.8987 - val_auc: 0.9561 - 19s/epoch - 65ms/step
Epoch 20/100
292/292 - 19s - loss: 0.2246 - acc: 0.9092 - auc: 0.9686 - val_loss: 0.2707 - val_acc: 0.9026 - val_auc: 0.9527 - 19s/epoch - 65ms/step
Epoch 21/100
292/292 - 20s - loss: 0.2248 - acc: 0.9096 - auc: 0.9684 - val_loss: 0.2670 - val_acc: 0.9065 - val_auc: 0.9546 - 20s/epoch - 67ms/step
Epoch 22/100
292/292 - 19s - loss: 0.2234 - acc: 0.9077 - auc: 0.9693 - val_loss: 0.2662 - val_acc: 0.9055 - val_auc: 0.9537 - 19s/epoch - 66ms/step
Epoch 23/100
292/292 - 17s - loss: 0.2204 - acc: 0.9075 - auc: 0.9703 - val_loss: 0.2642 - val_acc: 0.9055 - val_auc: 0.9537 - 17s/epoch - 57ms/step
Epoch 24/100
292/292 - 18s - loss: 0.2179 - acc: 0.9137 - auc: 0.9704 - val_loss: 0.2750 - val_acc: 0.9016 - val_auc: 0.9510 - 18s/epoch - 62ms/step
Epoch 25/100
292/292 - 19s - loss: 0.2127 - acc: 0.9112 - auc: 0.9722 - val_loss: 0.2575 - val_acc: 0.9055 - val_auc: 0.9566 - 19s/epoch - 65ms/step
Epoch 26/100
292/292 - 21s - loss: 0.2116 - acc: 0.9154 - auc: 0.9722 - val_loss: 0.2544 - val_acc: 0.9016 - val_auc: 0.9587 - 21s/epoch - 71ms/step
Epoch 27/100
292/292 - 21s - loss: 0.2059 - acc: 0.9198 - auc: 0.9739 - val_loss: 0.2581 - val_acc: 0.9055 - val_auc: 0.9561 - 21s/epoch - 72ms/step
Epoch 28/100
292/292 - 21s - loss: 0.1990 - acc: 0.9187 - auc: 0.9759 - val_loss: 0.2735 - val_acc: 0.9055 - val_auc: 0.9555 - 21s/epoch - 71ms/step
Epoch 29/100
292/292 - 20s - loss: 0.1979 - acc: 0.9203 - auc: 0.9763 - val_loss: 0.2627 - val_acc: 0.8968 - val_auc: 0.9577 - 20s/epoch - 68ms/step
Epoch 30/100
292/292 - 20s - loss: 0.1937 - acc: 0.9235 - auc: 0.9770 - val_loss: 0.2638 - val_acc: 0.8959 - val_auc: 0.9556 - 20s/epoch - 68ms/step
Epoch 31/100
292/292 - 20s - loss: 0.1852 - acc: 0.9243 - auc: 0.9793 - val_loss: 0.2781 - val_acc: 0.8997 - val_auc: 0.9514 - 20s/epoch - 67ms/step
Epoch 32/100
292/292 - 19s - loss: 0.1865 - acc: 0.9259 - auc: 0.9789 - val_loss: 0.2713 - val_acc: 0.9016 - val_auc: 0.9535 - 19s/epoch - 67ms/step
Epoch 33/100
292/292 - 19s - loss: 0.1822 - acc: 0.9284 - auc: 0.9794 - val_loss: 0.2886 - val_acc: 0.8968 - val_auc: 0.9518 - 19s/epoch - 66ms/step
Epoch 34/100
292/292 - 19s - loss: 0.1706 - acc: 0.9331 - auc: 0.9819 - val_loss: 0.2761 - val_acc: 0.9026 - val_auc: 0.9497 - 19s/epoch - 66ms/step
Epoch 35/100
292/292 - 19s - loss: 0.1650 - acc: 0.9338 - auc: 0.9836 - val_loss: 0.2886 - val_acc: 0.9007 - val_auc: 0.9526 - 19s/epoch - 66ms/step
Epoch 36/100
292/292 - 19s - loss: 0.1595 - acc: 0.9369 - auc: 0.9845 - val_loss: 0.2963 - val_acc: 0.8959 - val_auc: 0.9525 - 19s/epoch - 65ms/step
Early stopping epoch: 35
******Evaluating TEST set*********
33/33 - 1s - 934ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.90      0.84      0.87       403
           1       0.90      0.94      0.92       634

    accuracy                           0.90      1037
   macro avg       0.90      0.89      0.90      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.45      0.41       403
           1       0.60      0.52      0.56       634

    accuracy                           0.49      1037
   macro avg       0.49      0.49      0.48      1037
weighted avg       0.51      0.49      0.50      1037

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8996] IC [0.8929, 0.9062]
Mean Recall[0.8900] IC [0.8828, 0.8972]
Mean F1[0.8933] IC [0.8863, 0.9004]
Median Accuracy[0.9036]
Median Recall[0.8904]
Median F1[0.8968]
