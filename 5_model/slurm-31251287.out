fold 0
Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 2772, 6, 3)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d (Conv2D)                 │ (None, 2772, 6, 32)    │         1,760 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (Conv2D)               │ (None, 2772, 6, 64)    │        36,928 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d (MaxPooling2D)    │ (None, 2772, 1, 64)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda (Lambda)                 │ (None, 2772, 64)       │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 2772, 64),     │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention (SelfAttention)  │ [(None, 1024), (None,  │         2,560 │
│                                 │ 16, 2772)]             │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 76,322 (298.13 KB)
 Trainable params: 76,322 (298.13 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 549s - 2s/step - acc: 0.0000e+00 - loss: 2.1974 - val_acc: 0.0000e+00 - val_loss: 3.6414
Epoch 2/100
292/292 - 558s - 2s/step - acc: 0.0000e+00 - loss: 1.6701 - val_acc: 0.0000e+00 - val_loss: 2.8879
Epoch 3/100
292/292 - 557s - 2s/step - acc: 0.0000e+00 - loss: 1.8258 - val_acc: 0.0000e+00 - val_loss: 2.6236
Epoch 4/100
292/292 - 565s - 2s/step - acc: 0.0000e+00 - loss: 2.1092 - val_acc: 0.0000e+00 - val_loss: 1.4057
Epoch 5/100
292/292 - 558s - 2s/step - acc: 0.0000e+00 - loss: 0.9807 - val_acc: 0.0000e+00 - val_loss: 1.2390
Epoch 6/100
292/292 - 565s - 2s/step - acc: 0.0000e+00 - loss: 0.8558 - val_acc: 0.0000e+00 - val_loss: 0.3865
Epoch 7/100
292/292 - 571s - 2s/step - acc: 0.0000e+00 - loss: 0.5100 - val_acc: 0.0000e+00 - val_loss: 0.3385
Epoch 8/100
292/292 - 569s - 2s/step - acc: 0.0000e+00 - loss: 0.3307 - val_acc: 0.0000e+00 - val_loss: 0.2580
Epoch 9/100
292/292 - 567s - 2s/step - acc: 0.0000e+00 - loss: 0.3368 - val_acc: 0.0000e+00 - val_loss: 0.4039
Epoch 10/100
292/292 - 566s - 2s/step - acc: 0.0000e+00 - loss: 0.3107 - val_acc: 0.0000e+00 - val_loss: 0.2607
Epoch 11/100
292/292 - 556s - 2s/step - acc: 0.0000e+00 - loss: 0.3313 - val_acc: 0.0000e+00 - val_loss: 0.2452
Epoch 12/100
292/292 - 554s - 2s/step - acc: 0.0000e+00 - loss: 0.2847 - val_acc: 0.0000e+00 - val_loss: 0.2613
Epoch 13/100
292/292 - 558s - 2s/step - acc: 0.0000e+00 - loss: 0.2950 - val_acc: 0.0000e+00 - val_loss: 0.2827
Epoch 14/100
292/292 - 557s - 2s/step - acc: 0.0000e+00 - loss: 0.2828 - val_acc: 0.0000e+00 - val_loss: 0.2438
Epoch 15/100
292/292 - 558s - 2s/step - acc: 0.0000e+00 - loss: 0.2694 - val_acc: 0.0000e+00 - val_loss: 0.2807
Epoch 16/100
292/292 - 567s - 2s/step - acc: 0.0000e+00 - loss: 0.2753 - val_acc: 0.0000e+00 - val_loss: 0.2827
Epoch 17/100
292/292 - 558s - 2s/step - acc: 0.0000e+00 - loss: 0.2740 - val_acc: 0.0000e+00 - val_loss: 0.2562
Epoch 18/100
292/292 - 557s - 2s/step - acc: 0.0000e+00 - loss: 0.3074 - val_acc: 0.0000e+00 - val_loss: 0.2407
Epoch 19/100
292/292 - 555s - 2s/step - acc: 0.0000e+00 - loss: 0.2687 - val_acc: 0.0000e+00 - val_loss: 0.2494
Epoch 20/100
292/292 - 560s - 2s/step - acc: 0.0000e+00 - loss: 0.2719 - val_acc: 0.0000e+00 - val_loss: 0.2700
Epoch 21/100
292/292 - 561s - 2s/step - acc: 0.0000e+00 - loss: 0.2652 - val_acc: 0.0000e+00 - val_loss: 0.2357
Early stopping epoch: 20
******Evaluating TEST set*********
33/33 - 15s - 467ms/step
              precision    recall  f1-score   support

           0       0.14      0.24      0.17       403
           1       0.09      0.05      0.06       635

    accuracy                           0.12      1038
   macro avg       0.11      0.14      0.12      1038
weighted avg       0.11      0.12      0.10      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.47      0.41       403
           1       0.59      0.49      0.54       635

    accuracy                           0.48      1038
   macro avg       0.48      0.48      0.48      1038
weighted avg       0.51      0.48      0.49      1038

______________________________________________________
fold 1
Model: "functional_3"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)      │ (None, 2772, 6, 3)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_2 (Conv2D)               │ (None, 2772, 6, 32)    │         1,760 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_3 (Conv2D)               │ (None, 2772, 6, 64)    │        36,928 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_1 (MaxPooling2D)  │ (None, 2772, 1, 64)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_1 (Lambda)               │ (None, 2772, 64)       │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 2772, 64),     │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_1                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 2772)]             │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 76,322 (298.13 KB)
 Trainable params: 76,322 (298.13 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 577s - 2s/step - acc: 0.0000e+00 - loss: 6.3026 - val_acc: 0.0000e+00 - val_loss: 6.2889
Epoch 2/100
292/292 - 571s - 2s/step - acc: 0.0000e+00 - loss: 6.3026 - val_acc: 0.0000e+00 - val_loss: 6.2889
Epoch 3/100
292/292 - 555s - 2s/step - acc: 0.0000e+00 - loss: 6.3026 - val_acc: 0.0000e+00 - val_loss: 6.2889
Epoch 4/100
292/292 - 577s - 2s/step - acc: 0.0000e+00 - loss: 6.3026 - val_acc: 0.0000e+00 - val_loss: 6.2889
Epoch 5/100
slurmstepd: error: *** JOB 31251287 ON nc10137 CANCELLED AT 2024-07-04T21:41:58 DUE TO TIME LIMIT ***
