fold 0
Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)        │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d (Conv2D)                 │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda (Lambda)                 │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention (SelfAttention)  │ [(None, 1024), (None,  │         2,560 │
│                                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 17s - 59ms/step - acc: 0.0000e+00 - auc-prc: 0.4065 - auc-roc: 0.3860 - loss: 1.5260 - val_acc: 0.0000e+00 - val_auc-prc: 0.4692 - val_auc-roc: 0.4798 - val_loss: 0.4750
Epoch 2/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4432 - auc-roc: 0.4478 - loss: 1.0269 - val_acc: 0.0000e+00 - val_auc-prc: 0.5047 - val_auc-roc: 0.5048 - val_loss: 0.6032
Epoch 3/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.5261 - auc-roc: 0.5089 - loss: 0.7724 - val_acc: 0.0000e+00 - val_auc-prc: 0.5000 - val_auc-roc: 0.5000 - val_loss: 0.4132
Epoch 4/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4380 - auc-roc: 0.4424 - loss: 0.3217 - val_acc: 0.0000e+00 - val_auc-prc: 0.4225 - val_auc-roc: 0.4171 - val_loss: 0.2983
Epoch 5/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4251 - auc-roc: 0.4218 - loss: 0.3946 - val_acc: 0.0000e+00 - val_auc-prc: 0.4145 - val_auc-roc: 0.4021 - val_loss: 0.4635
Epoch 6/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4310 - auc-roc: 0.4316 - loss: 0.4570 - val_acc: 0.0000e+00 - val_auc-prc: 0.4385 - val_auc-roc: 0.4432 - val_loss: 0.3188
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4290 - auc-roc: 0.4284 - loss: 0.5482 - val_acc: 0.0000e+00 - val_auc-prc: 0.4801 - val_auc-roc: 0.4889 - val_loss: 0.5376
Epoch 8/100
292/292 - 12s - 43ms/step - acc: 0.0000e+00 - auc-prc: 0.4259 - auc-roc: 0.4231 - loss: 0.6691 - val_acc: 0.0000e+00 - val_auc-prc: 0.4157 - val_auc-roc: 0.4046 - val_loss: 0.6544
Epoch 9/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4399 - auc-roc: 0.4452 - loss: 0.4618 - val_acc: 0.0000e+00 - val_auc-prc: 0.4183 - val_auc-roc: 0.4095 - val_loss: 0.4679
Epoch 10/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4217 - auc-roc: 0.4158 - loss: 0.4979 - val_acc: 0.0000e+00 - val_auc-prc: 0.4180 - val_auc-roc: 0.4089 - val_loss: 0.4175
Epoch 11/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4153 - auc-roc: 0.4038 - loss: 0.5299 - val_acc: 0.0000e+00 - val_auc-prc: 0.4167 - val_auc-roc: 0.4065 - val_loss: 0.4139
Epoch 12/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4181 - auc-roc: 0.4092 - loss: 0.4997 - val_acc: 0.0000e+00 - val_auc-prc: 0.4143 - val_auc-roc: 0.4017 - val_loss: 0.4270
Early stopping epoch: 11
******Evaluating TEST set*********
33/33 - 1s - 21ms/step
              precision    recall  f1-score   support

           0       0.39      0.99      0.55       403
           1       0.17      0.00      0.00       635

    accuracy                           0.38      1038
   macro avg       0.28      0.49      0.28      1038
weighted avg       0.25      0.38      0.22      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.47      0.41       403
           1       0.59      0.49      0.54       635

    accuracy                           0.48      1038
   macro avg       0.48      0.48      0.48      1038
weighted avg       0.51      0.48      0.49      1038

______________________________________________________
fold 1
Model: "functional_3"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_1 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_1 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_1                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 14s - 49ms/step - acc: 0.0000e+00 - auc-prc: 0.4345 - auc-roc: 0.4372 - loss: 0.8863 - val_acc: 0.0000e+00 - val_auc-prc: 0.4101 - val_auc-roc: 0.3934 - val_loss: 2.9280
Epoch 2/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4977 - auc-roc: 0.4954 - loss: 0.8240 - val_acc: 0.0000e+00 - val_auc-prc: 0.5000 - val_auc-roc: 0.5000 - val_loss: 0.6226
Epoch 3/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5576 - auc-roc: 0.6029 - loss: 0.4793 - val_acc: 0.0000e+00 - val_auc-prc: 0.6338 - val_auc-roc: 0.7103 - val_loss: 0.3082
Epoch 4/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.6360 - auc-roc: 0.7126 - loss: 0.3962 - val_acc: 0.0000e+00 - val_auc-prc: 0.5000 - val_auc-roc: 0.5000 - val_loss: 0.5948
Epoch 5/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5191 - auc-roc: 0.5368 - loss: 0.5287 - val_acc: 0.0000e+00 - val_auc-prc: 0.5285 - val_auc-roc: 0.5539 - val_loss: 0.4563
Epoch 6/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5658 - auc-roc: 0.6158 - loss: 0.4498 - val_acc: 0.0000e+00 - val_auc-prc: 0.5878 - val_auc-roc: 0.6486 - val_loss: 0.3814
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5700 - auc-roc: 0.6223 - loss: 0.4183 - val_acc: 0.0000e+00 - val_auc-prc: 0.5000 - val_auc-roc: 0.5000 - val_loss: 0.3572
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6327 - auc-roc: 0.7084 - loss: 0.3441 - val_acc: 0.0000e+00 - val_auc-prc: 0.6862 - val_auc-roc: 0.7703 - val_loss: 0.2816
Epoch 9/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6529 - auc-roc: 0.7331 - loss: 0.3134 - val_acc: 0.0000e+00 - val_auc-prc: 0.6559 - val_auc-roc: 0.7373 - val_loss: 0.2821
Epoch 10/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5672 - auc-roc: 0.6182 - loss: 0.4205 - val_acc: 0.0000e+00 - val_auc-prc: 0.5455 - val_auc-roc: 0.5833 - val_loss: 0.3634
Epoch 11/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5575 - auc-roc: 0.6030 - loss: 0.3352 - val_acc: 0.0000e+00 - val_auc-prc: 0.5830 - val_auc-roc: 0.6423 - val_loss: 0.2845
Epoch 12/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5929 - auc-roc: 0.6565 - loss: 0.3252 - val_acc: 0.0000e+00 - val_auc-prc: 0.5539 - val_auc-roc: 0.5973 - val_loss: 0.3500
Epoch 13/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5475 - auc-roc: 0.5867 - loss: 0.3377 - val_acc: 0.0000e+00 - val_auc-prc: 0.5533 - val_auc-roc: 0.5963 - val_loss: 0.2816
Epoch 14/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.6101 - auc-roc: 0.6800 - loss: 0.3193 - val_acc: 0.0000e+00 - val_auc-prc: 0.5683 - val_auc-roc: 0.6201 - val_loss: 0.2737
Epoch 15/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5813 - auc-roc: 0.6396 - loss: 0.3175 - val_acc: 0.0000e+00 - val_auc-prc: 0.5787 - val_auc-roc: 0.6360 - val_loss: 0.2740
Epoch 16/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6053 - auc-roc: 0.6735 - loss: 0.3072 - val_acc: 0.0000e+00 - val_auc-prc: 0.5560 - val_auc-roc: 0.6007 - val_loss: 0.2774
Epoch 17/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.6047 - auc-roc: 0.6727 - loss: 0.3021 - val_acc: 0.0000e+00 - val_auc-prc: 0.5672 - val_auc-roc: 0.6185 - val_loss: 0.2718
Epoch 18/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.6146 - auc-roc: 0.6860 - loss: 0.2997 - val_acc: 0.0000e+00 - val_auc-prc: 0.5578 - val_auc-roc: 0.6036 - val_loss: 0.2706
Early stopping epoch: 17
******Evaluating TEST set*********
33/33 - 1s - 21ms/step
              precision    recall  f1-score   support

           0       0.88      0.81      0.84       403
           1       0.88      0.93      0.91       635

    accuracy                           0.88      1038
   macro avg       0.88      0.87      0.88      1038
weighted avg       0.88      0.88      0.88      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.52      0.44       403
           1       0.60      0.47      0.53       635

    accuracy                           0.49      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.52      0.49      0.49      1038

______________________________________________________
fold 2
Model: "functional_5"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_2 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_2 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_2 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_2                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 14s - 47ms/step - acc: 0.0000e+00 - auc-prc: 0.4007 - auc-roc: 0.3732 - loss: 0.5070 - val_acc: 0.0000e+00 - val_auc-prc: 0.3603 - val_auc-roc: 0.2584 - val_loss: 0.5118
Epoch 2/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4097 - auc-roc: 0.3927 - loss: 1.4284 - val_acc: 0.0000e+00 - val_auc-prc: 0.4476 - val_auc-roc: 0.4557 - val_loss: 0.3285
Epoch 3/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4154 - auc-roc: 0.4039 - loss: 0.6409 - val_acc: 0.0000e+00 - val_auc-prc: 0.4154 - val_auc-roc: 0.4039 - val_loss: 0.7147
Epoch 4/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4320 - auc-roc: 0.4333 - loss: 0.5089 - val_acc: 0.0000e+00 - val_auc-prc: 0.4206 - val_auc-roc: 0.4137 - val_loss: 0.3097
Epoch 5/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4163 - auc-roc: 0.4057 - loss: 0.3157 - val_acc: 0.0000e+00 - val_auc-prc: 0.4227 - val_auc-roc: 0.4176 - val_loss: 0.3042
Epoch 6/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4243 - auc-roc: 0.4204 - loss: 0.3723 - val_acc: 0.0000e+00 - val_auc-prc: 0.4162 - val_auc-roc: 0.4054 - val_loss: 0.3377
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4173 - auc-roc: 0.4077 - loss: 0.3258 - val_acc: 0.0000e+00 - val_auc-prc: 0.4206 - val_auc-roc: 0.4138 - val_loss: 0.2910
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4145 - auc-roc: 0.4023 - loss: 0.3268 - val_acc: 0.0000e+00 - val_auc-prc: 0.4152 - val_auc-roc: 0.4035 - val_loss: 0.3763
Epoch 9/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4140 - auc-roc: 0.4013 - loss: 0.3752 - val_acc: 0.0000e+00 - val_auc-prc: 0.4191 - val_auc-roc: 0.4109 - val_loss: 0.2881
Epoch 10/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4284 - auc-roc: 0.4273 - loss: 0.3233 - val_acc: 0.0000e+00 - val_auc-prc: 0.4741 - val_auc-roc: 0.4841 - val_loss: 0.3231
Epoch 11/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4402 - auc-roc: 0.4456 - loss: 0.2959 - val_acc: 0.0000e+00 - val_auc-prc: 0.4310 - val_auc-roc: 0.4316 - val_loss: 0.2894
Epoch 12/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4191 - auc-roc: 0.4110 - loss: 0.3253 - val_acc: 0.0000e+00 - val_auc-prc: 0.4167 - val_auc-roc: 0.4065 - val_loss: 0.2961
Epoch 13/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4155 - auc-roc: 0.4041 - loss: 0.2927 - val_acc: 0.0000e+00 - val_auc-prc: 0.4165 - val_auc-roc: 0.4060 - val_loss: 0.2936
Epoch 14/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4205 - auc-roc: 0.4135 - loss: 0.2881 - val_acc: 0.0000e+00 - val_auc-prc: 0.4201 - val_auc-roc: 0.4128 - val_loss: 0.2796
Epoch 15/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4150 - auc-roc: 0.4033 - loss: 0.2878 - val_acc: 0.0000e+00 - val_auc-prc: 0.4152 - val_auc-roc: 0.4036 - val_loss: 0.2926
Epoch 16/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4214 - auc-roc: 0.4152 - loss: 0.2928 - val_acc: 0.0000e+00 - val_auc-prc: 0.4319 - val_auc-roc: 0.4330 - val_loss: 0.2827
Epoch 17/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4220 - auc-roc: 0.4163 - loss: 0.2890 - val_acc: 0.0000e+00 - val_auc-prc: 0.4250 - val_auc-roc: 0.4215 - val_loss: 0.2823
Epoch 18/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4164 - auc-roc: 0.4059 - loss: 0.2865 - val_acc: 0.0000e+00 - val_auc-prc: 0.4157 - val_auc-roc: 0.4045 - val_loss: 0.2948
Epoch 19/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4149 - auc-roc: 0.4031 - loss: 0.2999 - val_acc: 0.0000e+00 - val_auc-prc: 0.4193 - val_auc-roc: 0.4113 - val_loss: 0.2938
Epoch 20/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4147 - auc-roc: 0.4026 - loss: 0.2870 - val_acc: 0.0000e+00 - val_auc-prc: 0.4157 - val_auc-roc: 0.4046 - val_loss: 0.2922
Early stopping epoch: 19
******Evaluating TEST set*********
33/33 - 1s - 21ms/step
              precision    recall  f1-score   support

           0       0.10      0.16      0.13       403
           1       0.16      0.10      0.12       635

    accuracy                           0.12      1038
   macro avg       0.13      0.13      0.12      1038
weighted avg       0.14      0.12      0.12      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.49      0.43       403
           1       0.61      0.50      0.55       635

    accuracy                           0.50      1038
   macro avg       0.50      0.50      0.49      1038
weighted avg       0.52      0.50      0.50      1038

______________________________________________________
fold 3
Model: "functional_7"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_3 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_3 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_3 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_3                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 14s - 48ms/step - acc: 0.0000e+00 - auc-prc: 0.6997 - auc-roc: 0.7598 - loss: 0.8970 - val_acc: 0.0000e+00 - val_auc-prc: 0.4945 - val_auc-roc: 0.4894 - val_loss: 0.7236
Epoch 2/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5056 - auc-roc: 0.5074 - loss: 0.7489 - val_acc: 0.0000e+00 - val_auc-prc: 0.4836 - val_auc-roc: 0.4913 - val_loss: 0.6079
Epoch 3/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4403 - auc-roc: 0.4458 - loss: 0.4638 - val_acc: 0.0000e+00 - val_auc-prc: 0.4588 - val_auc-roc: 0.4692 - val_loss: 0.3712
Epoch 4/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4032 - auc-roc: 0.3786 - loss: 0.3774 - val_acc: 0.0000e+00 - val_auc-prc: 0.4199 - val_auc-roc: 0.4123 - val_loss: 0.3493
Epoch 5/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4157 - auc-roc: 0.4046 - loss: 0.3665 - val_acc: 0.0000e+00 - val_auc-prc: 0.4115 - val_auc-roc: 0.3963 - val_loss: 0.3337
Epoch 6/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4156 - auc-roc: 0.4045 - loss: 0.3395 - val_acc: 0.0000e+00 - val_auc-prc: 0.4140 - val_auc-roc: 0.4012 - val_loss: 0.2984
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4381 - auc-roc: 0.4425 - loss: 0.3699 - val_acc: 0.0000e+00 - val_auc-prc: 0.4159 - val_auc-roc: 0.4050 - val_loss: 0.2976
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4287 - auc-roc: 0.4278 - loss: 0.3324 - val_acc: 0.0000e+00 - val_auc-prc: 0.4219 - val_auc-roc: 0.4161 - val_loss: 0.2831
Epoch 9/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4298 - auc-roc: 0.4296 - loss: 0.3365 - val_acc: 0.0000e+00 - val_auc-prc: 0.4177 - val_auc-roc: 0.4084 - val_loss: 0.2756
Epoch 10/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4190 - auc-roc: 0.4108 - loss: 0.3062 - val_acc: 0.0000e+00 - val_auc-prc: 0.4135 - val_auc-roc: 0.4002 - val_loss: 0.2807
Epoch 11/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4173 - auc-roc: 0.4076 - loss: 0.3087 - val_acc: 0.0000e+00 - val_auc-prc: 0.4147 - val_auc-roc: 0.4026 - val_loss: 0.2673
Early stopping epoch: 10
******Evaluating TEST set*********
33/33 - 1s - 20ms/step
              precision    recall  f1-score   support

           0       1.00      0.01      0.02       403
           1       0.61      1.00      0.76       635

    accuracy                           0.62      1038
   macro avg       0.81      0.51      0.39      1038
weighted avg       0.76      0.62      0.48      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.46      0.42       403
           1       0.60      0.52      0.56       635

    accuracy                           0.50      1038
   macro avg       0.49      0.49      0.49      1038
weighted avg       0.52      0.50      0.50      1038

______________________________________________________
fold 4
Model: "functional_9"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_4 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_4 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_4 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_4                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 15s - 51ms/step - acc: 0.0000e+00 - auc-prc: 0.7999 - auc-roc: 0.8363 - loss: 1.7603 - val_acc: 0.0000e+00 - val_auc-prc: 0.8416 - val_auc-roc: 0.8955 - val_loss: 1.8499
Epoch 2/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5118 - auc-roc: 0.5219 - loss: 0.8119 - val_acc: 0.0000e+00 - val_auc-prc: 0.5575 - val_auc-roc: 0.5985 - val_loss: 0.5928
Epoch 3/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5975 - auc-roc: 0.6512 - loss: 0.6146 - val_acc: 0.0000e+00 - val_auc-prc: 0.5212 - val_auc-roc: 0.5406 - val_loss: 0.5696
Epoch 4/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5157 - auc-roc: 0.5294 - loss: 0.5556 - val_acc: 0.0000e+00 - val_auc-prc: 0.5367 - val_auc-roc: 0.5667 - val_loss: 0.5240
Epoch 5/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5455 - auc-roc: 0.5794 - loss: 0.4823 - val_acc: 0.0000e+00 - val_auc-prc: 0.5549 - val_auc-roc: 0.5972 - val_loss: 0.4783
Epoch 6/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5362 - auc-roc: 0.5659 - loss: 0.4554 - val_acc: 0.0000e+00 - val_auc-prc: 0.5845 - val_auc-roc: 0.6439 - val_loss: 0.3478
Epoch 7/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.5389 - auc-roc: 0.5715 - loss: 0.4254 - val_acc: 0.0000e+00 - val_auc-prc: 0.5172 - val_auc-roc: 0.5332 - val_loss: 0.3706
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5411 - auc-roc: 0.5745 - loss: 0.3689 - val_acc: 0.0000e+00 - val_auc-prc: 0.6392 - val_auc-roc: 0.7136 - val_loss: 0.3654
Epoch 9/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5814 - auc-roc: 0.6314 - loss: 0.3766 - val_acc: 0.0000e+00 - val_auc-prc: 0.6183 - val_auc-roc: 0.6747 - val_loss: 0.3376
Epoch 10/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.5850 - auc-roc: 0.6379 - loss: 0.4069 - val_acc: 0.0000e+00 - val_auc-prc: 0.6804 - val_auc-roc: 0.7600 - val_loss: 0.3240
Epoch 11/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6115 - auc-roc: 0.6764 - loss: 0.3459 - val_acc: 0.0000e+00 - val_auc-prc: 0.5419 - val_auc-roc: 0.5757 - val_loss: 0.4037
Early stopping epoch: 10
******Evaluating TEST set*********
33/33 - 1s - 21ms/step
              precision    recall  f1-score   support

           0       0.86      0.79      0.82       403
           1       0.87      0.92      0.90       635

    accuracy                           0.87      1038
   macro avg       0.87      0.85      0.86      1038
weighted avg       0.87      0.87      0.87      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.51      0.43       403
           1       0.60      0.47      0.53       635

    accuracy                           0.49      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.52      0.49      0.49      1038

______________________________________________________
fold 5
Model: "functional_11"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_5 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_5 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_5 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_5                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 15s - 50ms/step - acc: 0.0000e+00 - auc-prc: 0.6506 - auc-roc: 0.6522 - loss: 5.8713 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 2/100
292/292 - 13s - 43ms/step - acc: 0.0000e+00 - auc-prc: 0.6047 - auc-roc: 0.6263 - loss: 6.2605 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 3/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6068 - auc-roc: 0.6277 - loss: 6.2622 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 4/100
292/292 - 12s - 43ms/step - acc: 0.0000e+00 - auc-prc: 0.6054 - auc-roc: 0.6268 - loss: 6.2622 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 5/100
292/292 - 12s - 43ms/step - acc: 0.0000e+00 - auc-prc: 0.6078 - auc-roc: 0.6284 - loss: 6.2605 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 6/100
292/292 - 12s - 43ms/step - acc: 0.0000e+00 - auc-prc: 0.6057 - auc-roc: 0.6270 - loss: 6.2622 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6063 - auc-roc: 0.6275 - loss: 6.2622 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6058 - auc-roc: 0.6272 - loss: 6.2605 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 9/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6061 - auc-roc: 0.6273 - loss: 6.2639 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 10/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6064 - auc-roc: 0.6275 - loss: 6.2622 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 11/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6052 - auc-roc: 0.6266 - loss: 6.2639 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Epoch 12/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.6048 - auc-roc: 0.6264 - loss: 6.2605 - val_acc: 0.0000e+00 - val_auc-prc: 0.6012 - val_auc-roc: 0.6239 - val_loss: 6.2638
Early stopping epoch: 11
******Evaluating TEST set*********
33/33 - 1s - 22ms/step
/lustre06/project/6003138/rezvank/5_model/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/lustre06/project/6003138/rezvank/5_model/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/lustre06/project/6003138/rezvank/5_model/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       403
           1       0.61      1.00      0.76       634

    accuracy                           0.61      1037
   macro avg       0.31      0.50      0.38      1037
weighted avg       0.37      0.61      0.46      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.50      0.43       403
           1       0.60      0.48      0.53       634

    accuracy                           0.48      1037
   macro avg       0.49      0.49      0.48      1037
weighted avg       0.51      0.48      0.49      1037

______________________________________________________
fold 6
Model: "functional_13"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_6 (InputLayer)      │ (None, 150, 6, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_6 (Conv2D)               │ (None, 146, 1, 64)     │         5,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lambda_6 (Lambda)               │ (None, 146, 64)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (LSTM)                     │ [(None, 146, 64),      │        33,024 │
│                                 │ (None, 64), (None,     │               │
│                                 │ 64)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ self_attention_6                │ [(None, 1024), (None,  │         2,560 │
│ (SelfAttention)                 │ 16, 146)]              │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_6 (Dense)                 │ (None, 2)              │         2,050 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 43,458 (169.76 KB)
 Trainable params: 43,458 (169.76 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/100
292/292 - 15s - 51ms/step - acc: 0.0000e+00 - auc-prc: 0.4207 - auc-roc: 0.4141 - loss: 0.7845 - val_acc: 0.0000e+00 - val_auc-prc: 0.4185 - val_auc-roc: 0.4099 - val_loss: 0.3824
Epoch 2/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4047 - auc-roc: 0.3821 - loss: 1.1549 - val_acc: 0.0000e+00 - val_auc-prc: 0.3992 - val_auc-roc: 0.3696 - val_loss: 0.4501
Epoch 3/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4624 - auc-roc: 0.4704 - loss: 0.9682 - val_acc: 0.0000e+00 - val_auc-prc: 0.5000 - val_auc-roc: 0.5000 - val_loss: 0.6186
Epoch 4/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4457 - auc-roc: 0.4532 - loss: 1.1628 - val_acc: 0.0000e+00 - val_auc-prc: 0.4249 - val_auc-roc: 0.4214 - val_loss: 2.1424
Epoch 5/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4132 - auc-roc: 0.3998 - loss: 2.8246 - val_acc: 0.0000e+00 - val_auc-prc: 0.4167 - val_auc-roc: 0.4065 - val_loss: 2.3819
Epoch 6/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4060 - auc-roc: 0.3851 - loss: 2.8164 - val_acc: 0.0000e+00 - val_auc-prc: 0.4156 - val_auc-roc: 0.4044 - val_loss: 2.1205
Epoch 7/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4079 - auc-roc: 0.3890 - loss: 2.1768 - val_acc: 0.0000e+00 - val_auc-prc: 0.4164 - val_auc-roc: 0.4059 - val_loss: 1.3598
Epoch 8/100
292/292 - 12s - 42ms/step - acc: 0.0000e+00 - auc-prc: 0.4282 - auc-roc: 0.4271 - loss: 1.1984 - val_acc: 0.0000e+00 - val_auc-prc: 0.4546 - val_auc-roc: 0.4643 - val_loss: 0.4246
Epoch 9/100
292/292 - 12s - 40ms/step - acc: 0.0000e+00 - auc-prc: 0.4215 - auc-roc: 0.4154 - loss: 0.4053 - val_acc: 0.0000e+00 - val_auc-prc: 0.4202 - val_auc-roc: 0.4131 - val_loss: 0.4387
Epoch 10/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4362 - auc-roc: 0.4398 - loss: 0.3848 - val_acc: 0.0000e+00 - val_auc-prc: 0.4369 - val_auc-roc: 0.4407 - val_loss: 0.3476
Epoch 11/100
292/292 - 12s - 41ms/step - acc: 0.0000e+00 - auc-prc: 0.4234 - auc-roc: 0.4187 - loss: 0.3131 - val_acc: 0.0000e+00 - val_auc-prc: 0.4241 - val_auc-roc: 0.4200 - val_loss: 0.3423
Epoch 12/100
slurmstepd: error: *** JOB 31379617 ON nc10247 CANCELLED AT 2024-07-09T18:20:28 ***
