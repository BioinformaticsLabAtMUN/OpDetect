fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3163 - acc: 0.8655 - auc: 0.9371 - val_loss: 0.2968 - val_acc: 0.8776 - val_auc: 0.9454 - 22s/epoch - 77ms/step
Epoch 2/100
292/292 - 20s - loss: 0.2877 - acc: 0.8804 - auc: 0.9471 - val_loss: 0.2788 - val_acc: 0.8815 - val_auc: 0.9514 - 20s/epoch - 68ms/step
Epoch 3/100
292/292 - 20s - loss: 0.2715 - acc: 0.8899 - auc: 0.9526 - val_loss: 0.2593 - val_acc: 0.8979 - val_auc: 0.9547 - 20s/epoch - 69ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2694 - acc: 0.8907 - auc: 0.9533 - val_loss: 0.2492 - val_acc: 0.9075 - val_auc: 0.9589 - 21s/epoch - 71ms/step
Epoch 5/100
292/292 - 21s - loss: 0.2616 - acc: 0.8965 - auc: 0.9562 - val_loss: 0.2708 - val_acc: 0.8911 - val_auc: 0.9518 - 21s/epoch - 72ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2587 - acc: 0.8994 - auc: 0.9577 - val_loss: 0.2517 - val_acc: 0.9008 - val_auc: 0.9585 - 21s/epoch - 70ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2549 - acc: 0.8970 - auc: 0.9593 - val_loss: 0.2569 - val_acc: 0.8998 - val_auc: 0.9594 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2553 - acc: 0.8988 - auc: 0.9591 - val_loss: 0.2443 - val_acc: 0.8969 - val_auc: 0.9616 - 21s/epoch - 73ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2535 - acc: 0.8963 - auc: 0.9595 - val_loss: 0.2513 - val_acc: 0.8960 - val_auc: 0.9593 - 21s/epoch - 73ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2523 - acc: 0.8965 - auc: 0.9608 - val_loss: 0.2498 - val_acc: 0.9017 - val_auc: 0.9595 - 21s/epoch - 73ms/step
Epoch 11/100
292/292 - 21s - loss: 0.2499 - acc: 0.8995 - auc: 0.9611 - val_loss: 0.2481 - val_acc: 0.9008 - val_auc: 0.9603 - 21s/epoch - 73ms/step
Epoch 12/100
292/292 - 21s - loss: 0.2466 - acc: 0.8987 - auc: 0.9626 - val_loss: 0.2477 - val_acc: 0.9046 - val_auc: 0.9599 - 21s/epoch - 72ms/step
Epoch 13/100
292/292 - 20s - loss: 0.2463 - acc: 0.8998 - auc: 0.9626 - val_loss: 0.2382 - val_acc: 0.9046 - val_auc: 0.9620 - 20s/epoch - 70ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2454 - acc: 0.8994 - auc: 0.9626 - val_loss: 0.2526 - val_acc: 0.8950 - val_auc: 0.9592 - 19s/epoch - 64ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2420 - acc: 0.8995 - auc: 0.9641 - val_loss: 0.2382 - val_acc: 0.9066 - val_auc: 0.9634 - 17s/epoch - 57ms/step
Epoch 16/100
292/292 - 17s - loss: 0.2404 - acc: 0.9018 - auc: 0.9638 - val_loss: 0.2595 - val_acc: 0.8979 - val_auc: 0.9585 - 17s/epoch - 57ms/step
Epoch 17/100
292/292 - 20s - loss: 0.2398 - acc: 0.9023 - auc: 0.9642 - val_loss: 0.2400 - val_acc: 0.9085 - val_auc: 0.9632 - 20s/epoch - 67ms/step
Epoch 18/100
292/292 - 17s - loss: 0.2375 - acc: 0.9041 - auc: 0.9649 - val_loss: 0.2415 - val_acc: 0.9066 - val_auc: 0.9636 - 17s/epoch - 59ms/step
Epoch 19/100
292/292 - 20s - loss: 0.2310 - acc: 0.9052 - auc: 0.9670 - val_loss: 0.2422 - val_acc: 0.9017 - val_auc: 0.9618 - 20s/epoch - 68ms/step
Epoch 20/100
292/292 - 21s - loss: 0.2312 - acc: 0.9055 - auc: 0.9672 - val_loss: 0.2357 - val_acc: 0.9133 - val_auc: 0.9637 - 21s/epoch - 71ms/step
Epoch 21/100
292/292 - 20s - loss: 0.2264 - acc: 0.9069 - auc: 0.9686 - val_loss: 0.2377 - val_acc: 0.9114 - val_auc: 0.9630 - 20s/epoch - 70ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2233 - acc: 0.9086 - auc: 0.9687 - val_loss: 0.2447 - val_acc: 0.9104 - val_auc: 0.9610 - 21s/epoch - 70ms/step
Epoch 23/100
292/292 - 20s - loss: 0.2228 - acc: 0.9083 - auc: 0.9697 - val_loss: 0.2380 - val_acc: 0.9056 - val_auc: 0.9639 - 20s/epoch - 70ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2151 - acc: 0.9122 - auc: 0.9714 - val_loss: 0.2441 - val_acc: 0.9085 - val_auc: 0.9629 - 21s/epoch - 71ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2126 - acc: 0.9144 - auc: 0.9720 - val_loss: 0.2480 - val_acc: 0.9066 - val_auc: 0.9585 - 21s/epoch - 71ms/step
Epoch 26/100
292/292 - 20s - loss: 0.2100 - acc: 0.9165 - auc: 0.9729 - val_loss: 0.2500 - val_acc: 0.9171 - val_auc: 0.9615 - 20s/epoch - 68ms/step
Epoch 27/100
292/292 - 20s - loss: 0.2045 - acc: 0.9198 - auc: 0.9739 - val_loss: 0.2524 - val_acc: 0.9056 - val_auc: 0.9592 - 20s/epoch - 69ms/step
Epoch 28/100
292/292 - 21s - loss: 0.2018 - acc: 0.9182 - auc: 0.9747 - val_loss: 0.2524 - val_acc: 0.9037 - val_auc: 0.9591 - 21s/epoch - 70ms/step
Epoch 29/100
292/292 - 20s - loss: 0.1933 - acc: 0.9226 - auc: 0.9772 - val_loss: 0.2504 - val_acc: 0.9104 - val_auc: 0.9611 - 20s/epoch - 69ms/step
Epoch 30/100
292/292 - 21s - loss: 0.1906 - acc: 0.9264 - auc: 0.9776 - val_loss: 0.2589 - val_acc: 0.9104 - val_auc: 0.9555 - 21s/epoch - 71ms/step
Epoch 31/100
292/292 - 21s - loss: 0.1908 - acc: 0.9230 - auc: 0.9772 - val_loss: 0.2444 - val_acc: 0.9094 - val_auc: 0.9617 - 21s/epoch - 71ms/step
Epoch 32/100
292/292 - 21s - loss: 0.1801 - acc: 0.9293 - auc: 0.9802 - val_loss: 0.2702 - val_acc: 0.9123 - val_auc: 0.9568 - 21s/epoch - 70ms/step
Epoch 33/100
292/292 - 20s - loss: 0.1766 - acc: 0.9262 - auc: 0.9806 - val_loss: 0.2639 - val_acc: 0.9114 - val_auc: 0.9570 - 20s/epoch - 70ms/step
Early stopping epoch: 32
******Evaluating TEST set*********
33/33 - 1s - 946ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.90      0.85      0.88       403
           1       0.91      0.94      0.92       635

    accuracy                           0.91      1038
   macro avg       0.90      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.45      0.39       403
           1       0.58      0.48      0.52       635

    accuracy                           0.47      1038
   macro avg       0.46      0.46      0.46      1038
weighted avg       0.49      0.47      0.47      1038

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3113 - acc: 0.8686 - auc: 0.9390 - val_loss: 0.3033 - val_acc: 0.8767 - val_auc: 0.9439 - 23s/epoch - 79ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2828 - acc: 0.8879 - auc: 0.9483 - val_loss: 0.2937 - val_acc: 0.8815 - val_auc: 0.9470 - 21s/epoch - 71ms/step
Epoch 3/100
292/292 - 21s - loss: 0.2684 - acc: 0.8916 - auc: 0.9528 - val_loss: 0.2806 - val_acc: 0.8882 - val_auc: 0.9518 - 21s/epoch - 72ms/step
Epoch 4/100
292/292 - 20s - loss: 0.2624 - acc: 0.8948 - auc: 0.9553 - val_loss: 0.2906 - val_acc: 0.8757 - val_auc: 0.9501 - 20s/epoch - 69ms/step
Epoch 5/100
292/292 - 20s - loss: 0.2608 - acc: 0.8948 - auc: 0.9562 - val_loss: 0.2794 - val_acc: 0.8844 - val_auc: 0.9540 - 20s/epoch - 69ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2578 - acc: 0.8961 - auc: 0.9576 - val_loss: 0.2835 - val_acc: 0.8854 - val_auc: 0.9529 - 21s/epoch - 71ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2541 - acc: 0.8984 - auc: 0.9589 - val_loss: 0.2926 - val_acc: 0.8834 - val_auc: 0.9517 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2561 - acc: 0.8955 - auc: 0.9584 - val_loss: 0.2779 - val_acc: 0.8834 - val_auc: 0.9550 - 21s/epoch - 72ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2500 - acc: 0.8998 - auc: 0.9603 - val_loss: 0.2734 - val_acc: 0.8844 - val_auc: 0.9557 - 21s/epoch - 71ms/step
Epoch 10/100
292/292 - 18s - loss: 0.2477 - acc: 0.9016 - auc: 0.9612 - val_loss: 0.2859 - val_acc: 0.8776 - val_auc: 0.9532 - 18s/epoch - 63ms/step
Epoch 11/100
292/292 - 17s - loss: 0.2455 - acc: 0.9032 - auc: 0.9613 - val_loss: 0.2750 - val_acc: 0.8786 - val_auc: 0.9556 - 17s/epoch - 57ms/step
Epoch 12/100
292/292 - 17s - loss: 0.2449 - acc: 0.9051 - auc: 0.9619 - val_loss: 0.2954 - val_acc: 0.8719 - val_auc: 0.9499 - 17s/epoch - 57ms/step
Epoch 13/100
292/292 - 17s - loss: 0.2434 - acc: 0.9018 - auc: 0.9625 - val_loss: 0.2882 - val_acc: 0.8825 - val_auc: 0.9530 - 17s/epoch - 57ms/step
Epoch 14/100
292/292 - 17s - loss: 0.2395 - acc: 0.9017 - auc: 0.9637 - val_loss: 0.2705 - val_acc: 0.8911 - val_auc: 0.9582 - 17s/epoch - 58ms/step
Epoch 15/100
292/292 - 21s - loss: 0.2383 - acc: 0.9030 - auc: 0.9643 - val_loss: 0.2734 - val_acc: 0.8815 - val_auc: 0.9563 - 21s/epoch - 72ms/step
Epoch 16/100
292/292 - 21s - loss: 0.2359 - acc: 0.9044 - auc: 0.9650 - val_loss: 0.2754 - val_acc: 0.8921 - val_auc: 0.9562 - 21s/epoch - 72ms/step
Epoch 17/100
292/292 - 21s - loss: 0.2340 - acc: 0.9034 - auc: 0.9662 - val_loss: 0.2730 - val_acc: 0.8863 - val_auc: 0.9577 - 21s/epoch - 72ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2319 - acc: 0.9063 - auc: 0.9665 - val_loss: 0.2706 - val_acc: 0.8911 - val_auc: 0.9576 - 21s/epoch - 72ms/step
Epoch 19/100
292/292 - 21s - loss: 0.2266 - acc: 0.9050 - auc: 0.9683 - val_loss: 0.2753 - val_acc: 0.8805 - val_auc: 0.9586 - 21s/epoch - 72ms/step
Epoch 20/100
292/292 - 21s - loss: 0.2250 - acc: 0.9094 - auc: 0.9687 - val_loss: 0.2706 - val_acc: 0.8863 - val_auc: 0.9588 - 21s/epoch - 72ms/step
Epoch 21/100
292/292 - 21s - loss: 0.2230 - acc: 0.9109 - auc: 0.9689 - val_loss: 0.2848 - val_acc: 0.8738 - val_auc: 0.9553 - 21s/epoch - 72ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2204 - acc: 0.9115 - auc: 0.9701 - val_loss: 0.2664 - val_acc: 0.8796 - val_auc: 0.9592 - 21s/epoch - 72ms/step
Epoch 23/100
292/292 - 18s - loss: 0.2162 - acc: 0.9112 - auc: 0.9710 - val_loss: 0.2743 - val_acc: 0.8834 - val_auc: 0.9567 - 18s/epoch - 62ms/step
Epoch 24/100
292/292 - 17s - loss: 0.2107 - acc: 0.9160 - auc: 0.9724 - val_loss: 0.2800 - val_acc: 0.8911 - val_auc: 0.9582 - 17s/epoch - 59ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2069 - acc: 0.9182 - auc: 0.9739 - val_loss: 0.2867 - val_acc: 0.8815 - val_auc: 0.9553 - 21s/epoch - 72ms/step
Epoch 26/100
292/292 - 20s - loss: 0.2050 - acc: 0.9183 - auc: 0.9742 - val_loss: 0.2926 - val_acc: 0.8767 - val_auc: 0.9526 - 20s/epoch - 70ms/step
Epoch 27/100
292/292 - 21s - loss: 0.1995 - acc: 0.9218 - auc: 0.9754 - val_loss: 0.2733 - val_acc: 0.8815 - val_auc: 0.9562 - 21s/epoch - 71ms/step
Epoch 28/100
292/292 - 21s - loss: 0.1953 - acc: 0.9236 - auc: 0.9768 - val_loss: 0.3040 - val_acc: 0.8805 - val_auc: 0.9517 - 21s/epoch - 72ms/step
Epoch 29/100
292/292 - 21s - loss: 0.1949 - acc: 0.9220 - auc: 0.9761 - val_loss: 0.2878 - val_acc: 0.8834 - val_auc: 0.9557 - 21s/epoch - 72ms/step
Epoch 30/100
292/292 - 21s - loss: 0.1860 - acc: 0.9252 - auc: 0.9786 - val_loss: 0.2846 - val_acc: 0.8892 - val_auc: 0.9568 - 21s/epoch - 70ms/step
Epoch 31/100
292/292 - 21s - loss: 0.1792 - acc: 0.9305 - auc: 0.9795 - val_loss: 0.2901 - val_acc: 0.8844 - val_auc: 0.9573 - 21s/epoch - 72ms/step
Epoch 32/100
292/292 - 21s - loss: 0.1743 - acc: 0.9316 - auc: 0.9811 - val_loss: 0.3012 - val_acc: 0.8815 - val_auc: 0.9533 - 21s/epoch - 72ms/step
Early stopping epoch: 31
******Evaluating TEST set*********
33/33 - 1s - 881ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.87      0.82      0.84       403
           1       0.89      0.92      0.90       635

    accuracy                           0.88      1038
   macro avg       0.88      0.87      0.87      1038
weighted avg       0.88      0.88      0.88      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.51      0.43       403
           1       0.60      0.46      0.52       635

    accuracy                           0.48      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.48      0.49      1038

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3183 - acc: 0.8641 - auc: 0.9368 - val_loss: 0.2743 - val_acc: 0.8940 - val_auc: 0.9501 - 23s/epoch - 80ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2806 - acc: 0.8871 - auc: 0.9496 - val_loss: 0.2673 - val_acc: 0.9037 - val_auc: 0.9536 - 21s/epoch - 71ms/step
Epoch 3/100
292/292 - 17s - loss: 0.2723 - acc: 0.8903 - auc: 0.9529 - val_loss: 0.2577 - val_acc: 0.9008 - val_auc: 0.9565 - 17s/epoch - 57ms/step
Epoch 4/100
292/292 - 17s - loss: 0.2662 - acc: 0.8915 - auc: 0.9546 - val_loss: 0.2529 - val_acc: 0.9046 - val_auc: 0.9576 - 17s/epoch - 59ms/step
Epoch 5/100
292/292 - 17s - loss: 0.2641 - acc: 0.8954 - auc: 0.9554 - val_loss: 0.2562 - val_acc: 0.9008 - val_auc: 0.9593 - 17s/epoch - 58ms/step
Epoch 6/100
292/292 - 17s - loss: 0.2605 - acc: 0.8959 - auc: 0.9572 - val_loss: 0.2519 - val_acc: 0.9123 - val_auc: 0.9579 - 17s/epoch - 58ms/step
Epoch 7/100
292/292 - 16s - loss: 0.2609 - acc: 0.8968 - auc: 0.9569 - val_loss: 0.2477 - val_acc: 0.9037 - val_auc: 0.9610 - 16s/epoch - 56ms/step
Epoch 8/100
292/292 - 16s - loss: 0.2573 - acc: 0.8961 - auc: 0.9584 - val_loss: 0.2499 - val_acc: 0.9008 - val_auc: 0.9592 - 16s/epoch - 56ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2552 - acc: 0.8953 - auc: 0.9589 - val_loss: 0.2586 - val_acc: 0.9066 - val_auc: 0.9585 - 21s/epoch - 71ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2504 - acc: 0.8974 - auc: 0.9605 - val_loss: 0.2466 - val_acc: 0.9066 - val_auc: 0.9622 - 21s/epoch - 72ms/step
Epoch 11/100
292/292 - 18s - loss: 0.2481 - acc: 0.9001 - auc: 0.9615 - val_loss: 0.2538 - val_acc: 0.9017 - val_auc: 0.9589 - 18s/epoch - 62ms/step
Epoch 12/100
292/292 - 21s - loss: 0.2489 - acc: 0.8973 - auc: 0.9617 - val_loss: 0.2370 - val_acc: 0.9046 - val_auc: 0.9645 - 21s/epoch - 72ms/step
Epoch 13/100
292/292 - 17s - loss: 0.2445 - acc: 0.9014 - auc: 0.9625 - val_loss: 0.2429 - val_acc: 0.9056 - val_auc: 0.9620 - 17s/epoch - 58ms/step
Epoch 14/100
292/292 - 17s - loss: 0.2400 - acc: 0.9020 - auc: 0.9644 - val_loss: 0.2353 - val_acc: 0.9094 - val_auc: 0.9657 - 17s/epoch - 60ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2386 - acc: 0.9000 - auc: 0.9646 - val_loss: 0.2449 - val_acc: 0.9094 - val_auc: 0.9624 - 17s/epoch - 60ms/step
Epoch 16/100
292/292 - 19s - loss: 0.2354 - acc: 0.9021 - auc: 0.9656 - val_loss: 0.2394 - val_acc: 0.9085 - val_auc: 0.9653 - 19s/epoch - 66ms/step
Epoch 17/100
292/292 - 21s - loss: 0.2324 - acc: 0.9025 - auc: 0.9667 - val_loss: 0.2493 - val_acc: 0.9094 - val_auc: 0.9619 - 21s/epoch - 70ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2286 - acc: 0.9069 - auc: 0.9672 - val_loss: 0.2390 - val_acc: 0.9075 - val_auc: 0.9650 - 21s/epoch - 71ms/step
Epoch 19/100
292/292 - 20s - loss: 0.2243 - acc: 0.9102 - auc: 0.9691 - val_loss: 0.2412 - val_acc: 0.9037 - val_auc: 0.9642 - 20s/epoch - 67ms/step
Epoch 20/100
292/292 - 17s - loss: 0.2241 - acc: 0.9082 - auc: 0.9688 - val_loss: 0.2329 - val_acc: 0.9104 - val_auc: 0.9653 - 17s/epoch - 58ms/step
Epoch 21/100
292/292 - 17s - loss: 0.2187 - acc: 0.9121 - auc: 0.9700 - val_loss: 0.2358 - val_acc: 0.9037 - val_auc: 0.9650 - 17s/epoch - 60ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2151 - acc: 0.9139 - auc: 0.9711 - val_loss: 0.2406 - val_acc: 0.9094 - val_auc: 0.9644 - 21s/epoch - 71ms/step
Epoch 23/100
292/292 - 18s - loss: 0.2127 - acc: 0.9155 - auc: 0.9716 - val_loss: 0.2357 - val_acc: 0.9114 - val_auc: 0.9652 - 18s/epoch - 63ms/step
Epoch 24/100
292/292 - 17s - loss: 0.2058 - acc: 0.9171 - auc: 0.9737 - val_loss: 0.2479 - val_acc: 0.9085 - val_auc: 0.9628 - 17s/epoch - 57ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
33/33 - 1s - 773ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.91      0.85      0.88       403
           1       0.91      0.95      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.60      0.49      0.54       635

    accuracy                           0.49      1038
   macro avg       0.49      0.49      0.48      1038
weighted avg       0.51      0.49      0.49      1038

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 22s - loss: 0.3219 - acc: 0.8629 - auc: 0.9343 - val_loss: 0.2566 - val_acc: 0.9008 - val_auc: 0.9582 - 22s/epoch - 76ms/step
Epoch 2/100
292/292 - 17s - loss: 0.2852 - acc: 0.8851 - auc: 0.9477 - val_loss: 0.2566 - val_acc: 0.8882 - val_auc: 0.9614 - 17s/epoch - 58ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2771 - acc: 0.8883 - auc: 0.9507 - val_loss: 0.2461 - val_acc: 0.9056 - val_auc: 0.9640 - 19s/epoch - 65ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2694 - acc: 0.8912 - auc: 0.9535 - val_loss: 0.2583 - val_acc: 0.8873 - val_auc: 0.9624 - 21s/epoch - 72ms/step
Epoch 5/100
292/292 - 20s - loss: 0.2636 - acc: 0.8927 - auc: 0.9559 - val_loss: 0.2257 - val_acc: 0.9104 - val_auc: 0.9688 - 20s/epoch - 70ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2678 - acc: 0.8912 - auc: 0.9547 - val_loss: 0.2272 - val_acc: 0.9104 - val_auc: 0.9680 - 21s/epoch - 71ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2603 - acc: 0.8948 - auc: 0.9569 - val_loss: 0.2215 - val_acc: 0.9094 - val_auc: 0.9703 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2579 - acc: 0.8966 - auc: 0.9579 - val_loss: 0.2275 - val_acc: 0.9123 - val_auc: 0.9693 - 21s/epoch - 71ms/step
Epoch 9/100
292/292 - 20s - loss: 0.2588 - acc: 0.8948 - auc: 0.9580 - val_loss: 0.2225 - val_acc: 0.9056 - val_auc: 0.9710 - 20s/epoch - 70ms/step
Epoch 10/100
292/292 - 20s - loss: 0.2559 - acc: 0.8986 - auc: 0.9582 - val_loss: 0.2398 - val_acc: 0.8979 - val_auc: 0.9671 - 20s/epoch - 70ms/step
Epoch 11/100
292/292 - 17s - loss: 0.2529 - acc: 0.8979 - auc: 0.9601 - val_loss: 0.2212 - val_acc: 0.9075 - val_auc: 0.9718 - 17s/epoch - 60ms/step
Epoch 12/100
292/292 - 21s - loss: 0.2516 - acc: 0.8975 - auc: 0.9604 - val_loss: 0.2242 - val_acc: 0.9046 - val_auc: 0.9715 - 21s/epoch - 71ms/step
Epoch 13/100
292/292 - 21s - loss: 0.2454 - acc: 0.8985 - auc: 0.9625 - val_loss: 0.2254 - val_acc: 0.9133 - val_auc: 0.9709 - 21s/epoch - 71ms/step
Epoch 14/100
292/292 - 18s - loss: 0.2490 - acc: 0.8998 - auc: 0.9614 - val_loss: 0.2178 - val_acc: 0.9075 - val_auc: 0.9721 - 18s/epoch - 62ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2437 - acc: 0.9021 - auc: 0.9632 - val_loss: 0.2202 - val_acc: 0.9094 - val_auc: 0.9721 - 17s/epoch - 58ms/step
Epoch 16/100
292/292 - 17s - loss: 0.2395 - acc: 0.9055 - auc: 0.9643 - val_loss: 0.2414 - val_acc: 0.8911 - val_auc: 0.9666 - 17s/epoch - 57ms/step
Epoch 17/100
292/292 - 17s - loss: 0.2406 - acc: 0.9024 - auc: 0.9644 - val_loss: 0.2179 - val_acc: 0.9104 - val_auc: 0.9721 - 17s/epoch - 57ms/step
Epoch 18/100
292/292 - 17s - loss: 0.2379 - acc: 0.9037 - auc: 0.9651 - val_loss: 0.2288 - val_acc: 0.9046 - val_auc: 0.9691 - 17s/epoch - 58ms/step
Epoch 19/100
292/292 - 17s - loss: 0.2323 - acc: 0.9091 - auc: 0.9662 - val_loss: 0.2282 - val_acc: 0.9017 - val_auc: 0.9682 - 17s/epoch - 58ms/step
Epoch 20/100
292/292 - 18s - loss: 0.2307 - acc: 0.9076 - auc: 0.9667 - val_loss: 0.2264 - val_acc: 0.9085 - val_auc: 0.9698 - 18s/epoch - 61ms/step
Epoch 21/100
292/292 - 17s - loss: 0.2260 - acc: 0.9089 - auc: 0.9679 - val_loss: 0.2338 - val_acc: 0.9008 - val_auc: 0.9682 - 17s/epoch - 59ms/step
Epoch 22/100
292/292 - 17s - loss: 0.2203 - acc: 0.9129 - auc: 0.9700 - val_loss: 0.2700 - val_acc: 0.8911 - val_auc: 0.9599 - 17s/epoch - 58ms/step
Epoch 23/100
292/292 - 17s - loss: 0.2198 - acc: 0.9098 - auc: 0.9704 - val_loss: 0.2284 - val_acc: 0.9114 - val_auc: 0.9688 - 17s/epoch - 60ms/step
Epoch 24/100
292/292 - 18s - loss: 0.2144 - acc: 0.9150 - auc: 0.9718 - val_loss: 0.2474 - val_acc: 0.9046 - val_auc: 0.9640 - 18s/epoch - 63ms/step
Epoch 25/100
292/292 - 17s - loss: 0.2092 - acc: 0.9157 - auc: 0.9730 - val_loss: 0.2444 - val_acc: 0.9066 - val_auc: 0.9675 - 17s/epoch - 59ms/step
Epoch 26/100
292/292 - 18s - loss: 0.2059 - acc: 0.9151 - auc: 0.9741 - val_loss: 0.2541 - val_acc: 0.9056 - val_auc: 0.9602 - 18s/epoch - 61ms/step
Epoch 27/100
292/292 - 16s - loss: 0.1998 - acc: 0.9202 - auc: 0.9758 - val_loss: 0.2470 - val_acc: 0.9085 - val_auc: 0.9649 - 16s/epoch - 56ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
33/33 - 1s - 796ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.88       403
           1       0.91      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.46      0.42       403
           1       0.60      0.52      0.56       635

    accuracy                           0.50      1038
   macro avg       0.49      0.49      0.49      1038
weighted avg       0.52      0.50      0.50      1038

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3191 - acc: 0.8654 - auc: 0.9363 - val_loss: 0.2712 - val_acc: 0.8863 - val_auc: 0.9556 - 23s/epoch - 78ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2856 - acc: 0.8855 - auc: 0.9475 - val_loss: 0.2535 - val_acc: 0.8979 - val_auc: 0.9601 - 21s/epoch - 71ms/step
Epoch 3/100
292/292 - 21s - loss: 0.2795 - acc: 0.8892 - auc: 0.9499 - val_loss: 0.2397 - val_acc: 0.9027 - val_auc: 0.9658 - 21s/epoch - 73ms/step
Epoch 4/100
292/292 - 19s - loss: 0.2743 - acc: 0.8915 - auc: 0.9516 - val_loss: 0.2338 - val_acc: 0.8979 - val_auc: 0.9657 - 19s/epoch - 66ms/step
Epoch 5/100
292/292 - 21s - loss: 0.2690 - acc: 0.8914 - auc: 0.9538 - val_loss: 0.2356 - val_acc: 0.9008 - val_auc: 0.9661 - 21s/epoch - 72ms/step
Epoch 6/100
292/292 - 17s - loss: 0.2638 - acc: 0.8953 - auc: 0.9551 - val_loss: 0.2372 - val_acc: 0.9085 - val_auc: 0.9670 - 17s/epoch - 59ms/step
Epoch 7/100
292/292 - 17s - loss: 0.2651 - acc: 0.8963 - auc: 0.9552 - val_loss: 0.2453 - val_acc: 0.8988 - val_auc: 0.9639 - 17s/epoch - 57ms/step
Epoch 8/100
292/292 - 17s - loss: 0.2592 - acc: 0.8959 - auc: 0.9578 - val_loss: 0.2279 - val_acc: 0.9066 - val_auc: 0.9666 - 17s/epoch - 57ms/step
Epoch 9/100
292/292 - 17s - loss: 0.2596 - acc: 0.8955 - auc: 0.9576 - val_loss: 0.2342 - val_acc: 0.9094 - val_auc: 0.9660 - 17s/epoch - 59ms/step
Epoch 10/100
292/292 - 17s - loss: 0.2579 - acc: 0.8963 - auc: 0.9580 - val_loss: 0.2225 - val_acc: 0.9085 - val_auc: 0.9686 - 17s/epoch - 58ms/step
Epoch 11/100
292/292 - 17s - loss: 0.2544 - acc: 0.8980 - auc: 0.9592 - val_loss: 0.2200 - val_acc: 0.9094 - val_auc: 0.9698 - 17s/epoch - 57ms/step
Epoch 12/100
292/292 - 17s - loss: 0.2535 - acc: 0.8991 - auc: 0.9596 - val_loss: 0.2257 - val_acc: 0.9104 - val_auc: 0.9687 - 17s/epoch - 59ms/step
Epoch 13/100
292/292 - 20s - loss: 0.2499 - acc: 0.8991 - auc: 0.9609 - val_loss: 0.2202 - val_acc: 0.9114 - val_auc: 0.9713 - 20s/epoch - 69ms/step
Epoch 14/100
292/292 - 18s - loss: 0.2492 - acc: 0.8989 - auc: 0.9611 - val_loss: 0.2147 - val_acc: 0.9133 - val_auc: 0.9715 - 18s/epoch - 62ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2466 - acc: 0.9014 - auc: 0.9621 - val_loss: 0.2168 - val_acc: 0.9104 - val_auc: 0.9725 - 17s/epoch - 59ms/step
Epoch 16/100
292/292 - 17s - loss: 0.2451 - acc: 0.9013 - auc: 0.9627 - val_loss: 0.2156 - val_acc: 0.9123 - val_auc: 0.9710 - 17s/epoch - 57ms/step
Epoch 17/100
292/292 - 20s - loss: 0.2440 - acc: 0.9032 - auc: 0.9628 - val_loss: 0.2307 - val_acc: 0.9066 - val_auc: 0.9675 - 20s/epoch - 67ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2420 - acc: 0.9035 - auc: 0.9633 - val_loss: 0.2091 - val_acc: 0.9200 - val_auc: 0.9735 - 21s/epoch - 71ms/step
Epoch 19/100
292/292 - 17s - loss: 0.2389 - acc: 0.9025 - auc: 0.9651 - val_loss: 0.2115 - val_acc: 0.9143 - val_auc: 0.9737 - 17s/epoch - 57ms/step
Epoch 20/100
292/292 - 16s - loss: 0.2368 - acc: 0.9039 - auc: 0.9652 - val_loss: 0.2176 - val_acc: 0.9123 - val_auc: 0.9713 - 16s/epoch - 56ms/step
Epoch 21/100
292/292 - 19s - loss: 0.2304 - acc: 0.9063 - auc: 0.9674 - val_loss: 0.2068 - val_acc: 0.9094 - val_auc: 0.9745 - 19s/epoch - 65ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2298 - acc: 0.9073 - auc: 0.9674 - val_loss: 0.2092 - val_acc: 0.9104 - val_auc: 0.9740 - 21s/epoch - 72ms/step
Epoch 23/100
292/292 - 21s - loss: 0.2305 - acc: 0.9076 - auc: 0.9667 - val_loss: 0.2123 - val_acc: 0.9056 - val_auc: 0.9728 - 21s/epoch - 71ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2271 - acc: 0.9084 - auc: 0.9684 - val_loss: 0.2125 - val_acc: 0.9143 - val_auc: 0.9728 - 21s/epoch - 70ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2218 - acc: 0.9108 - auc: 0.9693 - val_loss: 0.2208 - val_acc: 0.9075 - val_auc: 0.9718 - 21s/epoch - 72ms/step
Epoch 26/100
292/292 - 21s - loss: 0.2197 - acc: 0.9121 - auc: 0.9699 - val_loss: 0.2196 - val_acc: 0.9104 - val_auc: 0.9714 - 21s/epoch - 72ms/step
Epoch 27/100
292/292 - 20s - loss: 0.2161 - acc: 0.9119 - auc: 0.9709 - val_loss: 0.2173 - val_acc: 0.9075 - val_auc: 0.9712 - 20s/epoch - 69ms/step
Epoch 28/100
292/292 - 18s - loss: 0.2124 - acc: 0.9158 - auc: 0.9723 - val_loss: 0.2334 - val_acc: 0.8998 - val_auc: 0.9684 - 18s/epoch - 60ms/step
Epoch 29/100
292/292 - 17s - loss: 0.2065 - acc: 0.9167 - auc: 0.9734 - val_loss: 0.2204 - val_acc: 0.9085 - val_auc: 0.9709 - 17s/epoch - 57ms/step
Epoch 30/100
292/292 - 17s - loss: 0.2057 - acc: 0.9207 - auc: 0.9733 - val_loss: 0.2223 - val_acc: 0.9114 - val_auc: 0.9715 - 17s/epoch - 57ms/step
Epoch 31/100
292/292 - 17s - loss: 0.1991 - acc: 0.9195 - auc: 0.9756 - val_loss: 0.2335 - val_acc: 0.9075 - val_auc: 0.9669 - 17s/epoch - 59ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
33/33 - 1s - 845ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.90      0.86      0.88       403
           1       0.91      0.94      0.93       635

    accuracy                           0.91      1038
   macro avg       0.91      0.90      0.90      1038
weighted avg       0.91      0.91      0.91      1038

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.52      0.44       403
           1       0.61      0.48      0.54       635

    accuracy                           0.50      1038
   macro avg       0.50      0.50      0.49      1038
weighted avg       0.53      0.50      0.50      1038

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3157 - acc: 0.8695 - auc: 0.9374 - val_loss: 0.2887 - val_acc: 0.8824 - val_auc: 0.9541 - 23s/epoch - 80ms/step
Epoch 2/100
292/292 - 20s - loss: 0.2855 - acc: 0.8833 - auc: 0.9476 - val_loss: 0.2724 - val_acc: 0.8824 - val_auc: 0.9556 - 20s/epoch - 69ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2710 - acc: 0.8918 - auc: 0.9525 - val_loss: 0.2658 - val_acc: 0.8881 - val_auc: 0.9603 - 19s/epoch - 67ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2650 - acc: 0.8952 - auc: 0.9551 - val_loss: 0.2565 - val_acc: 0.8872 - val_auc: 0.9607 - 21s/epoch - 72ms/step
Epoch 5/100
292/292 - 20s - loss: 0.2616 - acc: 0.8927 - auc: 0.9562 - val_loss: 0.2527 - val_acc: 0.8930 - val_auc: 0.9620 - 20s/epoch - 70ms/step
Epoch 6/100
292/292 - 20s - loss: 0.2622 - acc: 0.8948 - auc: 0.9558 - val_loss: 0.2676 - val_acc: 0.8785 - val_auc: 0.9584 - 20s/epoch - 68ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2588 - acc: 0.8975 - auc: 0.9571 - val_loss: 0.2522 - val_acc: 0.8930 - val_auc: 0.9638 - 21s/epoch - 72ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2562 - acc: 0.8971 - auc: 0.9584 - val_loss: 0.2480 - val_acc: 0.8978 - val_auc: 0.9648 - 21s/epoch - 72ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2531 - acc: 0.8991 - auc: 0.9591 - val_loss: 0.2551 - val_acc: 0.8987 - val_auc: 0.9629 - 21s/epoch - 72ms/step
Epoch 10/100
292/292 - 17s - loss: 0.2494 - acc: 0.9003 - auc: 0.9608 - val_loss: 0.2558 - val_acc: 0.8978 - val_auc: 0.9633 - 17s/epoch - 58ms/step
Epoch 11/100
292/292 - 17s - loss: 0.2487 - acc: 0.9014 - auc: 0.9612 - val_loss: 0.2618 - val_acc: 0.8901 - val_auc: 0.9609 - 17s/epoch - 59ms/step
Epoch 12/100
292/292 - 20s - loss: 0.2476 - acc: 0.9016 - auc: 0.9613 - val_loss: 0.2660 - val_acc: 0.8872 - val_auc: 0.9583 - 20s/epoch - 69ms/step
Epoch 13/100
292/292 - 21s - loss: 0.2447 - acc: 0.9020 - auc: 0.9624 - val_loss: 0.2489 - val_acc: 0.8901 - val_auc: 0.9646 - 21s/epoch - 71ms/step
Epoch 14/100
292/292 - 19s - loss: 0.2420 - acc: 0.9031 - auc: 0.9630 - val_loss: 0.2501 - val_acc: 0.8959 - val_auc: 0.9637 - 19s/epoch - 64ms/step
Epoch 15/100
292/292 - 21s - loss: 0.2388 - acc: 0.9037 - auc: 0.9645 - val_loss: 0.2571 - val_acc: 0.8891 - val_auc: 0.9620 - 21s/epoch - 71ms/step
Epoch 16/100
292/292 - 21s - loss: 0.2376 - acc: 0.9064 - auc: 0.9646 - val_loss: 0.2505 - val_acc: 0.8920 - val_auc: 0.9620 - 21s/epoch - 72ms/step
Epoch 17/100
292/292 - 21s - loss: 0.2346 - acc: 0.9055 - auc: 0.9657 - val_loss: 0.2512 - val_acc: 0.8968 - val_auc: 0.9653 - 21s/epoch - 72ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2332 - acc: 0.9057 - auc: 0.9663 - val_loss: 0.2503 - val_acc: 0.8901 - val_auc: 0.9627 - 21s/epoch - 72ms/step
Epoch 19/100
292/292 - 21s - loss: 0.2318 - acc: 0.9098 - auc: 0.9664 - val_loss: 0.2475 - val_acc: 0.8891 - val_auc: 0.9644 - 21s/epoch - 72ms/step
Epoch 20/100
292/292 - 21s - loss: 0.2306 - acc: 0.9064 - auc: 0.9670 - val_loss: 0.2673 - val_acc: 0.8939 - val_auc: 0.9578 - 21s/epoch - 71ms/step
Epoch 21/100
292/292 - 20s - loss: 0.2262 - acc: 0.9102 - auc: 0.9686 - val_loss: 0.2510 - val_acc: 0.8910 - val_auc: 0.9633 - 20s/epoch - 69ms/step
Epoch 22/100
292/292 - 19s - loss: 0.2178 - acc: 0.9135 - auc: 0.9707 - val_loss: 0.2555 - val_acc: 0.8949 - val_auc: 0.9606 - 19s/epoch - 67ms/step
Epoch 23/100
292/292 - 18s - loss: 0.2175 - acc: 0.9110 - auc: 0.9706 - val_loss: 0.2601 - val_acc: 0.8968 - val_auc: 0.9650 - 18s/epoch - 60ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2142 - acc: 0.9132 - auc: 0.9711 - val_loss: 0.2627 - val_acc: 0.8891 - val_auc: 0.9616 - 21s/epoch - 71ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2108 - acc: 0.9153 - auc: 0.9724 - val_loss: 0.2583 - val_acc: 0.8978 - val_auc: 0.9611 - 21s/epoch - 72ms/step
Epoch 26/100
292/292 - 21s - loss: 0.2090 - acc: 0.9158 - auc: 0.9732 - val_loss: 0.2478 - val_acc: 0.8987 - val_auc: 0.9670 - 21s/epoch - 73ms/step
Epoch 27/100
292/292 - 21s - loss: 0.2027 - acc: 0.9199 - auc: 0.9745 - val_loss: 0.2570 - val_acc: 0.8997 - val_auc: 0.9636 - 21s/epoch - 71ms/step
Epoch 28/100
292/292 - 18s - loss: 0.1976 - acc: 0.9226 - auc: 0.9756 - val_loss: 0.3035 - val_acc: 0.8785 - val_auc: 0.9495 - 18s/epoch - 62ms/step
Epoch 29/100
292/292 - 17s - loss: 0.1965 - acc: 0.9195 - auc: 0.9760 - val_loss: 0.2795 - val_acc: 0.8881 - val_auc: 0.9603 - 17s/epoch - 57ms/step
Epoch 30/100
292/292 - 16s - loss: 0.1877 - acc: 0.9257 - auc: 0.9782 - val_loss: 0.2844 - val_acc: 0.8910 - val_auc: 0.9541 - 16s/epoch - 56ms/step
Epoch 31/100
292/292 - 19s - loss: 0.1826 - acc: 0.9300 - auc: 0.9796 - val_loss: 0.3168 - val_acc: 0.8833 - val_auc: 0.9442 - 19s/epoch - 67ms/step
Epoch 32/100
292/292 - 19s - loss: 0.1789 - acc: 0.9294 - auc: 0.9802 - val_loss: 0.3100 - val_acc: 0.8833 - val_auc: 0.9475 - 19s/epoch - 65ms/step
Epoch 33/100
292/292 - 20s - loss: 0.1682 - acc: 0.9366 - auc: 0.9819 - val_loss: 0.2932 - val_acc: 0.8881 - val_auc: 0.9520 - 20s/epoch - 70ms/step
Epoch 34/100
292/292 - 21s - loss: 0.1689 - acc: 0.9339 - auc: 0.9819 - val_loss: 0.3223 - val_acc: 0.8824 - val_auc: 0.9433 - 21s/epoch - 72ms/step
Epoch 35/100
292/292 - 21s - loss: 0.1618 - acc: 0.9363 - auc: 0.9835 - val_loss: 0.3103 - val_acc: 0.8891 - val_auc: 0.9520 - 21s/epoch - 72ms/step
Epoch 36/100
292/292 - 21s - loss: 0.1548 - acc: 0.9399 - auc: 0.9846 - val_loss: 0.3444 - val_acc: 0.8804 - val_auc: 0.9382 - 21s/epoch - 72ms/step
Early stopping epoch: 35
******Evaluating TEST set*********
33/33 - 1s - 871ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.87      0.87      0.87       403
           1       0.92      0.92      0.92       634

    accuracy                           0.90      1037
   macro avg       0.89      0.89      0.89      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.61      0.48      0.54       634

    accuracy                           0.49      1037
   macro avg       0.50      0.50      0.49      1037
weighted avg       0.52      0.49      0.50      1037

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 24s - loss: 0.3136 - acc: 0.8638 - auc: 0.9382 - val_loss: 0.3072 - val_acc: 0.8785 - val_auc: 0.9384 - 24s/epoch - 81ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2790 - acc: 0.8895 - auc: 0.9504 - val_loss: 0.3121 - val_acc: 0.8795 - val_auc: 0.9397 - 21s/epoch - 72ms/step
Epoch 3/100
292/292 - 21s - loss: 0.2682 - acc: 0.8909 - auc: 0.9538 - val_loss: 0.2968 - val_acc: 0.8833 - val_auc: 0.9433 - 21s/epoch - 71ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2608 - acc: 0.8974 - auc: 0.9566 - val_loss: 0.2991 - val_acc: 0.8843 - val_auc: 0.9443 - 21s/epoch - 72ms/step
Epoch 5/100
292/292 - 21s - loss: 0.2592 - acc: 0.8976 - auc: 0.9576 - val_loss: 0.2994 - val_acc: 0.8852 - val_auc: 0.9454 - 21s/epoch - 73ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2569 - acc: 0.8969 - auc: 0.9582 - val_loss: 0.2856 - val_acc: 0.8872 - val_auc: 0.9468 - 21s/epoch - 72ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2542 - acc: 0.8987 - auc: 0.9594 - val_loss: 0.2881 - val_acc: 0.8881 - val_auc: 0.9476 - 21s/epoch - 73ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2532 - acc: 0.8982 - auc: 0.9598 - val_loss: 0.2843 - val_acc: 0.8910 - val_auc: 0.9510 - 21s/epoch - 73ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2518 - acc: 0.8989 - auc: 0.9607 - val_loss: 0.3132 - val_acc: 0.8843 - val_auc: 0.9389 - 21s/epoch - 72ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2505 - acc: 0.8985 - auc: 0.9607 - val_loss: 0.2805 - val_acc: 0.8872 - val_auc: 0.9516 - 21s/epoch - 72ms/step
Epoch 11/100
292/292 - 21s - loss: 0.2456 - acc: 0.8988 - auc: 0.9627 - val_loss: 0.2782 - val_acc: 0.8852 - val_auc: 0.9511 - 21s/epoch - 71ms/step
Epoch 12/100
292/292 - 21s - loss: 0.2459 - acc: 0.8992 - auc: 0.9629 - val_loss: 0.2854 - val_acc: 0.8901 - val_auc: 0.9501 - 21s/epoch - 73ms/step
Epoch 13/100
292/292 - 21s - loss: 0.2409 - acc: 0.9033 - auc: 0.9642 - val_loss: 0.2825 - val_acc: 0.8872 - val_auc: 0.9504 - 21s/epoch - 72ms/step
Epoch 14/100
292/292 - 21s - loss: 0.2406 - acc: 0.9031 - auc: 0.9644 - val_loss: 0.2772 - val_acc: 0.8920 - val_auc: 0.9525 - 21s/epoch - 72ms/step
Epoch 15/100
292/292 - 17s - loss: 0.2372 - acc: 0.9024 - auc: 0.9655 - val_loss: 0.2785 - val_acc: 0.8910 - val_auc: 0.9506 - 17s/epoch - 60ms/step
Epoch 16/100
292/292 - 17s - loss: 0.2370 - acc: 0.9022 - auc: 0.9657 - val_loss: 0.2801 - val_acc: 0.8872 - val_auc: 0.9515 - 17s/epoch - 58ms/step
Epoch 17/100
292/292 - 18s - loss: 0.2310 - acc: 0.9063 - auc: 0.9669 - val_loss: 0.2895 - val_acc: 0.8814 - val_auc: 0.9476 - 18s/epoch - 62ms/step
Epoch 18/100
292/292 - 21s - loss: 0.2304 - acc: 0.9099 - auc: 0.9673 - val_loss: 0.2753 - val_acc: 0.8862 - val_auc: 0.9533 - 21s/epoch - 72ms/step
Epoch 19/100
292/292 - 21s - loss: 0.2268 - acc: 0.9054 - auc: 0.9683 - val_loss: 0.3017 - val_acc: 0.8766 - val_auc: 0.9443 - 21s/epoch - 71ms/step
Epoch 20/100
292/292 - 21s - loss: 0.2277 - acc: 0.9074 - auc: 0.9680 - val_loss: 0.2983 - val_acc: 0.8862 - val_auc: 0.9434 - 21s/epoch - 71ms/step
Epoch 21/100
292/292 - 21s - loss: 0.2219 - acc: 0.9089 - auc: 0.9698 - val_loss: 0.2854 - val_acc: 0.8930 - val_auc: 0.9540 - 21s/epoch - 72ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2197 - acc: 0.9092 - auc: 0.9707 - val_loss: 0.2728 - val_acc: 0.8930 - val_auc: 0.9535 - 21s/epoch - 72ms/step
Epoch 23/100
292/292 - 21s - loss: 0.2162 - acc: 0.9100 - auc: 0.9716 - val_loss: 0.2977 - val_acc: 0.8862 - val_auc: 0.9456 - 21s/epoch - 72ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2127 - acc: 0.9144 - auc: 0.9718 - val_loss: 0.3022 - val_acc: 0.8804 - val_auc: 0.9439 - 21s/epoch - 72ms/step
Epoch 25/100
292/292 - 21s - loss: 0.2081 - acc: 0.9139 - auc: 0.9735 - val_loss: 0.2854 - val_acc: 0.8949 - val_auc: 0.9483 - 21s/epoch - 72ms/step
Epoch 26/100
292/292 - 21s - loss: 0.2019 - acc: 0.9196 - auc: 0.9747 - val_loss: 0.2889 - val_acc: 0.8949 - val_auc: 0.9490 - 21s/epoch - 72ms/step
Epoch 27/100
292/292 - 19s - loss: 0.1997 - acc: 0.9188 - auc: 0.9754 - val_loss: 0.2858 - val_acc: 0.8920 - val_auc: 0.9453 - 19s/epoch - 67ms/step
Epoch 28/100
292/292 - 19s - loss: 0.1925 - acc: 0.9214 - auc: 0.9762 - val_loss: 0.2933 - val_acc: 0.8901 - val_auc: 0.9435 - 19s/epoch - 66ms/step
Epoch 29/100
292/292 - 21s - loss: 0.1872 - acc: 0.9255 - auc: 0.9781 - val_loss: 0.2994 - val_acc: 0.8959 - val_auc: 0.9418 - 21s/epoch - 72ms/step
Epoch 30/100
292/292 - 21s - loss: 0.1842 - acc: 0.9270 - auc: 0.9790 - val_loss: 0.3069 - val_acc: 0.8862 - val_auc: 0.9415 - 21s/epoch - 72ms/step
Epoch 31/100
292/292 - 21s - loss: 0.1779 - acc: 0.9314 - auc: 0.9802 - val_loss: 0.3092 - val_acc: 0.8968 - val_auc: 0.9423 - 21s/epoch - 73ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
33/33 - 1s - 952ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.89      0.82      0.86       403
           1       0.89      0.94      0.91       634

    accuracy                           0.89      1037
   macro avg       0.89      0.88      0.89      1037
weighted avg       0.89      0.89      0.89      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       403
           1       0.62      0.50      0.56       634

    accuracy                           0.51      1037
   macro avg       0.51      0.51      0.50      1037
weighted avg       0.53      0.51      0.51      1037

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 23s - loss: 0.3195 - acc: 0.8620 - auc: 0.9357 - val_loss: 0.2651 - val_acc: 0.9016 - val_auc: 0.9527 - 23s/epoch - 80ms/step
Epoch 2/100
292/292 - 18s - loss: 0.2821 - acc: 0.8836 - auc: 0.9497 - val_loss: 0.2525 - val_acc: 0.9036 - val_auc: 0.9563 - 18s/epoch - 62ms/step
Epoch 3/100
292/292 - 19s - loss: 0.2720 - acc: 0.8911 - auc: 0.9523 - val_loss: 0.2744 - val_acc: 0.8959 - val_auc: 0.9543 - 19s/epoch - 64ms/step
Epoch 4/100
292/292 - 18s - loss: 0.2663 - acc: 0.8937 - auc: 0.9550 - val_loss: 0.2633 - val_acc: 0.8949 - val_auc: 0.9581 - 18s/epoch - 62ms/step
Epoch 5/100
292/292 - 17s - loss: 0.2634 - acc: 0.8930 - auc: 0.9558 - val_loss: 0.2485 - val_acc: 0.9007 - val_auc: 0.9598 - 17s/epoch - 60ms/step
Epoch 6/100
292/292 - 17s - loss: 0.2641 - acc: 0.8961 - auc: 0.9552 - val_loss: 0.2466 - val_acc: 0.9074 - val_auc: 0.9623 - 17s/epoch - 60ms/step
Epoch 7/100
292/292 - 17s - loss: 0.2582 - acc: 0.8948 - auc: 0.9581 - val_loss: 0.2466 - val_acc: 0.9026 - val_auc: 0.9610 - 17s/epoch - 60ms/step
Epoch 8/100
292/292 - 20s - loss: 0.2546 - acc: 0.8989 - auc: 0.9591 - val_loss: 0.2480 - val_acc: 0.9036 - val_auc: 0.9622 - 20s/epoch - 67ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2534 - acc: 0.8970 - auc: 0.9596 - val_loss: 0.2522 - val_acc: 0.8987 - val_auc: 0.9593 - 21s/epoch - 72ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2467 - acc: 0.8996 - auc: 0.9622 - val_loss: 0.2461 - val_acc: 0.8987 - val_auc: 0.9621 - 21s/epoch - 72ms/step
Epoch 11/100
292/292 - 21s - loss: 0.2468 - acc: 0.9003 - auc: 0.9620 - val_loss: 0.2738 - val_acc: 0.8997 - val_auc: 0.9536 - 21s/epoch - 72ms/step
Epoch 12/100
292/292 - 21s - loss: 0.2444 - acc: 0.9007 - auc: 0.9628 - val_loss: 0.2500 - val_acc: 0.8987 - val_auc: 0.9622 - 21s/epoch - 72ms/step
Epoch 13/100
292/292 - 21s - loss: 0.2461 - acc: 0.8982 - auc: 0.9624 - val_loss: 0.2561 - val_acc: 0.8959 - val_auc: 0.9578 - 21s/epoch - 73ms/step
Epoch 14/100
292/292 - 21s - loss: 0.2401 - acc: 0.9040 - auc: 0.9642 - val_loss: 0.2528 - val_acc: 0.9036 - val_auc: 0.9596 - 21s/epoch - 73ms/step
Epoch 15/100
292/292 - 21s - loss: 0.2387 - acc: 0.9020 - auc: 0.9651 - val_loss: 0.2557 - val_acc: 0.9036 - val_auc: 0.9589 - 21s/epoch - 72ms/step
Epoch 16/100
292/292 - 21s - loss: 0.2353 - acc: 0.9037 - auc: 0.9659 - val_loss: 0.2613 - val_acc: 0.9007 - val_auc: 0.9567 - 21s/epoch - 71ms/step
Early stopping epoch: 15
******Evaluating TEST set*********
33/33 - 1s - 795ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.89      0.87      0.88       403
           1       0.92      0.93      0.93       634

    accuracy                           0.91      1037
   macro avg       0.90      0.90      0.90      1037
weighted avg       0.91      0.91      0.91      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       403
           1       0.61      0.51      0.55       634

    accuracy                           0.50      1037
   macro avg       0.49      0.49      0.49      1037
weighted avg       0.52      0.50      0.50      1037

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 20s - loss: 0.3135 - acc: 0.8675 - auc: 0.9377 - val_loss: 0.3030 - val_acc: 0.8766 - val_auc: 0.9427 - 20s/epoch - 70ms/step
Epoch 2/100
292/292 - 18s - loss: 0.2796 - acc: 0.8866 - auc: 0.9502 - val_loss: 0.2979 - val_acc: 0.8679 - val_auc: 0.9461 - 18s/epoch - 63ms/step
Epoch 3/100
292/292 - 18s - loss: 0.2717 - acc: 0.8907 - auc: 0.9528 - val_loss: 0.2896 - val_acc: 0.8766 - val_auc: 0.9494 - 18s/epoch - 60ms/step
Epoch 4/100
292/292 - 17s - loss: 0.2620 - acc: 0.8958 - auc: 0.9561 - val_loss: 0.2877 - val_acc: 0.8795 - val_auc: 0.9496 - 17s/epoch - 59ms/step
Epoch 5/100
292/292 - 17s - loss: 0.2590 - acc: 0.8970 - auc: 0.9575 - val_loss: 0.2900 - val_acc: 0.8814 - val_auc: 0.9489 - 17s/epoch - 60ms/step
Epoch 6/100
292/292 - 17s - loss: 0.2555 - acc: 0.8981 - auc: 0.9590 - val_loss: 0.2946 - val_acc: 0.8746 - val_auc: 0.9505 - 17s/epoch - 60ms/step
Epoch 7/100
292/292 - 17s - loss: 0.2553 - acc: 0.8988 - auc: 0.9586 - val_loss: 0.2897 - val_acc: 0.8824 - val_auc: 0.9489 - 17s/epoch - 59ms/step
Epoch 8/100
292/292 - 17s - loss: 0.2526 - acc: 0.8983 - auc: 0.9592 - val_loss: 0.3022 - val_acc: 0.8737 - val_auc: 0.9504 - 17s/epoch - 59ms/step
Epoch 9/100
292/292 - 17s - loss: 0.2483 - acc: 0.8993 - auc: 0.9618 - val_loss: 0.2856 - val_acc: 0.8785 - val_auc: 0.9516 - 17s/epoch - 60ms/step
Epoch 10/100
292/292 - 17s - loss: 0.2475 - acc: 0.9002 - auc: 0.9613 - val_loss: 0.2907 - val_acc: 0.8785 - val_auc: 0.9504 - 17s/epoch - 59ms/step
Epoch 11/100
292/292 - 17s - loss: 0.2442 - acc: 0.9020 - auc: 0.9625 - val_loss: 0.3077 - val_acc: 0.8756 - val_auc: 0.9486 - 17s/epoch - 59ms/step
Epoch 12/100
292/292 - 20s - loss: 0.2430 - acc: 0.8998 - auc: 0.9631 - val_loss: 0.2951 - val_acc: 0.8775 - val_auc: 0.9484 - 20s/epoch - 68ms/step
Epoch 13/100
292/292 - 21s - loss: 0.2383 - acc: 0.9048 - auc: 0.9647 - val_loss: 0.2943 - val_acc: 0.8727 - val_auc: 0.9501 - 21s/epoch - 73ms/step
Epoch 14/100
292/292 - 21s - loss: 0.2376 - acc: 0.9042 - auc: 0.9647 - val_loss: 0.2858 - val_acc: 0.8804 - val_auc: 0.9517 - 21s/epoch - 73ms/step
Epoch 15/100
292/292 - 21s - loss: 0.2339 - acc: 0.9065 - auc: 0.9660 - val_loss: 0.2956 - val_acc: 0.8689 - val_auc: 0.9494 - 21s/epoch - 73ms/step
Epoch 16/100
292/292 - 21s - loss: 0.2322 - acc: 0.9072 - auc: 0.9667 - val_loss: 0.2911 - val_acc: 0.8689 - val_auc: 0.9516 - 21s/epoch - 70ms/step
Epoch 17/100
292/292 - 17s - loss: 0.2310 - acc: 0.9088 - auc: 0.9673 - val_loss: 0.2958 - val_acc: 0.8717 - val_auc: 0.9498 - 17s/epoch - 60ms/step
Epoch 18/100
292/292 - 17s - loss: 0.2282 - acc: 0.9069 - auc: 0.9678 - val_loss: 0.2993 - val_acc: 0.8592 - val_auc: 0.9469 - 17s/epoch - 60ms/step
Epoch 19/100
292/292 - 17s - loss: 0.2275 - acc: 0.9094 - auc: 0.9683 - val_loss: 0.3011 - val_acc: 0.8679 - val_auc: 0.9480 - 17s/epoch - 60ms/step
Epoch 20/100
292/292 - 18s - loss: 0.2225 - acc: 0.9102 - auc: 0.9694 - val_loss: 0.3007 - val_acc: 0.8756 - val_auc: 0.9477 - 18s/epoch - 62ms/step
Epoch 21/100
292/292 - 18s - loss: 0.2214 - acc: 0.9080 - auc: 0.9700 - val_loss: 0.2930 - val_acc: 0.8737 - val_auc: 0.9472 - 18s/epoch - 62ms/step
Epoch 22/100
292/292 - 20s - loss: 0.2173 - acc: 0.9114 - auc: 0.9711 - val_loss: 0.3115 - val_acc: 0.8708 - val_auc: 0.9414 - 20s/epoch - 70ms/step
Epoch 23/100
292/292 - 21s - loss: 0.2153 - acc: 0.9135 - auc: 0.9717 - val_loss: 0.3164 - val_acc: 0.8708 - val_auc: 0.9440 - 21s/epoch - 73ms/step
Epoch 24/100
292/292 - 22s - loss: 0.2110 - acc: 0.9142 - auc: 0.9726 - val_loss: 0.3317 - val_acc: 0.8640 - val_auc: 0.9368 - 22s/epoch - 74ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
33/33 - 1s - 959ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.86      0.83      0.84       403
           1       0.89      0.91      0.90       634

    accuracy                           0.88      1037
   macro avg       0.88      0.87      0.87      1037
weighted avg       0.88      0.88      0.88      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.55      0.47       403
           1       0.63      0.49      0.55       634

    accuracy                           0.51      1037
   macro avg       0.52      0.52      0.51      1037
weighted avg       0.54      0.51      0.52      1037

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
292/292 - 24s - loss: 0.3135 - acc: 0.8667 - auc: 0.9385 - val_loss: 0.3101 - val_acc: 0.8717 - val_auc: 0.9387 - 24s/epoch - 81ms/step
Epoch 2/100
292/292 - 21s - loss: 0.2756 - acc: 0.8894 - auc: 0.9516 - val_loss: 0.2787 - val_acc: 0.8978 - val_auc: 0.9477 - 21s/epoch - 73ms/step
Epoch 3/100
292/292 - 21s - loss: 0.2679 - acc: 0.8936 - auc: 0.9542 - val_loss: 0.2760 - val_acc: 0.8978 - val_auc: 0.9492 - 21s/epoch - 73ms/step
Epoch 4/100
292/292 - 21s - loss: 0.2635 - acc: 0.8967 - auc: 0.9559 - val_loss: 0.2826 - val_acc: 0.8939 - val_auc: 0.9471 - 21s/epoch - 72ms/step
Epoch 5/100
292/292 - 21s - loss: 0.2592 - acc: 0.8945 - auc: 0.9577 - val_loss: 0.2768 - val_acc: 0.8978 - val_auc: 0.9502 - 21s/epoch - 74ms/step
Epoch 6/100
292/292 - 21s - loss: 0.2560 - acc: 0.8989 - auc: 0.9585 - val_loss: 0.2840 - val_acc: 0.8881 - val_auc: 0.9474 - 21s/epoch - 73ms/step
Epoch 7/100
292/292 - 21s - loss: 0.2571 - acc: 0.8969 - auc: 0.9583 - val_loss: 0.2699 - val_acc: 0.8949 - val_auc: 0.9504 - 21s/epoch - 73ms/step
Epoch 8/100
292/292 - 21s - loss: 0.2553 - acc: 0.8972 - auc: 0.9588 - val_loss: 0.2673 - val_acc: 0.9007 - val_auc: 0.9512 - 21s/epoch - 73ms/step
Epoch 9/100
292/292 - 21s - loss: 0.2543 - acc: 0.8946 - auc: 0.9596 - val_loss: 0.2659 - val_acc: 0.9074 - val_auc: 0.9527 - 21s/epoch - 73ms/step
Epoch 10/100
292/292 - 21s - loss: 0.2483 - acc: 0.9013 - auc: 0.9614 - val_loss: 0.2705 - val_acc: 0.9007 - val_auc: 0.9512 - 21s/epoch - 72ms/step
Epoch 11/100
292/292 - 19s - loss: 0.2473 - acc: 0.9016 - auc: 0.9617 - val_loss: 0.2695 - val_acc: 0.8891 - val_auc: 0.9526 - 19s/epoch - 64ms/step
Epoch 12/100
292/292 - 18s - loss: 0.2445 - acc: 0.8990 - auc: 0.9631 - val_loss: 0.2645 - val_acc: 0.9016 - val_auc: 0.9539 - 18s/epoch - 63ms/step
Epoch 13/100
292/292 - 17s - loss: 0.2464 - acc: 0.8999 - auc: 0.9629 - val_loss: 0.2740 - val_acc: 0.8949 - val_auc: 0.9492 - 17s/epoch - 60ms/step
Epoch 14/100
292/292 - 18s - loss: 0.2423 - acc: 0.9032 - auc: 0.9633 - val_loss: 0.2578 - val_acc: 0.8987 - val_auc: 0.9561 - 18s/epoch - 61ms/step
Epoch 15/100
292/292 - 18s - loss: 0.2387 - acc: 0.9032 - auc: 0.9647 - val_loss: 0.2568 - val_acc: 0.9016 - val_auc: 0.9553 - 18s/epoch - 60ms/step
Epoch 16/100
292/292 - 18s - loss: 0.2405 - acc: 0.9035 - auc: 0.9643 - val_loss: 0.2595 - val_acc: 0.9007 - val_auc: 0.9551 - 18s/epoch - 62ms/step
Epoch 17/100
292/292 - 18s - loss: 0.2349 - acc: 0.9052 - auc: 0.9660 - val_loss: 0.2542 - val_acc: 0.8997 - val_auc: 0.9559 - 18s/epoch - 60ms/step
Epoch 18/100
292/292 - 18s - loss: 0.2350 - acc: 0.9045 - auc: 0.9659 - val_loss: 0.2701 - val_acc: 0.9007 - val_auc: 0.9518 - 18s/epoch - 61ms/step
Epoch 19/100
292/292 - 17s - loss: 0.2326 - acc: 0.9082 - auc: 0.9662 - val_loss: 0.2557 - val_acc: 0.9055 - val_auc: 0.9557 - 17s/epoch - 59ms/step
Epoch 20/100
292/292 - 17s - loss: 0.2279 - acc: 0.9085 - auc: 0.9682 - val_loss: 0.2734 - val_acc: 0.9016 - val_auc: 0.9501 - 17s/epoch - 59ms/step
Epoch 21/100
292/292 - 18s - loss: 0.2272 - acc: 0.9097 - auc: 0.9679 - val_loss: 0.2649 - val_acc: 0.9016 - val_auc: 0.9558 - 18s/epoch - 60ms/step
Epoch 22/100
292/292 - 21s - loss: 0.2258 - acc: 0.9080 - auc: 0.9689 - val_loss: 0.2678 - val_acc: 0.9007 - val_auc: 0.9530 - 21s/epoch - 71ms/step
Epoch 23/100
292/292 - 18s - loss: 0.2219 - acc: 0.9096 - auc: 0.9697 - val_loss: 0.2825 - val_acc: 0.8968 - val_auc: 0.9495 - 18s/epoch - 60ms/step
Epoch 24/100
292/292 - 21s - loss: 0.2208 - acc: 0.9115 - auc: 0.9700 - val_loss: 0.2740 - val_acc: 0.8978 - val_auc: 0.9495 - 21s/epoch - 71ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
33/33 - 1s - 937ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.89      0.84      0.87       403
           1       0.90      0.94      0.92       634

    accuracy                           0.90      1037
   macro avg       0.90      0.89      0.89      1037
weighted avg       0.90      0.90      0.90      1037

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.45      0.41       403
           1       0.60      0.52      0.56       634

    accuracy                           0.49      1037
   macro avg       0.49      0.49      0.48      1037
weighted avg       0.51      0.49      0.50      1037

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8993] IC [0.8925, 0.9060]
Mean Recall[0.8896] IC [0.8825, 0.8967]
Mean F1[0.8930] IC [0.8859, 0.9002]
Median Accuracy[0.9022]
Median Recall[0.8942]
Median F1[0.8964]
