fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 18s - loss: 0.3189 - acc: 0.8607 - auc: 0.9367 - val_loss: 0.2907 - val_acc: 0.8772 - val_auc: 0.9467 - 18s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2857 - acc: 0.8830 - auc: 0.9482 - val_loss: 0.2809 - val_acc: 0.8867 - val_auc: 0.9510 - 15s/epoch - 55ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2709 - acc: 0.8915 - auc: 0.9531 - val_loss: 0.2846 - val_acc: 0.8835 - val_auc: 0.9501 - 15s/epoch - 55ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2687 - acc: 0.8922 - auc: 0.9542 - val_loss: 0.2853 - val_acc: 0.8867 - val_auc: 0.9513 - 15s/epoch - 55ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2639 - acc: 0.8960 - auc: 0.9558 - val_loss: 0.2792 - val_acc: 0.8909 - val_auc: 0.9508 - 15s/epoch - 54ms/step
Epoch 6/100
268/268 - 14s - loss: 0.2628 - acc: 0.8977 - auc: 0.9560 - val_loss: 0.2742 - val_acc: 0.8888 - val_auc: 0.9533 - 14s/epoch - 51ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2610 - acc: 0.8938 - auc: 0.9567 - val_loss: 0.2761 - val_acc: 0.8898 - val_auc: 0.9529 - 15s/epoch - 57ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2552 - acc: 0.8979 - auc: 0.9590 - val_loss: 0.2902 - val_acc: 0.8793 - val_auc: 0.9499 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2535 - acc: 0.8979 - auc: 0.9592 - val_loss: 0.2886 - val_acc: 0.8825 - val_auc: 0.9491 - 15s/epoch - 57ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2542 - acc: 0.9020 - auc: 0.9588 - val_loss: 0.2795 - val_acc: 0.8846 - val_auc: 0.9514 - 15s/epoch - 57ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2500 - acc: 0.9004 - auc: 0.9602 - val_loss: 0.2746 - val_acc: 0.8877 - val_auc: 0.9533 - 15s/epoch - 57ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2484 - acc: 0.9012 - auc: 0.9611 - val_loss: 0.2947 - val_acc: 0.8741 - val_auc: 0.9481 - 15s/epoch - 57ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2507 - acc: 0.8993 - auc: 0.9607 - val_loss: 0.2696 - val_acc: 0.8877 - val_auc: 0.9545 - 15s/epoch - 57ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2472 - acc: 0.9008 - auc: 0.9613 - val_loss: 0.2772 - val_acc: 0.8856 - val_auc: 0.9538 - 15s/epoch - 57ms/step
Epoch 15/100
268/268 - 16s - loss: 0.2437 - acc: 0.9029 - auc: 0.9627 - val_loss: 0.2708 - val_acc: 0.8856 - val_auc: 0.9538 - 16s/epoch - 59ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2425 - acc: 0.9028 - auc: 0.9632 - val_loss: 0.2706 - val_acc: 0.8898 - val_auc: 0.9538 - 15s/epoch - 57ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2378 - acc: 0.9053 - auc: 0.9647 - val_loss: 0.2727 - val_acc: 0.8909 - val_auc: 0.9548 - 15s/epoch - 58ms/step
Epoch 18/100
268/268 - 16s - loss: 0.2366 - acc: 0.9033 - auc: 0.9650 - val_loss: 0.2873 - val_acc: 0.8888 - val_auc: 0.9534 - 16s/epoch - 58ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2342 - acc: 0.9048 - auc: 0.9661 - val_loss: 0.2742 - val_acc: 0.8909 - val_auc: 0.9529 - 15s/epoch - 58ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2338 - acc: 0.9050 - auc: 0.9660 - val_loss: 0.2714 - val_acc: 0.8888 - val_auc: 0.9559 - 15s/epoch - 58ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2287 - acc: 0.9079 - auc: 0.9676 - val_loss: 0.2679 - val_acc: 0.8898 - val_auc: 0.9554 - 15s/epoch - 55ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2277 - acc: 0.9078 - auc: 0.9676 - val_loss: 0.2781 - val_acc: 0.8877 - val_auc: 0.9500 - 15s/epoch - 58ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2208 - acc: 0.9119 - auc: 0.9699 - val_loss: 0.2664 - val_acc: 0.8993 - val_auc: 0.9554 - 15s/epoch - 58ms/step
Epoch 24/100
268/268 - 14s - loss: 0.2200 - acc: 0.9133 - auc: 0.9697 - val_loss: 0.2657 - val_acc: 0.8919 - val_auc: 0.9549 - 14s/epoch - 53ms/step
Epoch 25/100
268/268 - 14s - loss: 0.2183 - acc: 0.9124 - auc: 0.9704 - val_loss: 0.2628 - val_acc: 0.8909 - val_auc: 0.9573 - 14s/epoch - 54ms/step
Epoch 26/100
268/268 - 14s - loss: 0.2138 - acc: 0.9135 - auc: 0.9718 - val_loss: 0.2665 - val_acc: 0.8919 - val_auc: 0.9558 - 14s/epoch - 53ms/step
Epoch 27/100
268/268 - 14s - loss: 0.2088 - acc: 0.9163 - auc: 0.9733 - val_loss: 0.2676 - val_acc: 0.8961 - val_auc: 0.9568 - 14s/epoch - 53ms/step
Epoch 28/100
268/268 - 14s - loss: 0.2053 - acc: 0.9161 - auc: 0.9740 - val_loss: 0.2718 - val_acc: 0.8856 - val_auc: 0.9552 - 14s/epoch - 53ms/step
Epoch 29/100
268/268 - 14s - loss: 0.2024 - acc: 0.9162 - auc: 0.9746 - val_loss: 0.2703 - val_acc: 0.8930 - val_auc: 0.9559 - 14s/epoch - 53ms/step
Epoch 30/100
268/268 - 14s - loss: 0.1973 - acc: 0.9237 - auc: 0.9760 - val_loss: 0.2655 - val_acc: 0.8982 - val_auc: 0.9568 - 14s/epoch - 53ms/step
Epoch 31/100
268/268 - 14s - loss: 0.1890 - acc: 0.9233 - auc: 0.9783 - val_loss: 0.2791 - val_acc: 0.8867 - val_auc: 0.9545 - 14s/epoch - 53ms/step
Epoch 32/100
268/268 - 14s - loss: 0.1872 - acc: 0.9251 - auc: 0.9784 - val_loss: 0.2755 - val_acc: 0.8951 - val_auc: 0.9536 - 14s/epoch - 53ms/step
Epoch 33/100
268/268 - 14s - loss: 0.1813 - acc: 0.9299 - auc: 0.9796 - val_loss: 0.2635 - val_acc: 0.8982 - val_auc: 0.9578 - 14s/epoch - 53ms/step
Epoch 34/100
268/268 - 14s - loss: 0.1766 - acc: 0.9312 - auc: 0.9806 - val_loss: 0.2885 - val_acc: 0.8867 - val_auc: 0.9516 - 14s/epoch - 53ms/step
Epoch 35/100
268/268 - 14s - loss: 0.1679 - acc: 0.9315 - auc: 0.9829 - val_loss: 0.2730 - val_acc: 0.8940 - val_auc: 0.9554 - 14s/epoch - 54ms/step
Epoch 36/100
268/268 - 16s - loss: 0.1696 - acc: 0.9355 - auc: 0.9820 - val_loss: 0.2774 - val_acc: 0.9014 - val_auc: 0.9558 - 16s/epoch - 59ms/step
Epoch 37/100
268/268 - 16s - loss: 0.1581 - acc: 0.9375 - auc: 0.9846 - val_loss: 0.2848 - val_acc: 0.8951 - val_auc: 0.9498 - 16s/epoch - 58ms/step
Epoch 38/100
268/268 - 15s - loss: 0.1505 - acc: 0.9396 - auc: 0.9858 - val_loss: 0.2902 - val_acc: 0.8898 - val_auc: 0.9476 - 15s/epoch - 56ms/step
Epoch 39/100
268/268 - 14s - loss: 0.1464 - acc: 0.9398 - auc: 0.9864 - val_loss: 0.2979 - val_acc: 0.8888 - val_auc: 0.9490 - 14s/epoch - 53ms/step
Epoch 40/100
268/268 - 15s - loss: 0.1358 - acc: 0.9463 - auc: 0.9883 - val_loss: 0.3095 - val_acc: 0.8940 - val_auc: 0.9500 - 15s/epoch - 58ms/step
Epoch 41/100
268/268 - 15s - loss: 0.1376 - acc: 0.9441 - auc: 0.9881 - val_loss: 0.3124 - val_acc: 0.8856 - val_auc: 0.9442 - 15s/epoch - 58ms/step
Epoch 42/100
268/268 - 15s - loss: 0.1316 - acc: 0.9469 - auc: 0.9893 - val_loss: 0.3259 - val_acc: 0.8972 - val_auc: 0.9402 - 15s/epoch - 57ms/step
Epoch 43/100
268/268 - 15s - loss: 0.1219 - acc: 0.9538 - auc: 0.9906 - val_loss: 0.3528 - val_acc: 0.8751 - val_auc: 0.9323 - 15s/epoch - 58ms/step
Early stopping epoch: 42
******Evaluating TEST set*********
30/30 - 1s - 766ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.90      0.84      0.87       383
           1       0.89      0.94      0.92       570

    accuracy                           0.90       953
   macro avg       0.90      0.89      0.89       953
weighted avg       0.90      0.90      0.90       953

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.49      0.44       383
           1       0.60      0.50      0.55       570

    accuracy                           0.50       953
   macro avg       0.50      0.50      0.49       953
weighted avg       0.52      0.50      0.50       953

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 18s - loss: 0.3186 - acc: 0.8680 - auc: 0.9362 - val_loss: 0.2886 - val_acc: 0.8804 - val_auc: 0.9473 - 18s/epoch - 67ms/step
Epoch 2/100
268/268 - 16s - loss: 0.2837 - acc: 0.8828 - auc: 0.9490 - val_loss: 0.2782 - val_acc: 0.8951 - val_auc: 0.9499 - 16s/epoch - 58ms/step
Epoch 3/100
268/268 - 14s - loss: 0.2729 - acc: 0.8912 - auc: 0.9521 - val_loss: 0.2777 - val_acc: 0.8940 - val_auc: 0.9521 - 14s/epoch - 54ms/step
Epoch 4/100
268/268 - 14s - loss: 0.2666 - acc: 0.8938 - auc: 0.9550 - val_loss: 0.2726 - val_acc: 0.8961 - val_auc: 0.9525 - 14s/epoch - 52ms/step
Epoch 5/100
268/268 - 14s - loss: 0.2641 - acc: 0.8961 - auc: 0.9556 - val_loss: 0.2626 - val_acc: 0.9003 - val_auc: 0.9572 - 14s/epoch - 53ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2614 - acc: 0.8957 - auc: 0.9562 - val_loss: 0.2698 - val_acc: 0.8972 - val_auc: 0.9550 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 14s - loss: 0.2601 - acc: 0.8944 - auc: 0.9570 - val_loss: 0.2638 - val_acc: 0.8951 - val_auc: 0.9559 - 14s/epoch - 53ms/step
Epoch 8/100
268/268 - 14s - loss: 0.2542 - acc: 0.8981 - auc: 0.9594 - val_loss: 0.2635 - val_acc: 0.8940 - val_auc: 0.9576 - 14s/epoch - 53ms/step
Epoch 9/100
268/268 - 14s - loss: 0.2544 - acc: 0.8949 - auc: 0.9591 - val_loss: 0.2508 - val_acc: 0.9024 - val_auc: 0.9603 - 14s/epoch - 53ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2503 - acc: 0.8982 - auc: 0.9602 - val_loss: 0.2536 - val_acc: 0.8982 - val_auc: 0.9604 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2502 - acc: 0.8978 - auc: 0.9611 - val_loss: 0.2805 - val_acc: 0.8898 - val_auc: 0.9512 - 15s/epoch - 55ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2473 - acc: 0.9006 - auc: 0.9616 - val_loss: 0.2550 - val_acc: 0.8993 - val_auc: 0.9593 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2469 - acc: 0.8994 - auc: 0.9616 - val_loss: 0.2570 - val_acc: 0.8919 - val_auc: 0.9595 - 15s/epoch - 56ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2428 - acc: 0.9036 - auc: 0.9632 - val_loss: 0.2524 - val_acc: 0.8982 - val_auc: 0.9614 - 15s/epoch - 56ms/step
Epoch 15/100
268/268 - 14s - loss: 0.2425 - acc: 0.9030 - auc: 0.9631 - val_loss: 0.2591 - val_acc: 0.8888 - val_auc: 0.9599 - 14s/epoch - 54ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2395 - acc: 0.9022 - auc: 0.9645 - val_loss: 0.3023 - val_acc: 0.8888 - val_auc: 0.9457 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2389 - acc: 0.9035 - auc: 0.9643 - val_loss: 0.2564 - val_acc: 0.8930 - val_auc: 0.9592 - 15s/epoch - 56ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2327 - acc: 0.9058 - auc: 0.9657 - val_loss: 0.2553 - val_acc: 0.8993 - val_auc: 0.9599 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2317 - acc: 0.9049 - auc: 0.9666 - val_loss: 0.2672 - val_acc: 0.8919 - val_auc: 0.9572 - 15s/epoch - 56ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2310 - acc: 0.9044 - auc: 0.9667 - val_loss: 0.2574 - val_acc: 0.8972 - val_auc: 0.9601 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2294 - acc: 0.9063 - auc: 0.9673 - val_loss: 0.2554 - val_acc: 0.8951 - val_auc: 0.9607 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2207 - acc: 0.9118 - auc: 0.9694 - val_loss: 0.2681 - val_acc: 0.8898 - val_auc: 0.9565 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2215 - acc: 0.9119 - auc: 0.9698 - val_loss: 0.2781 - val_acc: 0.8930 - val_auc: 0.9565 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2170 - acc: 0.9103 - auc: 0.9711 - val_loss: 0.2695 - val_acc: 0.8898 - val_auc: 0.9566 - 15s/epoch - 56ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
30/30 - 1s - 746ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.88      0.86      0.87       383
           1       0.91      0.92      0.92       570

    accuracy                           0.90       953
   macro avg       0.90      0.89      0.89       953
weighted avg       0.90      0.90      0.90       953

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.52      0.46       383
           1       0.61      0.49      0.54       570

    accuracy                           0.50       953
   macro avg       0.51      0.51      0.50       953
weighted avg       0.53      0.50      0.51       953

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3165 - acc: 0.8653 - auc: 0.9374 - val_loss: 0.2815 - val_acc: 0.8867 - val_auc: 0.9474 - 17s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2848 - acc: 0.8849 - auc: 0.9483 - val_loss: 0.3032 - val_acc: 0.8783 - val_auc: 0.9420 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2774 - acc: 0.8886 - auc: 0.9507 - val_loss: 0.2707 - val_acc: 0.8951 - val_auc: 0.9570 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2700 - acc: 0.8922 - auc: 0.9531 - val_loss: 0.2781 - val_acc: 0.8856 - val_auc: 0.9523 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2663 - acc: 0.8943 - auc: 0.9550 - val_loss: 0.2602 - val_acc: 0.8982 - val_auc: 0.9588 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2632 - acc: 0.8965 - auc: 0.9564 - val_loss: 0.2567 - val_acc: 0.9003 - val_auc: 0.9599 - 15s/epoch - 55ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2605 - acc: 0.8956 - auc: 0.9572 - val_loss: 0.2549 - val_acc: 0.8993 - val_auc: 0.9598 - 15s/epoch - 56ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2575 - acc: 0.8961 - auc: 0.9577 - val_loss: 0.2585 - val_acc: 0.8972 - val_auc: 0.9607 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2549 - acc: 0.8975 - auc: 0.9587 - val_loss: 0.2497 - val_acc: 0.9003 - val_auc: 0.9637 - 15s/epoch - 56ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2545 - acc: 0.8989 - auc: 0.9590 - val_loss: 0.2510 - val_acc: 0.9003 - val_auc: 0.9608 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2524 - acc: 0.9001 - auc: 0.9597 - val_loss: 0.2471 - val_acc: 0.9024 - val_auc: 0.9616 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2527 - acc: 0.8991 - auc: 0.9594 - val_loss: 0.2681 - val_acc: 0.8909 - val_auc: 0.9569 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2483 - acc: 0.9012 - auc: 0.9610 - val_loss: 0.2478 - val_acc: 0.9003 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2481 - acc: 0.8985 - auc: 0.9615 - val_loss: 0.2496 - val_acc: 0.9024 - val_auc: 0.9599 - 15s/epoch - 56ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2448 - acc: 0.9042 - auc: 0.9625 - val_loss: 0.2469 - val_acc: 0.9035 - val_auc: 0.9627 - 15s/epoch - 56ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2437 - acc: 0.9006 - auc: 0.9630 - val_loss: 0.2425 - val_acc: 0.9045 - val_auc: 0.9634 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2404 - acc: 0.9046 - auc: 0.9635 - val_loss: 0.2482 - val_acc: 0.8961 - val_auc: 0.9614 - 15s/epoch - 57ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2394 - acc: 0.9041 - auc: 0.9639 - val_loss: 0.2413 - val_acc: 0.8972 - val_auc: 0.9623 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2382 - acc: 0.9053 - auc: 0.9647 - val_loss: 0.2419 - val_acc: 0.9066 - val_auc: 0.9636 - 15s/epoch - 57ms/step
Early stopping epoch: 18
******Evaluating TEST set*********
30/30 - 1s - 746ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.87      0.88      0.88       382
           1       0.92      0.91      0.92       571

    accuracy                           0.90       953
   macro avg       0.90      0.90      0.90       953
weighted avg       0.90      0.90      0.90       953

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.50      0.44       382
           1       0.60      0.49      0.54       571

    accuracy                           0.50       953
   macro avg       0.50      0.50      0.49       953
weighted avg       0.52      0.50      0.50       953

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3155 - acc: 0.8644 - auc: 0.9372 - val_loss: 0.3132 - val_acc: 0.8803 - val_auc: 0.9447 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2825 - acc: 0.8880 - auc: 0.9485 - val_loss: 0.2580 - val_acc: 0.9002 - val_auc: 0.9595 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2739 - acc: 0.8916 - auc: 0.9514 - val_loss: 0.2559 - val_acc: 0.8939 - val_auc: 0.9616 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2674 - acc: 0.8924 - auc: 0.9546 - val_loss: 0.2533 - val_acc: 0.8992 - val_auc: 0.9617 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2641 - acc: 0.8959 - auc: 0.9551 - val_loss: 0.2598 - val_acc: 0.8929 - val_auc: 0.9592 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2643 - acc: 0.8974 - auc: 0.9553 - val_loss: 0.2722 - val_acc: 0.8918 - val_auc: 0.9544 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2643 - acc: 0.8955 - auc: 0.9552 - val_loss: 0.2695 - val_acc: 0.8971 - val_auc: 0.9566 - 15s/epoch - 56ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2602 - acc: 0.8966 - auc: 0.9571 - val_loss: 0.2493 - val_acc: 0.9002 - val_auc: 0.9613 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2557 - acc: 0.8967 - auc: 0.9582 - val_loss: 0.2516 - val_acc: 0.8887 - val_auc: 0.9634 - 15s/epoch - 56ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2537 - acc: 0.8977 - auc: 0.9594 - val_loss: 0.2522 - val_acc: 0.8960 - val_auc: 0.9614 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2556 - acc: 0.8999 - auc: 0.9587 - val_loss: 0.2407 - val_acc: 0.8929 - val_auc: 0.9654 - 15s/epoch - 57ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2517 - acc: 0.8988 - auc: 0.9605 - val_loss: 0.2411 - val_acc: 0.8981 - val_auc: 0.9654 - 15s/epoch - 57ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2491 - acc: 0.9008 - auc: 0.9610 - val_loss: 0.2590 - val_acc: 0.8992 - val_auc: 0.9599 - 15s/epoch - 56ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2459 - acc: 0.9007 - auc: 0.9623 - val_loss: 0.2534 - val_acc: 0.8971 - val_auc: 0.9628 - 15s/epoch - 56ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2444 - acc: 0.9016 - auc: 0.9626 - val_loss: 0.2473 - val_acc: 0.9013 - val_auc: 0.9630 - 15s/epoch - 57ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2478 - acc: 0.8978 - auc: 0.9622 - val_loss: 0.2518 - val_acc: 0.9002 - val_auc: 0.9624 - 15s/epoch - 57ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2421 - acc: 0.9034 - auc: 0.9636 - val_loss: 0.2414 - val_acc: 0.9023 - val_auc: 0.9644 - 15s/epoch - 57ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2403 - acc: 0.9035 - auc: 0.9636 - val_loss: 0.2563 - val_acc: 0.8939 - val_auc: 0.9600 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2352 - acc: 0.9065 - auc: 0.9654 - val_loss: 0.2451 - val_acc: 0.9055 - val_auc: 0.9631 - 15s/epoch - 56ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2331 - acc: 0.9056 - auc: 0.9657 - val_loss: 0.2560 - val_acc: 0.8950 - val_auc: 0.9599 - 15s/epoch - 55ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2316 - acc: 0.9079 - auc: 0.9668 - val_loss: 0.2412 - val_acc: 0.8971 - val_auc: 0.9656 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2282 - acc: 0.9074 - auc: 0.9674 - val_loss: 0.2641 - val_acc: 0.8887 - val_auc: 0.9583 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2307 - acc: 0.9053 - auc: 0.9670 - val_loss: 0.2508 - val_acc: 0.9002 - val_auc: 0.9638 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2209 - acc: 0.9112 - auc: 0.9697 - val_loss: 0.2533 - val_acc: 0.8960 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2221 - acc: 0.9119 - auc: 0.9696 - val_loss: 0.2569 - val_acc: 0.8992 - val_auc: 0.9617 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2175 - acc: 0.9153 - auc: 0.9702 - val_loss: 0.2735 - val_acc: 0.8834 - val_auc: 0.9546 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2130 - acc: 0.9169 - auc: 0.9715 - val_loss: 0.2513 - val_acc: 0.9044 - val_auc: 0.9630 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2068 - acc: 0.9159 - auc: 0.9731 - val_loss: 0.2647 - val_acc: 0.8897 - val_auc: 0.9570 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2040 - acc: 0.9202 - auc: 0.9734 - val_loss: 0.2670 - val_acc: 0.8929 - val_auc: 0.9575 - 15s/epoch - 56ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1991 - acc: 0.9208 - auc: 0.9752 - val_loss: 0.2849 - val_acc: 0.8887 - val_auc: 0.9513 - 15s/epoch - 56ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1969 - acc: 0.9203 - auc: 0.9760 - val_loss: 0.3121 - val_acc: 0.8834 - val_auc: 0.9478 - 15s/epoch - 56ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
30/30 - 1s - 743ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.89      0.85      0.87       382
           1       0.90      0.93      0.92       570

    accuracy                           0.90       952
   macro avg       0.90      0.89      0.89       952
weighted avg       0.90      0.90      0.90       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.48      0.43       382
           1       0.59      0.51      0.55       570

    accuracy                           0.50       952
   macro avg       0.50      0.50      0.49       952
weighted avg       0.51      0.50      0.50       952

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3130 - acc: 0.8664 - auc: 0.9394 - val_loss: 0.2768 - val_acc: 0.8918 - val_auc: 0.9515 - 17s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2869 - acc: 0.8831 - auc: 0.9475 - val_loss: 0.2585 - val_acc: 0.8950 - val_auc: 0.9568 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2770 - acc: 0.8908 - auc: 0.9507 - val_loss: 0.2545 - val_acc: 0.8992 - val_auc: 0.9587 - 15s/epoch - 54ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2723 - acc: 0.8909 - auc: 0.9527 - val_loss: 0.2591 - val_acc: 0.8887 - val_auc: 0.9599 - 15s/epoch - 55ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2664 - acc: 0.8929 - auc: 0.9548 - val_loss: 0.2511 - val_acc: 0.8908 - val_auc: 0.9611 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2649 - acc: 0.8949 - auc: 0.9553 - val_loss: 0.2494 - val_acc: 0.8897 - val_auc: 0.9620 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2595 - acc: 0.8983 - auc: 0.9569 - val_loss: 0.2469 - val_acc: 0.8929 - val_auc: 0.9627 - 15s/epoch - 55ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2583 - acc: 0.8960 - auc: 0.9578 - val_loss: 0.2471 - val_acc: 0.8960 - val_auc: 0.9618 - 15s/epoch - 55ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2567 - acc: 0.8972 - auc: 0.9583 - val_loss: 0.2425 - val_acc: 0.8981 - val_auc: 0.9644 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2555 - acc: 0.8965 - auc: 0.9583 - val_loss: 0.2485 - val_acc: 0.8981 - val_auc: 0.9614 - 15s/epoch - 55ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2540 - acc: 0.8964 - auc: 0.9589 - val_loss: 0.2476 - val_acc: 0.9002 - val_auc: 0.9622 - 15s/epoch - 55ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2506 - acc: 0.8980 - auc: 0.9600 - val_loss: 0.2531 - val_acc: 0.8908 - val_auc: 0.9624 - 15s/epoch - 55ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2528 - acc: 0.8979 - auc: 0.9597 - val_loss: 0.2434 - val_acc: 0.8971 - val_auc: 0.9645 - 15s/epoch - 55ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2481 - acc: 0.8988 - auc: 0.9615 - val_loss: 0.2395 - val_acc: 0.9013 - val_auc: 0.9651 - 15s/epoch - 55ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2444 - acc: 0.9005 - auc: 0.9624 - val_loss: 0.2421 - val_acc: 0.9013 - val_auc: 0.9640 - 15s/epoch - 55ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2429 - acc: 0.9023 - auc: 0.9631 - val_loss: 0.2476 - val_acc: 0.8971 - val_auc: 0.9622 - 15s/epoch - 55ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2400 - acc: 0.9033 - auc: 0.9640 - val_loss: 0.2758 - val_acc: 0.8845 - val_auc: 0.9607 - 15s/epoch - 55ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2382 - acc: 0.9013 - auc: 0.9647 - val_loss: 0.2400 - val_acc: 0.8939 - val_auc: 0.9657 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2386 - acc: 0.9035 - auc: 0.9645 - val_loss: 0.2464 - val_acc: 0.9065 - val_auc: 0.9612 - 15s/epoch - 55ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2328 - acc: 0.9057 - auc: 0.9663 - val_loss: 0.2517 - val_acc: 0.8876 - val_auc: 0.9625 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2275 - acc: 0.9090 - auc: 0.9678 - val_loss: 0.2602 - val_acc: 0.8887 - val_auc: 0.9587 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2266 - acc: 0.9069 - auc: 0.9680 - val_loss: 0.2469 - val_acc: 0.8981 - val_auc: 0.9606 - 15s/epoch - 55ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2232 - acc: 0.9091 - auc: 0.9691 - val_loss: 0.2521 - val_acc: 0.8971 - val_auc: 0.9607 - 15s/epoch - 55ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2198 - acc: 0.9112 - auc: 0.9704 - val_loss: 0.2498 - val_acc: 0.8918 - val_auc: 0.9620 - 15s/epoch - 55ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2166 - acc: 0.9156 - auc: 0.9708 - val_loss: 0.2606 - val_acc: 0.8939 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2156 - acc: 0.9146 - auc: 0.9715 - val_loss: 0.2460 - val_acc: 0.8971 - val_auc: 0.9621 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2069 - acc: 0.9175 - auc: 0.9732 - val_loss: 0.2611 - val_acc: 0.8992 - val_auc: 0.9602 - 15s/epoch - 55ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2054 - acc: 0.9177 - auc: 0.9735 - val_loss: 0.2636 - val_acc: 0.9002 - val_auc: 0.9519 - 15s/epoch - 56ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
30/30 - 1s - 744ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.90      0.83      0.86       382
           1       0.89      0.94      0.91       570

    accuracy                           0.89       952
   macro avg       0.89      0.88      0.89       952
weighted avg       0.89      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.47      0.42       382
           1       0.57      0.48      0.52       570

    accuracy                           0.47       952
   macro avg       0.47      0.47      0.47       952
weighted avg       0.49      0.47      0.48       952

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3165 - acc: 0.8640 - auc: 0.9375 - val_loss: 0.3139 - val_acc: 0.8687 - val_auc: 0.9411 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2795 - acc: 0.8861 - auc: 0.9503 - val_loss: 0.2917 - val_acc: 0.8834 - val_auc: 0.9468 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2712 - acc: 0.8924 - auc: 0.9535 - val_loss: 0.2900 - val_acc: 0.8929 - val_auc: 0.9484 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2666 - acc: 0.8944 - auc: 0.9546 - val_loss: 0.2739 - val_acc: 0.8929 - val_auc: 0.9507 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2598 - acc: 0.8960 - auc: 0.9576 - val_loss: 0.3041 - val_acc: 0.8824 - val_auc: 0.9425 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2603 - acc: 0.8950 - auc: 0.9578 - val_loss: 0.2747 - val_acc: 0.8918 - val_auc: 0.9509 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2576 - acc: 0.8952 - auc: 0.9581 - val_loss: 0.2778 - val_acc: 0.8939 - val_auc: 0.9504 - 15s/epoch - 56ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2556 - acc: 0.8985 - auc: 0.9589 - val_loss: 0.2800 - val_acc: 0.8887 - val_auc: 0.9506 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2533 - acc: 0.9013 - auc: 0.9597 - val_loss: 0.2763 - val_acc: 0.8929 - val_auc: 0.9514 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2499 - acc: 0.9007 - auc: 0.9606 - val_loss: 0.2986 - val_acc: 0.8803 - val_auc: 0.9466 - 15s/epoch - 55ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2499 - acc: 0.8983 - auc: 0.9611 - val_loss: 0.2719 - val_acc: 0.8971 - val_auc: 0.9535 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2472 - acc: 0.9008 - auc: 0.9617 - val_loss: 0.2810 - val_acc: 0.8981 - val_auc: 0.9509 - 15s/epoch - 55ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2478 - acc: 0.9007 - auc: 0.9619 - val_loss: 0.2800 - val_acc: 0.8960 - val_auc: 0.9499 - 15s/epoch - 55ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2445 - acc: 0.9027 - auc: 0.9625 - val_loss: 0.2767 - val_acc: 0.8971 - val_auc: 0.9517 - 15s/epoch - 55ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2396 - acc: 0.9049 - auc: 0.9642 - val_loss: 0.2750 - val_acc: 0.8939 - val_auc: 0.9529 - 15s/epoch - 55ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2404 - acc: 0.9055 - auc: 0.9635 - val_loss: 0.2757 - val_acc: 0.8960 - val_auc: 0.9530 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2343 - acc: 0.9041 - auc: 0.9654 - val_loss: 0.2747 - val_acc: 0.8908 - val_auc: 0.9522 - 15s/epoch - 55ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2329 - acc: 0.9048 - auc: 0.9660 - val_loss: 0.2736 - val_acc: 0.8866 - val_auc: 0.9538 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2325 - acc: 0.9084 - auc: 0.9667 - val_loss: 0.2835 - val_acc: 0.8855 - val_auc: 0.9488 - 15s/epoch - 55ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2278 - acc: 0.9069 - auc: 0.9677 - val_loss: 0.2987 - val_acc: 0.8929 - val_auc: 0.9528 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2270 - acc: 0.9088 - auc: 0.9674 - val_loss: 0.2819 - val_acc: 0.8908 - val_auc: 0.9502 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2210 - acc: 0.9113 - auc: 0.9696 - val_loss: 0.2833 - val_acc: 0.8887 - val_auc: 0.9517 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2190 - acc: 0.9100 - auc: 0.9700 - val_loss: 0.2921 - val_acc: 0.8887 - val_auc: 0.9490 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2215 - acc: 0.9104 - auc: 0.9699 - val_loss: 0.2910 - val_acc: 0.8939 - val_auc: 0.9532 - 15s/epoch - 56ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2130 - acc: 0.9145 - auc: 0.9719 - val_loss: 0.2725 - val_acc: 0.8971 - val_auc: 0.9544 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2110 - acc: 0.9149 - auc: 0.9727 - val_loss: 0.2829 - val_acc: 0.8908 - val_auc: 0.9527 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2058 - acc: 0.9144 - auc: 0.9743 - val_loss: 0.2861 - val_acc: 0.8897 - val_auc: 0.9527 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.1970 - acc: 0.9219 - auc: 0.9756 - val_loss: 0.2819 - val_acc: 0.8950 - val_auc: 0.9549 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.1977 - acc: 0.9212 - auc: 0.9759 - val_loss: 0.3082 - val_acc: 0.8887 - val_auc: 0.9470 - 15s/epoch - 56ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1924 - acc: 0.9233 - auc: 0.9773 - val_loss: 0.2951 - val_acc: 0.8950 - val_auc: 0.9527 - 15s/epoch - 56ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1867 - acc: 0.9264 - auc: 0.9781 - val_loss: 0.3034 - val_acc: 0.8866 - val_auc: 0.9466 - 15s/epoch - 56ms/step
Epoch 32/100
268/268 - 15s - loss: 0.1852 - acc: 0.9257 - auc: 0.9779 - val_loss: 0.2971 - val_acc: 0.8918 - val_auc: 0.9463 - 15s/epoch - 56ms/step
Epoch 33/100
268/268 - 15s - loss: 0.1798 - acc: 0.9286 - auc: 0.9795 - val_loss: 0.3136 - val_acc: 0.8887 - val_auc: 0.9433 - 15s/epoch - 55ms/step
Epoch 34/100
268/268 - 15s - loss: 0.1707 - acc: 0.9343 - auc: 0.9820 - val_loss: 0.3309 - val_acc: 0.8771 - val_auc: 0.9433 - 15s/epoch - 56ms/step
Epoch 35/100
268/268 - 15s - loss: 0.1687 - acc: 0.9350 - auc: 0.9817 - val_loss: 0.3087 - val_acc: 0.8782 - val_auc: 0.9448 - 15s/epoch - 56ms/step
Epoch 36/100
268/268 - 15s - loss: 0.1635 - acc: 0.9352 - auc: 0.9832 - val_loss: 0.3196 - val_acc: 0.8845 - val_auc: 0.9421 - 15s/epoch - 56ms/step
Epoch 37/100
268/268 - 15s - loss: 0.1584 - acc: 0.9379 - auc: 0.9843 - val_loss: 0.3180 - val_acc: 0.8918 - val_auc: 0.9440 - 15s/epoch - 56ms/step
Epoch 38/100
268/268 - 15s - loss: 0.1510 - acc: 0.9396 - auc: 0.9860 - val_loss: 0.3332 - val_acc: 0.8803 - val_auc: 0.9391 - 15s/epoch - 55ms/step
Early stopping epoch: 37
******Evaluating TEST set*********
30/30 - 1s - 731ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.82      0.86       382
           1       0.89      0.95      0.92       570

    accuracy                           0.89       952
   macro avg       0.90      0.88      0.89       952
weighted avg       0.90      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.51      0.45       382
           1       0.60      0.49      0.54       570

    accuracy                           0.50       952
   macro avg       0.50      0.50      0.49       952
weighted avg       0.52      0.50      0.50       952

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3189 - acc: 0.8628 - auc: 0.9358 - val_loss: 0.2714 - val_acc: 0.8824 - val_auc: 0.9547 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2862 - acc: 0.8836 - auc: 0.9483 - val_loss: 0.2459 - val_acc: 0.9149 - val_auc: 0.9636 - 15s/epoch - 55ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2741 - acc: 0.8890 - auc: 0.9524 - val_loss: 0.2574 - val_acc: 0.8971 - val_auc: 0.9587 - 15s/epoch - 55ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2715 - acc: 0.8915 - auc: 0.9531 - val_loss: 0.2464 - val_acc: 0.8981 - val_auc: 0.9622 - 15s/epoch - 55ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2673 - acc: 0.8934 - auc: 0.9549 - val_loss: 0.2479 - val_acc: 0.9055 - val_auc: 0.9627 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2653 - acc: 0.8948 - auc: 0.9553 - val_loss: 0.2566 - val_acc: 0.8971 - val_auc: 0.9601 - 15s/epoch - 55ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2613 - acc: 0.8944 - auc: 0.9572 - val_loss: 0.2341 - val_acc: 0.9118 - val_auc: 0.9651 - 15s/epoch - 55ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2593 - acc: 0.8965 - auc: 0.9576 - val_loss: 0.2332 - val_acc: 0.9076 - val_auc: 0.9660 - 15s/epoch - 55ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2583 - acc: 0.8956 - auc: 0.9580 - val_loss: 0.2491 - val_acc: 0.8939 - val_auc: 0.9621 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2555 - acc: 0.8966 - auc: 0.9592 - val_loss: 0.2456 - val_acc: 0.9023 - val_auc: 0.9612 - 15s/epoch - 55ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2569 - acc: 0.8970 - auc: 0.9587 - val_loss: 0.2472 - val_acc: 0.9002 - val_auc: 0.9651 - 15s/epoch - 55ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2548 - acc: 0.8948 - auc: 0.9596 - val_loss: 0.2327 - val_acc: 0.9055 - val_auc: 0.9660 - 15s/epoch - 55ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2510 - acc: 0.8977 - auc: 0.9608 - val_loss: 0.2387 - val_acc: 0.8908 - val_auc: 0.9648 - 15s/epoch - 55ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2486 - acc: 0.8977 - auc: 0.9617 - val_loss: 0.2401 - val_acc: 0.8981 - val_auc: 0.9633 - 15s/epoch - 55ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2478 - acc: 0.8974 - auc: 0.9621 - val_loss: 0.2340 - val_acc: 0.9107 - val_auc: 0.9666 - 15s/epoch - 55ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2449 - acc: 0.9009 - auc: 0.9629 - val_loss: 0.2388 - val_acc: 0.9034 - val_auc: 0.9654 - 15s/epoch - 55ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2420 - acc: 0.9008 - auc: 0.9636 - val_loss: 0.2391 - val_acc: 0.9013 - val_auc: 0.9651 - 15s/epoch - 55ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2402 - acc: 0.9047 - auc: 0.9643 - val_loss: 0.2313 - val_acc: 0.9065 - val_auc: 0.9658 - 15s/epoch - 55ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2368 - acc: 0.9005 - auc: 0.9654 - val_loss: 0.2395 - val_acc: 0.8950 - val_auc: 0.9656 - 15s/epoch - 55ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2352 - acc: 0.9023 - auc: 0.9657 - val_loss: 0.2284 - val_acc: 0.9086 - val_auc: 0.9684 - 15s/epoch - 55ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2324 - acc: 0.9054 - auc: 0.9663 - val_loss: 0.2412 - val_acc: 0.8981 - val_auc: 0.9652 - 15s/epoch - 55ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2262 - acc: 0.9074 - auc: 0.9685 - val_loss: 0.2351 - val_acc: 0.9065 - val_auc: 0.9672 - 15s/epoch - 55ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2267 - acc: 0.9085 - auc: 0.9682 - val_loss: 0.2352 - val_acc: 0.9055 - val_auc: 0.9649 - 15s/epoch - 55ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2251 - acc: 0.9078 - auc: 0.9692 - val_loss: 0.2467 - val_acc: 0.9002 - val_auc: 0.9633 - 15s/epoch - 55ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2190 - acc: 0.9114 - auc: 0.9706 - val_loss: 0.2391 - val_acc: 0.9055 - val_auc: 0.9658 - 15s/epoch - 55ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2156 - acc: 0.9140 - auc: 0.9714 - val_loss: 0.2355 - val_acc: 0.9076 - val_auc: 0.9651 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2123 - acc: 0.9161 - auc: 0.9727 - val_loss: 0.2461 - val_acc: 0.8971 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2101 - acc: 0.9179 - auc: 0.9724 - val_loss: 0.2405 - val_acc: 0.9023 - val_auc: 0.9634 - 15s/epoch - 55ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2048 - acc: 0.9190 - auc: 0.9741 - val_loss: 0.2542 - val_acc: 0.8929 - val_auc: 0.9610 - 15s/epoch - 55ms/step
Epoch 30/100
268/268 - 15s - loss: 0.2013 - acc: 0.9190 - auc: 0.9753 - val_loss: 0.2498 - val_acc: 0.8981 - val_auc: 0.9627 - 15s/epoch - 55ms/step
Early stopping epoch: 29
******Evaluating TEST set*********
30/30 - 1s - 727ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.88       382
           1       0.91      0.94      0.93       570

    accuracy                           0.91       952
   macro avg       0.91      0.90      0.90       952
weighted avg       0.91      0.91      0.91       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.55      0.48       382
           1       0.62      0.48      0.54       570

    accuracy                           0.51       952
   macro avg       0.52      0.52      0.51       952
weighted avg       0.54      0.51      0.52       952

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3101 - acc: 0.8679 - auc: 0.9395 - val_loss: 0.3439 - val_acc: 0.8498 - val_auc: 0.9271 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2833 - acc: 0.8857 - auc: 0.9491 - val_loss: 0.3039 - val_acc: 0.8803 - val_auc: 0.9403 - 15s/epoch - 57ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2718 - acc: 0.8896 - auc: 0.9530 - val_loss: 0.2956 - val_acc: 0.8855 - val_auc: 0.9441 - 15s/epoch - 57ms/step
Epoch 4/100
268/268 - 14s - loss: 0.2645 - acc: 0.8952 - auc: 0.9555 - val_loss: 0.2908 - val_acc: 0.8782 - val_auc: 0.9467 - 14s/epoch - 54ms/step
Epoch 5/100
268/268 - 14s - loss: 0.2619 - acc: 0.8949 - auc: 0.9565 - val_loss: 0.2854 - val_acc: 0.8813 - val_auc: 0.9464 - 14s/epoch - 54ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2607 - acc: 0.8977 - auc: 0.9572 - val_loss: 0.2967 - val_acc: 0.8803 - val_auc: 0.9446 - 15s/epoch - 55ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2572 - acc: 0.8967 - auc: 0.9582 - val_loss: 0.2848 - val_acc: 0.8845 - val_auc: 0.9488 - 15s/epoch - 58ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2552 - acc: 0.8959 - auc: 0.9591 - val_loss: 0.2825 - val_acc: 0.8845 - val_auc: 0.9490 - 15s/epoch - 57ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2520 - acc: 0.8998 - auc: 0.9602 - val_loss: 0.2851 - val_acc: 0.8866 - val_auc: 0.9489 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 16s - loss: 0.2492 - acc: 0.8995 - auc: 0.9607 - val_loss: 0.2945 - val_acc: 0.8834 - val_auc: 0.9491 - 16s/epoch - 60ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2509 - acc: 0.8992 - auc: 0.9609 - val_loss: 0.2851 - val_acc: 0.8897 - val_auc: 0.9487 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2450 - acc: 0.9007 - auc: 0.9626 - val_loss: 0.2839 - val_acc: 0.8803 - val_auc: 0.9495 - 15s/epoch - 55ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2427 - acc: 0.9037 - auc: 0.9631 - val_loss: 0.2879 - val_acc: 0.8813 - val_auc: 0.9485 - 15s/epoch - 54ms/step
Epoch 14/100
268/268 - 14s - loss: 0.2422 - acc: 0.9030 - auc: 0.9637 - val_loss: 0.2827 - val_acc: 0.8908 - val_auc: 0.9491 - 14s/epoch - 54ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2422 - acc: 0.9023 - auc: 0.9636 - val_loss: 0.2923 - val_acc: 0.8813 - val_auc: 0.9463 - 15s/epoch - 54ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2393 - acc: 0.9053 - auc: 0.9640 - val_loss: 0.2959 - val_acc: 0.8855 - val_auc: 0.9466 - 15s/epoch - 54ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2359 - acc: 0.9034 - auc: 0.9653 - val_loss: 0.2821 - val_acc: 0.8950 - val_auc: 0.9495 - 15s/epoch - 54ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2305 - acc: 0.9063 - auc: 0.9669 - val_loss: 0.2890 - val_acc: 0.8845 - val_auc: 0.9488 - 15s/epoch - 54ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2305 - acc: 0.9068 - auc: 0.9670 - val_loss: 0.2874 - val_acc: 0.8761 - val_auc: 0.9483 - 15s/epoch - 54ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2239 - acc: 0.9111 - auc: 0.9686 - val_loss: 0.2792 - val_acc: 0.8876 - val_auc: 0.9518 - 15s/epoch - 54ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2237 - acc: 0.9106 - auc: 0.9688 - val_loss: 0.2760 - val_acc: 0.8929 - val_auc: 0.9524 - 15s/epoch - 54ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2200 - acc: 0.9137 - auc: 0.9698 - val_loss: 0.2739 - val_acc: 0.8897 - val_auc: 0.9524 - 15s/epoch - 54ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2132 - acc: 0.9172 - auc: 0.9712 - val_loss: 0.2769 - val_acc: 0.8897 - val_auc: 0.9518 - 15s/epoch - 54ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2106 - acc: 0.9189 - auc: 0.9721 - val_loss: 0.2750 - val_acc: 0.8855 - val_auc: 0.9535 - 15s/epoch - 54ms/step
Epoch 25/100
268/268 - 14s - loss: 0.2028 - acc: 0.9190 - auc: 0.9749 - val_loss: 0.2951 - val_acc: 0.8834 - val_auc: 0.9480 - 14s/epoch - 54ms/step
Epoch 26/100
268/268 - 14s - loss: 0.2015 - acc: 0.9204 - auc: 0.9750 - val_loss: 0.2868 - val_acc: 0.8813 - val_auc: 0.9480 - 14s/epoch - 54ms/step
Epoch 27/100
268/268 - 15s - loss: 0.1997 - acc: 0.9229 - auc: 0.9746 - val_loss: 0.2888 - val_acc: 0.8918 - val_auc: 0.9508 - 15s/epoch - 55ms/step
Epoch 28/100
268/268 - 15s - loss: 0.1929 - acc: 0.9233 - auc: 0.9766 - val_loss: 0.2923 - val_acc: 0.8855 - val_auc: 0.9503 - 15s/epoch - 55ms/step
Epoch 29/100
268/268 - 15s - loss: 0.1872 - acc: 0.9258 - auc: 0.9780 - val_loss: 0.3015 - val_acc: 0.8876 - val_auc: 0.9469 - 15s/epoch - 56ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1816 - acc: 0.9267 - auc: 0.9791 - val_loss: 0.2933 - val_acc: 0.8908 - val_auc: 0.9531 - 15s/epoch - 56ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1794 - acc: 0.9314 - auc: 0.9797 - val_loss: 0.2940 - val_acc: 0.8845 - val_auc: 0.9489 - 15s/epoch - 56ms/step
Epoch 32/100
268/268 - 15s - loss: 0.1696 - acc: 0.9345 - auc: 0.9813 - val_loss: 0.3116 - val_acc: 0.8866 - val_auc: 0.9410 - 15s/epoch - 56ms/step
Epoch 33/100
268/268 - 15s - loss: 0.1624 - acc: 0.9370 - auc: 0.9832 - val_loss: 0.3142 - val_acc: 0.8897 - val_auc: 0.9473 - 15s/epoch - 55ms/step
Epoch 34/100
268/268 - 15s - loss: 0.1650 - acc: 0.9347 - auc: 0.9831 - val_loss: 0.3214 - val_acc: 0.8813 - val_auc: 0.9434 - 15s/epoch - 56ms/step
Early stopping epoch: 33
******Evaluating TEST set*********
30/30 - 1s - 741ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.88      0.83      0.85       382
           1       0.89      0.92      0.91       570

    accuracy                           0.89       952
   macro avg       0.88      0.88      0.88       952
weighted avg       0.89      0.89      0.88       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.44      0.54      0.49       382
           1       0.64      0.55      0.59       570

    accuracy                           0.55       952
   macro avg       0.54      0.54      0.54       952
weighted avg       0.56      0.55      0.55       952

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3151 - acc: 0.8707 - auc: 0.9370 - val_loss: 0.2921 - val_acc: 0.8761 - val_auc: 0.9464 - 17s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2808 - acc: 0.8886 - auc: 0.9495 - val_loss: 0.2698 - val_acc: 0.8876 - val_auc: 0.9532 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2703 - acc: 0.8914 - auc: 0.9532 - val_loss: 0.2725 - val_acc: 0.8897 - val_auc: 0.9517 - 15s/epoch - 55ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2662 - acc: 0.8939 - auc: 0.9550 - val_loss: 0.2746 - val_acc: 0.8771 - val_auc: 0.9534 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2655 - acc: 0.8930 - auc: 0.9551 - val_loss: 0.2717 - val_acc: 0.8887 - val_auc: 0.9545 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2615 - acc: 0.8960 - auc: 0.9568 - val_loss: 0.2922 - val_acc: 0.8771 - val_auc: 0.9483 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2571 - acc: 0.8979 - auc: 0.9578 - val_loss: 0.2655 - val_acc: 0.8876 - val_auc: 0.9551 - 15s/epoch - 56ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2561 - acc: 0.8987 - auc: 0.9583 - val_loss: 0.2550 - val_acc: 0.8897 - val_auc: 0.9585 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2537 - acc: 0.9005 - auc: 0.9599 - val_loss: 0.2597 - val_acc: 0.8855 - val_auc: 0.9578 - 15s/epoch - 56ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2526 - acc: 0.8994 - auc: 0.9592 - val_loss: 0.2536 - val_acc: 0.9002 - val_auc: 0.9600 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2491 - acc: 0.9006 - auc: 0.9606 - val_loss: 0.2620 - val_acc: 0.8929 - val_auc: 0.9574 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2448 - acc: 0.9000 - auc: 0.9627 - val_loss: 0.2573 - val_acc: 0.8887 - val_auc: 0.9586 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2458 - acc: 0.9005 - auc: 0.9619 - val_loss: 0.2695 - val_acc: 0.8897 - val_auc: 0.9547 - 15s/epoch - 57ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2483 - acc: 0.9008 - auc: 0.9616 - val_loss: 0.2497 - val_acc: 0.8960 - val_auc: 0.9618 - 15s/epoch - 57ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2418 - acc: 0.9025 - auc: 0.9632 - val_loss: 0.2483 - val_acc: 0.9023 - val_auc: 0.9621 - 15s/epoch - 56ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2377 - acc: 0.9044 - auc: 0.9647 - val_loss: 0.2675 - val_acc: 0.8834 - val_auc: 0.9563 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2394 - acc: 0.9028 - auc: 0.9644 - val_loss: 0.2488 - val_acc: 0.9013 - val_auc: 0.9626 - 15s/epoch - 56ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2396 - acc: 0.9035 - auc: 0.9639 - val_loss: 0.2479 - val_acc: 0.9055 - val_auc: 0.9627 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2354 - acc: 0.9064 - auc: 0.9650 - val_loss: 0.2471 - val_acc: 0.8971 - val_auc: 0.9629 - 15s/epoch - 56ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2305 - acc: 0.9082 - auc: 0.9667 - val_loss: 0.2928 - val_acc: 0.8803 - val_auc: 0.9513 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2308 - acc: 0.9090 - auc: 0.9666 - val_loss: 0.2496 - val_acc: 0.8971 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2285 - acc: 0.9075 - auc: 0.9677 - val_loss: 0.2494 - val_acc: 0.8992 - val_auc: 0.9625 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2240 - acc: 0.9107 - auc: 0.9686 - val_loss: 0.2454 - val_acc: 0.9023 - val_auc: 0.9645 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2241 - acc: 0.9112 - auc: 0.9691 - val_loss: 0.2567 - val_acc: 0.9002 - val_auc: 0.9628 - 15s/epoch - 56ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2216 - acc: 0.9127 - auc: 0.9695 - val_loss: 0.2693 - val_acc: 0.9002 - val_auc: 0.9564 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2220 - acc: 0.9102 - auc: 0.9691 - val_loss: 0.2553 - val_acc: 0.8981 - val_auc: 0.9618 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2153 - acc: 0.9132 - auc: 0.9710 - val_loss: 0.2676 - val_acc: 0.8918 - val_auc: 0.9598 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2117 - acc: 0.9158 - auc: 0.9721 - val_loss: 0.2574 - val_acc: 0.8929 - val_auc: 0.9622 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2083 - acc: 0.9172 - auc: 0.9730 - val_loss: 0.2619 - val_acc: 0.9002 - val_auc: 0.9587 - 15s/epoch - 56ms/step
Epoch 30/100
268/268 - 15s - loss: 0.2010 - acc: 0.9202 - auc: 0.9748 - val_loss: 0.2694 - val_acc: 0.8876 - val_auc: 0.9578 - 15s/epoch - 56ms/step
Epoch 31/100
268/268 - 15s - loss: 0.2000 - acc: 0.9225 - auc: 0.9748 - val_loss: 0.2665 - val_acc: 0.8971 - val_auc: 0.9595 - 15s/epoch - 56ms/step
Epoch 32/100
268/268 - 15s - loss: 0.1959 - acc: 0.9214 - auc: 0.9762 - val_loss: 0.2658 - val_acc: 0.8897 - val_auc: 0.9578 - 15s/epoch - 56ms/step
Epoch 33/100
268/268 - 15s - loss: 0.1935 - acc: 0.9233 - auc: 0.9771 - val_loss: 0.2649 - val_acc: 0.8929 - val_auc: 0.9585 - 15s/epoch - 56ms/step
Early stopping epoch: 32
******Evaluating TEST set*********
30/30 - 1s - 803ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.91      0.84      0.87       382
           1       0.90      0.95      0.92       570

    accuracy                           0.90       952
   macro avg       0.90      0.89      0.90       952
weighted avg       0.90      0.90      0.90       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.48      0.43       382
           1       0.59      0.51      0.54       570

    accuracy                           0.49       952
   macro avg       0.49      0.49      0.49       952
weighted avg       0.51      0.49      0.50       952

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3206 - acc: 0.8620 - auc: 0.9355 - val_loss: 0.2875 - val_acc: 0.8813 - val_auc: 0.9483 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2833 - acc: 0.8864 - auc: 0.9488 - val_loss: 0.2563 - val_acc: 0.8992 - val_auc: 0.9585 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2734 - acc: 0.8913 - auc: 0.9525 - val_loss: 0.2494 - val_acc: 0.9044 - val_auc: 0.9603 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2683 - acc: 0.8920 - auc: 0.9542 - val_loss: 0.2640 - val_acc: 0.8960 - val_auc: 0.9575 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2682 - acc: 0.8930 - auc: 0.9545 - val_loss: 0.2558 - val_acc: 0.9055 - val_auc: 0.9600 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2664 - acc: 0.8911 - auc: 0.9549 - val_loss: 0.2477 - val_acc: 0.9076 - val_auc: 0.9616 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2594 - acc: 0.8970 - auc: 0.9575 - val_loss: 0.2546 - val_acc: 0.8960 - val_auc: 0.9606 - 15s/epoch - 55ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2585 - acc: 0.8948 - auc: 0.9580 - val_loss: 0.2410 - val_acc: 0.9076 - val_auc: 0.9644 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2560 - acc: 0.8964 - auc: 0.9586 - val_loss: 0.2517 - val_acc: 0.9002 - val_auc: 0.9612 - 15s/epoch - 56ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2562 - acc: 0.8970 - auc: 0.9585 - val_loss: 0.2432 - val_acc: 0.9055 - val_auc: 0.9640 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2523 - acc: 0.8991 - auc: 0.9601 - val_loss: 0.2422 - val_acc: 0.9034 - val_auc: 0.9640 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2508 - acc: 0.8978 - auc: 0.9605 - val_loss: 0.2419 - val_acc: 0.9055 - val_auc: 0.9646 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2500 - acc: 0.9002 - auc: 0.9607 - val_loss: 0.2419 - val_acc: 0.9034 - val_auc: 0.9651 - 15s/epoch - 57ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2479 - acc: 0.9015 - auc: 0.9616 - val_loss: 0.2506 - val_acc: 0.8971 - val_auc: 0.9610 - 15s/epoch - 57ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2426 - acc: 0.9037 - auc: 0.9629 - val_loss: 0.2503 - val_acc: 0.8971 - val_auc: 0.9618 - 15s/epoch - 57ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2416 - acc: 0.9070 - auc: 0.9630 - val_loss: 0.2401 - val_acc: 0.9023 - val_auc: 0.9652 - 15s/epoch - 57ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2383 - acc: 0.9021 - auc: 0.9643 - val_loss: 0.2414 - val_acc: 0.9034 - val_auc: 0.9647 - 15s/epoch - 57ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2366 - acc: 0.9027 - auc: 0.9653 - val_loss: 0.2466 - val_acc: 0.9044 - val_auc: 0.9637 - 15s/epoch - 57ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2331 - acc: 0.9063 - auc: 0.9662 - val_loss: 0.2424 - val_acc: 0.9013 - val_auc: 0.9638 - 15s/epoch - 57ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2330 - acc: 0.9067 - auc: 0.9665 - val_loss: 0.2349 - val_acc: 0.8992 - val_auc: 0.9653 - 15s/epoch - 57ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2295 - acc: 0.9086 - auc: 0.9673 - val_loss: 0.2437 - val_acc: 0.8971 - val_auc: 0.9641 - 15s/epoch - 57ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2269 - acc: 0.9077 - auc: 0.9682 - val_loss: 0.2532 - val_acc: 0.8981 - val_auc: 0.9590 - 15s/epoch - 57ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2224 - acc: 0.9091 - auc: 0.9693 - val_loss: 0.2452 - val_acc: 0.9044 - val_auc: 0.9629 - 15s/epoch - 57ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2219 - acc: 0.9102 - auc: 0.9695 - val_loss: 0.2387 - val_acc: 0.9023 - val_auc: 0.9650 - 15s/epoch - 57ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2167 - acc: 0.9116 - auc: 0.9706 - val_loss: 0.2390 - val_acc: 0.9023 - val_auc: 0.9653 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2163 - acc: 0.9134 - auc: 0.9710 - val_loss: 0.2379 - val_acc: 0.8939 - val_auc: 0.9636 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2094 - acc: 0.9146 - auc: 0.9731 - val_loss: 0.2423 - val_acc: 0.8981 - val_auc: 0.9622 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2092 - acc: 0.9153 - auc: 0.9730 - val_loss: 0.2561 - val_acc: 0.9023 - val_auc: 0.9598 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2003 - acc: 0.9202 - auc: 0.9754 - val_loss: 0.2547 - val_acc: 0.9013 - val_auc: 0.9593 - 15s/epoch - 55ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1957 - acc: 0.9208 - auc: 0.9764 - val_loss: 0.2418 - val_acc: 0.9013 - val_auc: 0.9643 - 15s/epoch - 57ms/step
Early stopping epoch: 29
******Evaluating TEST set*********
30/30 - 1s - 825ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.90      0.85      0.87       382
           1       0.90      0.93      0.92       570

    accuracy                           0.90       952
   macro avg       0.90      0.89      0.89       952
weighted avg       0.90      0.90      0.90       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.51      0.44       382
           1       0.59      0.47      0.53       570

    accuracy                           0.49       952
   macro avg       0.49      0.49      0.48       952
weighted avg       0.51      0.49      0.49       952

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8978] IC [0.8944, 0.9013]
Mean Recall[0.8891] IC [0.8850, 0.8932]
Mean F1[0.8926] IC [0.8889, 0.8963]
Median Accuracy[0.8982]
Median Recall[0.8901]
Median F1[0.8932]
********************txid224308********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.74      0.76      0.75       208
           1       0.92      0.91      0.92       644

    accuracy                           0.88       852
   macro avg       0.83      0.84      0.83       852
weighted avg       0.88      0.88      0.88       852

Predicted  0.0  1.0  All
True                    
0          158   50  208
1           55  589  644
All        213  639  852
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 14s - loss: 0.2967 - acc: 0.8712 - auc: 0.9455 - val_loss: 0.2553 - val_acc: 0.9015 - val_auc: 0.9547 - 14s/epoch - 63ms/step
Epoch 2/100
229/229 - 12s - loss: 0.2640 - acc: 0.8888 - auc: 0.9546 - val_loss: 0.2403 - val_acc: 0.9113 - val_auc: 0.9573 - 12s/epoch - 54ms/step
Epoch 3/100
229/229 - 12s - loss: 0.2526 - acc: 0.8954 - auc: 0.9585 - val_loss: 0.2308 - val_acc: 0.9163 - val_auc: 0.9634 - 12s/epoch - 52ms/step
Epoch 4/100
229/229 - 12s - loss: 0.2465 - acc: 0.8981 - auc: 0.9601 - val_loss: 0.2371 - val_acc: 0.9101 - val_auc: 0.9625 - 12s/epoch - 52ms/step
Epoch 5/100
229/229 - 12s - loss: 0.2441 - acc: 0.9002 - auc: 0.9612 - val_loss: 0.2284 - val_acc: 0.9089 - val_auc: 0.9655 - 12s/epoch - 52ms/step
Epoch 6/100
229/229 - 12s - loss: 0.2393 - acc: 0.9038 - auc: 0.9624 - val_loss: 0.2280 - val_acc: 0.9126 - val_auc: 0.9664 - 12s/epoch - 52ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2378 - acc: 0.9027 - auc: 0.9632 - val_loss: 0.2311 - val_acc: 0.9126 - val_auc: 0.9677 - 12s/epoch - 52ms/step
Epoch 8/100
229/229 - 12s - loss: 0.2364 - acc: 0.9027 - auc: 0.9643 - val_loss: 0.2320 - val_acc: 0.9126 - val_auc: 0.9626 - 12s/epoch - 52ms/step
Epoch 9/100
229/229 - 12s - loss: 0.2328 - acc: 0.9070 - auc: 0.9650 - val_loss: 0.2297 - val_acc: 0.9076 - val_auc: 0.9664 - 12s/epoch - 52ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2308 - acc: 0.9050 - auc: 0.9661 - val_loss: 0.2287 - val_acc: 0.9150 - val_auc: 0.9639 - 12s/epoch - 52ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2294 - acc: 0.9053 - auc: 0.9662 - val_loss: 0.2321 - val_acc: 0.9064 - val_auc: 0.9640 - 12s/epoch - 52ms/step
Epoch 12/100
229/229 - 12s - loss: 0.2265 - acc: 0.9072 - auc: 0.9680 - val_loss: 0.2324 - val_acc: 0.9052 - val_auc: 0.9649 - 12s/epoch - 52ms/step
Epoch 13/100
229/229 - 12s - loss: 0.2269 - acc: 0.9061 - auc: 0.9675 - val_loss: 0.2245 - val_acc: 0.9126 - val_auc: 0.9667 - 12s/epoch - 52ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2240 - acc: 0.9090 - auc: 0.9680 - val_loss: 0.2341 - val_acc: 0.9138 - val_auc: 0.9647 - 12s/epoch - 54ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2243 - acc: 0.9080 - auc: 0.9679 - val_loss: 0.2295 - val_acc: 0.9064 - val_auc: 0.9666 - 12s/epoch - 53ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2239 - acc: 0.9091 - auc: 0.9675 - val_loss: 0.2271 - val_acc: 0.9113 - val_auc: 0.9652 - 12s/epoch - 52ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2198 - acc: 0.9109 - auc: 0.9697 - val_loss: 0.2339 - val_acc: 0.9126 - val_auc: 0.9650 - 12s/epoch - 52ms/step
Early stopping epoch: 16
******Evaluating TEST set*********
26/26 - 1s - 649ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.91      0.83      0.87       285
           1       0.91      0.96      0.93       527

    accuracy                           0.91       812
   macro avg       0.91      0.89      0.90       812
weighted avg       0.91      0.91      0.91       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.51      0.43       285
           1       0.67      0.53      0.59       527

    accuracy                           0.52       812
   macro avg       0.52      0.52      0.51       812
weighted avg       0.56      0.52      0.54       812

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 14s - loss: 0.2988 - acc: 0.8702 - auc: 0.9435 - val_loss: 0.2567 - val_acc: 0.8879 - val_auc: 0.9589 - 14s/epoch - 62ms/step
Epoch 2/100
229/229 - 12s - loss: 0.2632 - acc: 0.8914 - auc: 0.9552 - val_loss: 0.2476 - val_acc: 0.8966 - val_auc: 0.9622 - 12s/epoch - 53ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2544 - acc: 0.8951 - auc: 0.9580 - val_loss: 0.2369 - val_acc: 0.9039 - val_auc: 0.9648 - 13s/epoch - 56ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2485 - acc: 0.8983 - auc: 0.9593 - val_loss: 0.2318 - val_acc: 0.9076 - val_auc: 0.9666 - 13s/epoch - 56ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2442 - acc: 0.8980 - auc: 0.9616 - val_loss: 0.2466 - val_acc: 0.8953 - val_auc: 0.9637 - 13s/epoch - 58ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2443 - acc: 0.9016 - auc: 0.9617 - val_loss: 0.2215 - val_acc: 0.9126 - val_auc: 0.9695 - 13s/epoch - 55ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2413 - acc: 0.9001 - auc: 0.9626 - val_loss: 0.2349 - val_acc: 0.9089 - val_auc: 0.9671 - 12s/epoch - 54ms/step
Epoch 8/100
229/229 - 13s - loss: 0.2393 - acc: 0.9021 - auc: 0.9628 - val_loss: 0.2238 - val_acc: 0.9187 - val_auc: 0.9711 - 13s/epoch - 58ms/step
Epoch 9/100
229/229 - 13s - loss: 0.2369 - acc: 0.9033 - auc: 0.9639 - val_loss: 0.2324 - val_acc: 0.8966 - val_auc: 0.9688 - 13s/epoch - 58ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2385 - acc: 0.9044 - auc: 0.9635 - val_loss: 0.2134 - val_acc: 0.9200 - val_auc: 0.9717 - 12s/epoch - 54ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2320 - acc: 0.9043 - auc: 0.9651 - val_loss: 0.2211 - val_acc: 0.9175 - val_auc: 0.9705 - 12s/epoch - 53ms/step
Epoch 12/100
229/229 - 12s - loss: 0.2288 - acc: 0.9077 - auc: 0.9661 - val_loss: 0.2097 - val_acc: 0.9273 - val_auc: 0.9716 - 12s/epoch - 53ms/step
Epoch 13/100
229/229 - 12s - loss: 0.2312 - acc: 0.9059 - auc: 0.9654 - val_loss: 0.2310 - val_acc: 0.9064 - val_auc: 0.9684 - 12s/epoch - 53ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2276 - acc: 0.9066 - auc: 0.9672 - val_loss: 0.2129 - val_acc: 0.9187 - val_auc: 0.9726 - 12s/epoch - 53ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2265 - acc: 0.9085 - auc: 0.9668 - val_loss: 0.2070 - val_acc: 0.9273 - val_auc: 0.9742 - 12s/epoch - 53ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2260 - acc: 0.9092 - auc: 0.9675 - val_loss: 0.2225 - val_acc: 0.9101 - val_auc: 0.9708 - 12s/epoch - 53ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2231 - acc: 0.9102 - auc: 0.9680 - val_loss: 0.2137 - val_acc: 0.9126 - val_auc: 0.9728 - 12s/epoch - 53ms/step
Epoch 18/100
229/229 - 12s - loss: 0.2225 - acc: 0.9102 - auc: 0.9683 - val_loss: 0.2208 - val_acc: 0.9138 - val_auc: 0.9716 - 12s/epoch - 53ms/step
Epoch 19/100
229/229 - 12s - loss: 0.2217 - acc: 0.9081 - auc: 0.9688 - val_loss: 0.2077 - val_acc: 0.9200 - val_auc: 0.9747 - 12s/epoch - 53ms/step
Epoch 20/100
229/229 - 12s - loss: 0.2185 - acc: 0.9117 - auc: 0.9696 - val_loss: 0.2167 - val_acc: 0.9150 - val_auc: 0.9712 - 12s/epoch - 53ms/step
Epoch 21/100
229/229 - 12s - loss: 0.2168 - acc: 0.9136 - auc: 0.9700 - val_loss: 0.2107 - val_acc: 0.9236 - val_auc: 0.9714 - 12s/epoch - 53ms/step
Epoch 22/100
229/229 - 12s - loss: 0.2177 - acc: 0.9116 - auc: 0.9697 - val_loss: 0.2294 - val_acc: 0.9101 - val_auc: 0.9692 - 12s/epoch - 53ms/step
Epoch 23/100
229/229 - 12s - loss: 0.2121 - acc: 0.9176 - auc: 0.9710 - val_loss: 0.2098 - val_acc: 0.9175 - val_auc: 0.9728 - 12s/epoch - 53ms/step
Epoch 24/100
229/229 - 12s - loss: 0.2070 - acc: 0.9176 - auc: 0.9733 - val_loss: 0.2197 - val_acc: 0.9126 - val_auc: 0.9719 - 12s/epoch - 53ms/step
Epoch 25/100
229/229 - 12s - loss: 0.2068 - acc: 0.9174 - auc: 0.9728 - val_loss: 0.2209 - val_acc: 0.9212 - val_auc: 0.9714 - 12s/epoch - 53ms/step
Epoch 26/100
229/229 - 12s - loss: 0.2078 - acc: 0.9177 - auc: 0.9727 - val_loss: 0.2136 - val_acc: 0.9150 - val_auc: 0.9730 - 12s/epoch - 53ms/step
Epoch 27/100
229/229 - 12s - loss: 0.2024 - acc: 0.9210 - auc: 0.9738 - val_loss: 0.2094 - val_acc: 0.9224 - val_auc: 0.9730 - 12s/epoch - 53ms/step
Epoch 28/100
229/229 - 13s - loss: 0.2028 - acc: 0.9188 - auc: 0.9738 - val_loss: 0.2114 - val_acc: 0.9200 - val_auc: 0.9734 - 13s/epoch - 55ms/step
Epoch 29/100
229/229 - 12s - loss: 0.1967 - acc: 0.9218 - auc: 0.9752 - val_loss: 0.2294 - val_acc: 0.9150 - val_auc: 0.9695 - 12s/epoch - 53ms/step
Early stopping epoch: 28
******Evaluating TEST set*********
26/26 - 1s - 633ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.93      0.84      0.88       285
           1       0.92      0.96      0.94       527

    accuracy                           0.92       812
   macro avg       0.92      0.90      0.91       812
weighted avg       0.92      0.92      0.92       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.36      0.53      0.43       285
           1       0.66      0.49      0.56       527

    accuracy                           0.50       812
   macro avg       0.51      0.51      0.49       812
weighted avg       0.55      0.50      0.51       812

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 14s - loss: 0.2980 - acc: 0.8729 - auc: 0.9440 - val_loss: 0.2804 - val_acc: 0.8793 - val_auc: 0.9506 - 14s/epoch - 63ms/step
Epoch 2/100
229/229 - 12s - loss: 0.2610 - acc: 0.8935 - auc: 0.9553 - val_loss: 0.2667 - val_acc: 0.8916 - val_auc: 0.9553 - 12s/epoch - 53ms/step
Epoch 3/100
229/229 - 12s - loss: 0.2494 - acc: 0.9021 - auc: 0.9583 - val_loss: 0.2648 - val_acc: 0.8867 - val_auc: 0.9559 - 12s/epoch - 54ms/step
Epoch 4/100
229/229 - 12s - loss: 0.2457 - acc: 0.8998 - auc: 0.9601 - val_loss: 0.2784 - val_acc: 0.8805 - val_auc: 0.9536 - 12s/epoch - 53ms/step
Epoch 5/100
229/229 - 12s - loss: 0.2440 - acc: 0.9021 - auc: 0.9615 - val_loss: 0.2697 - val_acc: 0.8793 - val_auc: 0.9559 - 12s/epoch - 53ms/step
Epoch 6/100
229/229 - 12s - loss: 0.2386 - acc: 0.9029 - auc: 0.9629 - val_loss: 0.2633 - val_acc: 0.8879 - val_auc: 0.9595 - 12s/epoch - 53ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2398 - acc: 0.9046 - auc: 0.9629 - val_loss: 0.2530 - val_acc: 0.8892 - val_auc: 0.9615 - 12s/epoch - 53ms/step
Epoch 8/100
229/229 - 12s - loss: 0.2360 - acc: 0.9065 - auc: 0.9635 - val_loss: 0.2509 - val_acc: 0.8966 - val_auc: 0.9609 - 12s/epoch - 53ms/step
Epoch 9/100
229/229 - 12s - loss: 0.2307 - acc: 0.9070 - auc: 0.9658 - val_loss: 0.2504 - val_acc: 0.8978 - val_auc: 0.9618 - 12s/epoch - 53ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2309 - acc: 0.9103 - auc: 0.9659 - val_loss: 0.2483 - val_acc: 0.8879 - val_auc: 0.9626 - 12s/epoch - 53ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2270 - acc: 0.9076 - auc: 0.9668 - val_loss: 0.2470 - val_acc: 0.8929 - val_auc: 0.9645 - 12s/epoch - 53ms/step
Epoch 12/100
229/229 - 12s - loss: 0.2249 - acc: 0.9092 - auc: 0.9663 - val_loss: 0.2436 - val_acc: 0.8978 - val_auc: 0.9650 - 12s/epoch - 53ms/step
Epoch 13/100
229/229 - 12s - loss: 0.2272 - acc: 0.9069 - auc: 0.9668 - val_loss: 0.2369 - val_acc: 0.8978 - val_auc: 0.9663 - 12s/epoch - 53ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2238 - acc: 0.9105 - auc: 0.9670 - val_loss: 0.2446 - val_acc: 0.8966 - val_auc: 0.9645 - 12s/epoch - 53ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2283 - acc: 0.9069 - auc: 0.9669 - val_loss: 0.2505 - val_acc: 0.8978 - val_auc: 0.9642 - 12s/epoch - 53ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2262 - acc: 0.9050 - auc: 0.9680 - val_loss: 0.2485 - val_acc: 0.8904 - val_auc: 0.9629 - 12s/epoch - 53ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2233 - acc: 0.9111 - auc: 0.9684 - val_loss: 0.2512 - val_acc: 0.8904 - val_auc: 0.9630 - 12s/epoch - 54ms/step
Epoch 18/100
229/229 - 12s - loss: 0.2193 - acc: 0.9105 - auc: 0.9695 - val_loss: 0.2442 - val_acc: 0.8966 - val_auc: 0.9646 - 12s/epoch - 54ms/step
Epoch 19/100
229/229 - 12s - loss: 0.2187 - acc: 0.9139 - auc: 0.9695 - val_loss: 0.2469 - val_acc: 0.8916 - val_auc: 0.9650 - 12s/epoch - 53ms/step
Epoch 20/100
229/229 - 12s - loss: 0.2153 - acc: 0.9162 - auc: 0.9695 - val_loss: 0.2434 - val_acc: 0.8953 - val_auc: 0.9653 - 12s/epoch - 53ms/step
Epoch 21/100
229/229 - 12s - loss: 0.2147 - acc: 0.9143 - auc: 0.9709 - val_loss: 0.2431 - val_acc: 0.8966 - val_auc: 0.9660 - 12s/epoch - 53ms/step
Epoch 22/100
229/229 - 12s - loss: 0.2103 - acc: 0.9179 - auc: 0.9716 - val_loss: 0.2452 - val_acc: 0.9039 - val_auc: 0.9660 - 12s/epoch - 53ms/step
Epoch 23/100
229/229 - 12s - loss: 0.2094 - acc: 0.9185 - auc: 0.9716 - val_loss: 0.2550 - val_acc: 0.8990 - val_auc: 0.9644 - 12s/epoch - 53ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
26/26 - 1s - 639ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.86      0.85      0.85       285
           1       0.92      0.93      0.92       527

    accuracy                           0.90       812
   macro avg       0.89      0.89      0.89       812
weighted avg       0.90      0.90      0.90       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.53      0.42       285
           1       0.65      0.47      0.55       527

    accuracy                           0.49       812
   macro avg       0.50      0.50      0.48       812
weighted avg       0.54      0.49      0.50       812

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 14s - loss: 0.2996 - acc: 0.8739 - auc: 0.9435 - val_loss: 0.2662 - val_acc: 0.8855 - val_auc: 0.9540 - 14s/epoch - 63ms/step
Epoch 2/100
229/229 - 12s - loss: 0.2571 - acc: 0.8965 - auc: 0.9560 - val_loss: 0.2576 - val_acc: 0.8842 - val_auc: 0.9561 - 12s/epoch - 53ms/step
Epoch 3/100
229/229 - 12s - loss: 0.2523 - acc: 0.8981 - auc: 0.9587 - val_loss: 0.2838 - val_acc: 0.8732 - val_auc: 0.9466 - 12s/epoch - 53ms/step
Epoch 4/100
229/229 - 12s - loss: 0.2430 - acc: 0.9039 - auc: 0.9615 - val_loss: 0.2715 - val_acc: 0.8842 - val_auc: 0.9543 - 12s/epoch - 54ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2403 - acc: 0.9033 - auc: 0.9627 - val_loss: 0.2604 - val_acc: 0.8855 - val_auc: 0.9556 - 13s/epoch - 58ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2382 - acc: 0.9021 - auc: 0.9631 - val_loss: 0.2541 - val_acc: 0.8892 - val_auc: 0.9551 - 13s/epoch - 55ms/step
Epoch 7/100
229/229 - 13s - loss: 0.2332 - acc: 0.9043 - auc: 0.9651 - val_loss: 0.2528 - val_acc: 0.8867 - val_auc: 0.9581 - 13s/epoch - 56ms/step
Epoch 8/100
229/229 - 13s - loss: 0.2337 - acc: 0.9016 - auc: 0.9651 - val_loss: 0.2647 - val_acc: 0.8916 - val_auc: 0.9572 - 13s/epoch - 55ms/step
Epoch 9/100
229/229 - 13s - loss: 0.2316 - acc: 0.9117 - auc: 0.9655 - val_loss: 0.2602 - val_acc: 0.8867 - val_auc: 0.9581 - 13s/epoch - 55ms/step
Epoch 10/100
229/229 - 13s - loss: 0.2313 - acc: 0.9040 - auc: 0.9664 - val_loss: 0.2736 - val_acc: 0.8842 - val_auc: 0.9552 - 13s/epoch - 55ms/step
Epoch 11/100
229/229 - 13s - loss: 0.2308 - acc: 0.9087 - auc: 0.9662 - val_loss: 0.2608 - val_acc: 0.8855 - val_auc: 0.9570 - 13s/epoch - 55ms/step
Epoch 12/100
229/229 - 13s - loss: 0.2277 - acc: 0.9080 - auc: 0.9664 - val_loss: 0.2508 - val_acc: 0.8916 - val_auc: 0.9603 - 13s/epoch - 55ms/step
Epoch 13/100
229/229 - 13s - loss: 0.2277 - acc: 0.9072 - auc: 0.9670 - val_loss: 0.2738 - val_acc: 0.8929 - val_auc: 0.9555 - 13s/epoch - 55ms/step
Epoch 14/100
229/229 - 13s - loss: 0.2243 - acc: 0.9085 - auc: 0.9683 - val_loss: 0.2555 - val_acc: 0.8830 - val_auc: 0.9595 - 13s/epoch - 55ms/step
Epoch 15/100
229/229 - 13s - loss: 0.2265 - acc: 0.9073 - auc: 0.9672 - val_loss: 0.2641 - val_acc: 0.8941 - val_auc: 0.9589 - 13s/epoch - 55ms/step
Epoch 16/100
229/229 - 13s - loss: 0.2241 - acc: 0.9099 - auc: 0.9680 - val_loss: 0.2621 - val_acc: 0.8879 - val_auc: 0.9595 - 13s/epoch - 55ms/step
Epoch 17/100
229/229 - 13s - loss: 0.2188 - acc: 0.9117 - auc: 0.9693 - val_loss: 0.2534 - val_acc: 0.8842 - val_auc: 0.9607 - 13s/epoch - 55ms/step
Epoch 18/100
229/229 - 13s - loss: 0.2203 - acc: 0.9113 - auc: 0.9694 - val_loss: 0.2511 - val_acc: 0.8904 - val_auc: 0.9578 - 13s/epoch - 55ms/step
Epoch 19/100
229/229 - 13s - loss: 0.2148 - acc: 0.9150 - auc: 0.9705 - val_loss: 0.2635 - val_acc: 0.8842 - val_auc: 0.9574 - 13s/epoch - 55ms/step
Epoch 20/100
229/229 - 13s - loss: 0.2133 - acc: 0.9169 - auc: 0.9711 - val_loss: 0.2745 - val_acc: 0.8892 - val_auc: 0.9543 - 13s/epoch - 56ms/step
Epoch 21/100
229/229 - 13s - loss: 0.2134 - acc: 0.9140 - auc: 0.9711 - val_loss: 0.2463 - val_acc: 0.8916 - val_auc: 0.9626 - 13s/epoch - 56ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2102 - acc: 0.9179 - auc: 0.9723 - val_loss: 0.2605 - val_acc: 0.8867 - val_auc: 0.9567 - 13s/epoch - 55ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2089 - acc: 0.9177 - auc: 0.9724 - val_loss: 0.2661 - val_acc: 0.8830 - val_auc: 0.9551 - 13s/epoch - 55ms/step
Epoch 24/100
229/229 - 13s - loss: 0.2040 - acc: 0.9191 - auc: 0.9737 - val_loss: 0.2732 - val_acc: 0.8855 - val_auc: 0.9543 - 13s/epoch - 56ms/step
Epoch 25/100
229/229 - 13s - loss: 0.2002 - acc: 0.9203 - auc: 0.9748 - val_loss: 0.2683 - val_acc: 0.8990 - val_auc: 0.9565 - 13s/epoch - 55ms/step
Epoch 26/100
229/229 - 13s - loss: 0.1959 - acc: 0.9240 - auc: 0.9758 - val_loss: 0.2900 - val_acc: 0.8867 - val_auc: 0.9564 - 13s/epoch - 56ms/step
Epoch 27/100
229/229 - 13s - loss: 0.1972 - acc: 0.9237 - auc: 0.9757 - val_loss: 0.2714 - val_acc: 0.8879 - val_auc: 0.9553 - 13s/epoch - 55ms/step
Epoch 28/100
229/229 - 13s - loss: 0.1900 - acc: 0.9266 - auc: 0.9771 - val_loss: 0.2726 - val_acc: 0.8830 - val_auc: 0.9540 - 13s/epoch - 55ms/step
Epoch 29/100
229/229 - 13s - loss: 0.1910 - acc: 0.9269 - auc: 0.9767 - val_loss: 0.2746 - val_acc: 0.8941 - val_auc: 0.9554 - 13s/epoch - 56ms/step
Epoch 30/100
229/229 - 13s - loss: 0.1827 - acc: 0.9296 - auc: 0.9786 - val_loss: 0.2638 - val_acc: 0.8978 - val_auc: 0.9568 - 13s/epoch - 55ms/step
Epoch 31/100
229/229 - 13s - loss: 0.1811 - acc: 0.9299 - auc: 0.9793 - val_loss: 0.2788 - val_acc: 0.8966 - val_auc: 0.9562 - 13s/epoch - 56ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
26/26 - 1s - 684ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.85      0.84      0.84       285
           1       0.91      0.92      0.92       527

    accuracy                           0.89       812
   macro avg       0.88      0.88      0.88       812
weighted avg       0.89      0.89      0.89       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.48      0.41       285
           1       0.65      0.52      0.58       527

    accuracy                           0.50       812
   macro avg       0.50      0.50      0.49       812
weighted avg       0.54      0.50      0.52       812

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 15s - loss: 0.3027 - acc: 0.8712 - auc: 0.9420 - val_loss: 0.2486 - val_acc: 0.9027 - val_auc: 0.9581 - 15s/epoch - 66ms/step
Epoch 2/100
229/229 - 13s - loss: 0.2612 - acc: 0.8924 - auc: 0.9555 - val_loss: 0.2463 - val_acc: 0.9027 - val_auc: 0.9626 - 13s/epoch - 55ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2534 - acc: 0.8961 - auc: 0.9583 - val_loss: 0.2369 - val_acc: 0.9039 - val_auc: 0.9639 - 13s/epoch - 56ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2468 - acc: 0.8985 - auc: 0.9611 - val_loss: 0.2360 - val_acc: 0.8978 - val_auc: 0.9637 - 13s/epoch - 56ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2474 - acc: 0.9013 - auc: 0.9604 - val_loss: 0.2317 - val_acc: 0.9138 - val_auc: 0.9666 - 13s/epoch - 56ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2410 - acc: 0.9064 - auc: 0.9628 - val_loss: 0.2403 - val_acc: 0.9126 - val_auc: 0.9668 - 13s/epoch - 56ms/step
Epoch 7/100
229/229 - 13s - loss: 0.2376 - acc: 0.9043 - auc: 0.9637 - val_loss: 0.2278 - val_acc: 0.9138 - val_auc: 0.9682 - 13s/epoch - 55ms/step
Epoch 8/100
229/229 - 13s - loss: 0.2385 - acc: 0.9046 - auc: 0.9629 - val_loss: 0.2241 - val_acc: 0.9101 - val_auc: 0.9697 - 13s/epoch - 56ms/step
Epoch 9/100
229/229 - 13s - loss: 0.2322 - acc: 0.9047 - auc: 0.9653 - val_loss: 0.2355 - val_acc: 0.8953 - val_auc: 0.9641 - 13s/epoch - 56ms/step
Epoch 10/100
229/229 - 13s - loss: 0.2316 - acc: 0.9076 - auc: 0.9655 - val_loss: 0.2231 - val_acc: 0.9101 - val_auc: 0.9686 - 13s/epoch - 56ms/step
Epoch 11/100
229/229 - 13s - loss: 0.2295 - acc: 0.9055 - auc: 0.9659 - val_loss: 0.2242 - val_acc: 0.9187 - val_auc: 0.9685 - 13s/epoch - 56ms/step
Epoch 12/100
229/229 - 13s - loss: 0.2275 - acc: 0.9064 - auc: 0.9665 - val_loss: 0.2228 - val_acc: 0.9027 - val_auc: 0.9690 - 13s/epoch - 56ms/step
Epoch 13/100
229/229 - 13s - loss: 0.2257 - acc: 0.9103 - auc: 0.9676 - val_loss: 0.2242 - val_acc: 0.9101 - val_auc: 0.9711 - 13s/epoch - 56ms/step
Epoch 14/100
229/229 - 13s - loss: 0.2240 - acc: 0.9127 - auc: 0.9673 - val_loss: 0.2254 - val_acc: 0.9126 - val_auc: 0.9694 - 13s/epoch - 56ms/step
Epoch 15/100
229/229 - 13s - loss: 0.2245 - acc: 0.9088 - auc: 0.9683 - val_loss: 0.2257 - val_acc: 0.9002 - val_auc: 0.9682 - 13s/epoch - 56ms/step
Epoch 16/100
229/229 - 13s - loss: 0.2210 - acc: 0.9111 - auc: 0.9693 - val_loss: 0.2243 - val_acc: 0.9101 - val_auc: 0.9672 - 13s/epoch - 56ms/step
Epoch 17/100
229/229 - 13s - loss: 0.2222 - acc: 0.9095 - auc: 0.9686 - val_loss: 0.2221 - val_acc: 0.9064 - val_auc: 0.9696 - 13s/epoch - 56ms/step
Epoch 18/100
229/229 - 13s - loss: 0.2155 - acc: 0.9144 - auc: 0.9708 - val_loss: 0.2245 - val_acc: 0.9076 - val_auc: 0.9698 - 13s/epoch - 56ms/step
Epoch 19/100
229/229 - 13s - loss: 0.2151 - acc: 0.9146 - auc: 0.9704 - val_loss: 0.2328 - val_acc: 0.9002 - val_auc: 0.9663 - 13s/epoch - 56ms/step
Epoch 20/100
229/229 - 13s - loss: 0.2127 - acc: 0.9144 - auc: 0.9715 - val_loss: 0.2332 - val_acc: 0.9113 - val_auc: 0.9685 - 13s/epoch - 56ms/step
Epoch 21/100
229/229 - 13s - loss: 0.2113 - acc: 0.9129 - auc: 0.9717 - val_loss: 0.2225 - val_acc: 0.9076 - val_auc: 0.9699 - 13s/epoch - 56ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2094 - acc: 0.9157 - auc: 0.9726 - val_loss: 0.2298 - val_acc: 0.9027 - val_auc: 0.9676 - 13s/epoch - 56ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2071 - acc: 0.9129 - auc: 0.9731 - val_loss: 0.2259 - val_acc: 0.9076 - val_auc: 0.9688 - 13s/epoch - 56ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
26/26 - 1s - 685ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.86      0.88      0.87       285
           1       0.94      0.92      0.93       527

    accuracy                           0.91       812
   macro avg       0.90      0.90      0.90       812
weighted avg       0.91      0.91      0.91       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.49      0.42       285
           1       0.66      0.54      0.59       527

    accuracy                           0.52       812
   macro avg       0.52      0.52      0.51       812
weighted avg       0.56      0.52      0.53       812

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 15s - loss: 0.3009 - acc: 0.8702 - auc: 0.9435 - val_loss: 0.2610 - val_acc: 0.8916 - val_auc: 0.9560 - 15s/epoch - 66ms/step
Epoch 2/100
229/229 - 13s - loss: 0.2638 - acc: 0.8916 - auc: 0.9553 - val_loss: 0.2431 - val_acc: 0.8990 - val_auc: 0.9596 - 13s/epoch - 56ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2559 - acc: 0.8939 - auc: 0.9578 - val_loss: 0.2497 - val_acc: 0.9015 - val_auc: 0.9587 - 13s/epoch - 57ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2470 - acc: 0.9016 - auc: 0.9601 - val_loss: 0.2447 - val_acc: 0.9027 - val_auc: 0.9560 - 13s/epoch - 57ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2444 - acc: 0.9007 - auc: 0.9613 - val_loss: 0.2341 - val_acc: 0.9113 - val_auc: 0.9629 - 13s/epoch - 56ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2443 - acc: 0.9018 - auc: 0.9617 - val_loss: 0.2480 - val_acc: 0.8978 - val_auc: 0.9632 - 13s/epoch - 56ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2375 - acc: 0.9022 - auc: 0.9641 - val_loss: 0.2300 - val_acc: 0.9113 - val_auc: 0.9623 - 12s/epoch - 54ms/step
Epoch 8/100
229/229 - 13s - loss: 0.2346 - acc: 0.9054 - auc: 0.9649 - val_loss: 0.2257 - val_acc: 0.9113 - val_auc: 0.9641 - 13s/epoch - 57ms/step
Epoch 9/100
229/229 - 13s - loss: 0.2347 - acc: 0.9098 - auc: 0.9652 - val_loss: 0.2288 - val_acc: 0.9200 - val_auc: 0.9648 - 13s/epoch - 57ms/step
Epoch 10/100
229/229 - 13s - loss: 0.2326 - acc: 0.9054 - auc: 0.9651 - val_loss: 0.2275 - val_acc: 0.9089 - val_auc: 0.9649 - 13s/epoch - 57ms/step
Epoch 11/100
229/229 - 13s - loss: 0.2341 - acc: 0.9051 - auc: 0.9649 - val_loss: 0.2290 - val_acc: 0.9089 - val_auc: 0.9632 - 13s/epoch - 56ms/step
Epoch 12/100
229/229 - 13s - loss: 0.2277 - acc: 0.9099 - auc: 0.9675 - val_loss: 0.2356 - val_acc: 0.9015 - val_auc: 0.9639 - 13s/epoch - 57ms/step
Epoch 13/100
229/229 - 13s - loss: 0.2303 - acc: 0.9068 - auc: 0.9665 - val_loss: 0.2287 - val_acc: 0.9138 - val_auc: 0.9661 - 13s/epoch - 57ms/step
Epoch 14/100
229/229 - 13s - loss: 0.2242 - acc: 0.9088 - auc: 0.9683 - val_loss: 0.2410 - val_acc: 0.9015 - val_auc: 0.9644 - 13s/epoch - 57ms/step
Epoch 15/100
229/229 - 13s - loss: 0.2253 - acc: 0.9081 - auc: 0.9684 - val_loss: 0.2308 - val_acc: 0.9126 - val_auc: 0.9656 - 13s/epoch - 56ms/step
Epoch 16/100
229/229 - 13s - loss: 0.2233 - acc: 0.9084 - auc: 0.9686 - val_loss: 0.2347 - val_acc: 0.9002 - val_auc: 0.9659 - 13s/epoch - 56ms/step
Epoch 17/100
229/229 - 13s - loss: 0.2233 - acc: 0.9102 - auc: 0.9685 - val_loss: 0.2373 - val_acc: 0.9002 - val_auc: 0.9650 - 13s/epoch - 56ms/step
Epoch 18/100
229/229 - 13s - loss: 0.2193 - acc: 0.9114 - auc: 0.9694 - val_loss: 0.2266 - val_acc: 0.9101 - val_auc: 0.9668 - 13s/epoch - 56ms/step
Epoch 19/100
229/229 - 13s - loss: 0.2177 - acc: 0.9120 - auc: 0.9703 - val_loss: 0.2260 - val_acc: 0.9126 - val_auc: 0.9652 - 13s/epoch - 56ms/step
Epoch 20/100
229/229 - 13s - loss: 0.2151 - acc: 0.9107 - auc: 0.9708 - val_loss: 0.2278 - val_acc: 0.9089 - val_auc: 0.9647 - 13s/epoch - 56ms/step
Epoch 21/100
229/229 - 13s - loss: 0.2122 - acc: 0.9148 - auc: 0.9711 - val_loss: 0.2331 - val_acc: 0.9113 - val_auc: 0.9618 - 13s/epoch - 57ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2130 - acc: 0.9151 - auc: 0.9714 - val_loss: 0.2775 - val_acc: 0.8904 - val_auc: 0.9529 - 13s/epoch - 56ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2080 - acc: 0.9183 - auc: 0.9732 - val_loss: 0.2379 - val_acc: 0.9126 - val_auc: 0.9619 - 13s/epoch - 56ms/step
Epoch 24/100
229/229 - 13s - loss: 0.2079 - acc: 0.9153 - auc: 0.9729 - val_loss: 0.2313 - val_acc: 0.9150 - val_auc: 0.9644 - 13s/epoch - 57ms/step
Epoch 25/100
229/229 - 13s - loss: 0.2048 - acc: 0.9180 - auc: 0.9738 - val_loss: 0.2333 - val_acc: 0.9126 - val_auc: 0.9620 - 13s/epoch - 56ms/step
Epoch 26/100
229/229 - 13s - loss: 0.2028 - acc: 0.9203 - auc: 0.9745 - val_loss: 0.2307 - val_acc: 0.9076 - val_auc: 0.9663 - 13s/epoch - 56ms/step
Epoch 27/100
229/229 - 13s - loss: 0.1976 - acc: 0.9221 - auc: 0.9753 - val_loss: 0.2345 - val_acc: 0.9076 - val_auc: 0.9656 - 13s/epoch - 56ms/step
Epoch 28/100
229/229 - 13s - loss: 0.1918 - acc: 0.9250 - auc: 0.9760 - val_loss: 0.2369 - val_acc: 0.9138 - val_auc: 0.9618 - 13s/epoch - 57ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
26/26 - 1s - 684ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.87      0.88      0.87       285
           1       0.93      0.93      0.93       527

    accuracy                           0.91       812
   macro avg       0.90      0.90      0.90       812
weighted avg       0.91      0.91      0.91       812

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.34      0.49      0.40       285
           1       0.64      0.48      0.55       527

    accuracy                           0.48       812
   macro avg       0.49      0.49      0.47       812
weighted avg       0.53      0.48      0.50       812

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 15s - loss: 0.2978 - acc: 0.8652 - auc: 0.9440 - val_loss: 0.2697 - val_acc: 0.8940 - val_auc: 0.9498 - 15s/epoch - 68ms/step
Epoch 2/100
229/229 - 13s - loss: 0.2582 - acc: 0.8934 - auc: 0.9570 - val_loss: 0.2614 - val_acc: 0.9038 - val_auc: 0.9538 - 13s/epoch - 56ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2517 - acc: 0.8986 - auc: 0.9586 - val_loss: 0.2655 - val_acc: 0.9026 - val_auc: 0.9523 - 13s/epoch - 56ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2460 - acc: 0.8973 - auc: 0.9607 - val_loss: 0.2616 - val_acc: 0.9026 - val_auc: 0.9536 - 13s/epoch - 56ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2420 - acc: 0.9029 - auc: 0.9611 - val_loss: 0.2649 - val_acc: 0.8940 - val_auc: 0.9508 - 13s/epoch - 56ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2397 - acc: 0.9016 - auc: 0.9635 - val_loss: 0.2875 - val_acc: 0.8890 - val_auc: 0.9451 - 13s/epoch - 56ms/step
Epoch 7/100
229/229 - 13s - loss: 0.2344 - acc: 0.9017 - auc: 0.9652 - val_loss: 0.2508 - val_acc: 0.9063 - val_auc: 0.9561 - 13s/epoch - 56ms/step
Epoch 8/100
229/229 - 13s - loss: 0.2349 - acc: 0.9031 - auc: 0.9652 - val_loss: 0.2749 - val_acc: 0.8964 - val_auc: 0.9519 - 13s/epoch - 56ms/step
Epoch 9/100
229/229 - 13s - loss: 0.2313 - acc: 0.9076 - auc: 0.9660 - val_loss: 0.2585 - val_acc: 0.9026 - val_auc: 0.9563 - 13s/epoch - 57ms/step
Epoch 10/100
229/229 - 13s - loss: 0.2303 - acc: 0.9050 - auc: 0.9664 - val_loss: 0.2767 - val_acc: 0.9014 - val_auc: 0.9491 - 13s/epoch - 56ms/step
Epoch 11/100
229/229 - 13s - loss: 0.2272 - acc: 0.9050 - auc: 0.9675 - val_loss: 0.2569 - val_acc: 0.9088 - val_auc: 0.9543 - 13s/epoch - 56ms/step
Epoch 12/100
229/229 - 13s - loss: 0.2293 - acc: 0.9057 - auc: 0.9665 - val_loss: 0.2487 - val_acc: 0.9075 - val_auc: 0.9561 - 13s/epoch - 56ms/step
Epoch 13/100
229/229 - 13s - loss: 0.2252 - acc: 0.9047 - auc: 0.9690 - val_loss: 0.2613 - val_acc: 0.9014 - val_auc: 0.9550 - 13s/epoch - 56ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2236 - acc: 0.9102 - auc: 0.9688 - val_loss: 0.2608 - val_acc: 0.9026 - val_auc: 0.9534 - 12s/epoch - 54ms/step
Epoch 15/100
229/229 - 13s - loss: 0.2209 - acc: 0.9076 - auc: 0.9696 - val_loss: 0.2601 - val_acc: 0.9038 - val_auc: 0.9553 - 13s/epoch - 56ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2191 - acc: 0.9091 - auc: 0.9703 - val_loss: 0.2496 - val_acc: 0.9063 - val_auc: 0.9568 - 12s/epoch - 52ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2155 - acc: 0.9123 - auc: 0.9706 - val_loss: 0.2690 - val_acc: 0.8977 - val_auc: 0.9550 - 12s/epoch - 52ms/step
Epoch 18/100
229/229 - 13s - loss: 0.2156 - acc: 0.9114 - auc: 0.9706 - val_loss: 0.2652 - val_acc: 0.9001 - val_auc: 0.9532 - 13s/epoch - 55ms/step
Epoch 19/100
229/229 - 13s - loss: 0.2089 - acc: 0.9157 - auc: 0.9726 - val_loss: 0.2680 - val_acc: 0.9001 - val_auc: 0.9521 - 13s/epoch - 56ms/step
Epoch 20/100
229/229 - 13s - loss: 0.2121 - acc: 0.9151 - auc: 0.9711 - val_loss: 0.2562 - val_acc: 0.9100 - val_auc: 0.9542 - 13s/epoch - 56ms/step
Epoch 21/100
229/229 - 13s - loss: 0.2064 - acc: 0.9181 - auc: 0.9727 - val_loss: 0.3007 - val_acc: 0.8915 - val_auc: 0.9454 - 13s/epoch - 56ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2055 - acc: 0.9180 - auc: 0.9740 - val_loss: 0.2994 - val_acc: 0.8853 - val_auc: 0.9489 - 13s/epoch - 56ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2033 - acc: 0.9194 - auc: 0.9737 - val_loss: 0.2714 - val_acc: 0.9001 - val_auc: 0.9516 - 13s/epoch - 56ms/step
Epoch 24/100
229/229 - 13s - loss: 0.2024 - acc: 0.9207 - auc: 0.9743 - val_loss: 0.2559 - val_acc: 0.9051 - val_auc: 0.9583 - 13s/epoch - 57ms/step
Epoch 25/100
229/229 - 12s - loss: 0.1947 - acc: 0.9242 - auc: 0.9757 - val_loss: 0.2763 - val_acc: 0.8940 - val_auc: 0.9543 - 12s/epoch - 54ms/step
Epoch 26/100
229/229 - 13s - loss: 0.1937 - acc: 0.9224 - auc: 0.9760 - val_loss: 0.2838 - val_acc: 0.8989 - val_auc: 0.9534 - 13s/epoch - 56ms/step
Epoch 27/100
229/229 - 13s - loss: 0.1918 - acc: 0.9225 - auc: 0.9764 - val_loss: 0.2713 - val_acc: 0.8989 - val_auc: 0.9526 - 13s/epoch - 55ms/step
Epoch 28/100
229/229 - 13s - loss: 0.1870 - acc: 0.9258 - auc: 0.9782 - val_loss: 0.2777 - val_acc: 0.8878 - val_auc: 0.9521 - 13s/epoch - 55ms/step
Epoch 29/100
229/229 - 13s - loss: 0.1868 - acc: 0.9253 - auc: 0.9778 - val_loss: 0.2781 - val_acc: 0.9063 - val_auc: 0.9533 - 13s/epoch - 55ms/step
Epoch 30/100
229/229 - 13s - loss: 0.1825 - acc: 0.9273 - auc: 0.9784 - val_loss: 0.2892 - val_acc: 0.9026 - val_auc: 0.9508 - 13s/epoch - 55ms/step
Epoch 31/100
229/229 - 13s - loss: 0.1768 - acc: 0.9279 - auc: 0.9805 - val_loss: 0.2917 - val_acc: 0.8927 - val_auc: 0.9469 - 13s/epoch - 56ms/step
Epoch 32/100
229/229 - 13s - loss: 0.1703 - acc: 0.9346 - auc: 0.9819 - val_loss: 0.2901 - val_acc: 0.8989 - val_auc: 0.9534 - 13s/epoch - 56ms/step
Epoch 33/100
229/229 - 13s - loss: 0.1663 - acc: 0.9361 - auc: 0.9826 - val_loss: 0.2941 - val_acc: 0.8940 - val_auc: 0.9512 - 13s/epoch - 56ms/step
Epoch 34/100
229/229 - 13s - loss: 0.1659 - acc: 0.9358 - auc: 0.9823 - val_loss: 0.2826 - val_acc: 0.8964 - val_auc: 0.9586 - 13s/epoch - 56ms/step
Epoch 35/100
229/229 - 13s - loss: 0.1547 - acc: 0.9413 - auc: 0.9843 - val_loss: 0.2973 - val_acc: 0.8927 - val_auc: 0.9534 - 13s/epoch - 56ms/step
Epoch 36/100
229/229 - 13s - loss: 0.1548 - acc: 0.9389 - auc: 0.9846 - val_loss: 0.3106 - val_acc: 0.8816 - val_auc: 0.9490 - 13s/epoch - 56ms/step
Epoch 37/100
229/229 - 13s - loss: 0.1518 - acc: 0.9428 - auc: 0.9853 - val_loss: 0.3344 - val_acc: 0.8742 - val_auc: 0.9421 - 13s/epoch - 56ms/step
Epoch 38/100
229/229 - 13s - loss: 0.1424 - acc: 0.9433 - auc: 0.9868 - val_loss: 0.3161 - val_acc: 0.9014 - val_auc: 0.9453 - 13s/epoch - 56ms/step
Epoch 39/100
229/229 - 13s - loss: 0.1377 - acc: 0.9465 - auc: 0.9871 - val_loss: 0.3076 - val_acc: 0.8915 - val_auc: 0.9437 - 13s/epoch - 56ms/step
Epoch 40/100
229/229 - 13s - loss: 0.1373 - acc: 0.9504 - auc: 0.9878 - val_loss: 0.3593 - val_acc: 0.8878 - val_auc: 0.9368 - 13s/epoch - 56ms/step
Epoch 41/100
229/229 - 12s - loss: 0.1265 - acc: 0.9526 - auc: 0.9892 - val_loss: 0.3221 - val_acc: 0.8866 - val_auc: 0.9425 - 12s/epoch - 52ms/step
Epoch 42/100
229/229 - 12s - loss: 0.1240 - acc: 0.9550 - auc: 0.9900 - val_loss: 0.3227 - val_acc: 0.8915 - val_auc: 0.9449 - 12s/epoch - 52ms/step
Epoch 43/100
229/229 - 12s - loss: 0.1199 - acc: 0.9548 - auc: 0.9905 - val_loss: 0.3597 - val_acc: 0.8903 - val_auc: 0.9359 - 12s/epoch - 54ms/step
Epoch 44/100
229/229 - 12s - loss: 0.1157 - acc: 0.9563 - auc: 0.9912 - val_loss: 0.3696 - val_acc: 0.8903 - val_auc: 0.9276 - 12s/epoch - 53ms/step
Early stopping epoch: 43
******Evaluating TEST set*********
26/26 - 1s - 692ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.89      0.81      0.85       284
           1       0.90      0.94      0.92       527

    accuracy                           0.90       811
   macro avg       0.89      0.88      0.88       811
weighted avg       0.90      0.90      0.90       811

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.36      0.52      0.43       284
           1       0.66      0.50      0.57       527

    accuracy                           0.51       811
   macro avg       0.51      0.51      0.50       811
weighted avg       0.56      0.51      0.52       811

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 16s - loss: 0.2991 - acc: 0.8704 - auc: 0.9434 - val_loss: 0.2641 - val_acc: 0.8866 - val_auc: 0.9567 - 16s/epoch - 68ms/step
Epoch 2/100
229/229 - 13s - loss: 0.2638 - acc: 0.8923 - auc: 0.9542 - val_loss: 0.2846 - val_acc: 0.8668 - val_auc: 0.9498 - 13s/epoch - 55ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2529 - acc: 0.8975 - auc: 0.9580 - val_loss: 0.2494 - val_acc: 0.9001 - val_auc: 0.9613 - 13s/epoch - 55ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2482 - acc: 0.9018 - auc: 0.9598 - val_loss: 0.2315 - val_acc: 0.8964 - val_auc: 0.9686 - 13s/epoch - 56ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2489 - acc: 0.9008 - auc: 0.9594 - val_loss: 0.2439 - val_acc: 0.8890 - val_auc: 0.9643 - 13s/epoch - 58ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2439 - acc: 0.9020 - auc: 0.9611 - val_loss: 0.2291 - val_acc: 0.8952 - val_auc: 0.9681 - 13s/epoch - 58ms/step
Epoch 7/100
229/229 - 13s - loss: 0.2394 - acc: 0.9025 - auc: 0.9623 - val_loss: 0.2374 - val_acc: 0.8927 - val_auc: 0.9670 - 13s/epoch - 59ms/step
Epoch 8/100
229/229 - 12s - loss: 0.2378 - acc: 0.9054 - auc: 0.9628 - val_loss: 0.2253 - val_acc: 0.9001 - val_auc: 0.9686 - 12s/epoch - 55ms/step
Epoch 9/100
229/229 - 12s - loss: 0.2343 - acc: 0.9062 - auc: 0.9637 - val_loss: 0.2179 - val_acc: 0.9038 - val_auc: 0.9713 - 12s/epoch - 54ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2337 - acc: 0.9035 - auc: 0.9640 - val_loss: 0.2200 - val_acc: 0.9001 - val_auc: 0.9702 - 12s/epoch - 54ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2323 - acc: 0.9060 - auc: 0.9651 - val_loss: 0.2204 - val_acc: 0.8989 - val_auc: 0.9712 - 12s/epoch - 54ms/step
Epoch 12/100
229/229 - 12s - loss: 0.2311 - acc: 0.9092 - auc: 0.9657 - val_loss: 0.2183 - val_acc: 0.9038 - val_auc: 0.9718 - 12s/epoch - 54ms/step
Epoch 13/100
229/229 - 12s - loss: 0.2277 - acc: 0.9072 - auc: 0.9663 - val_loss: 0.2142 - val_acc: 0.9038 - val_auc: 0.9728 - 12s/epoch - 54ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2267 - acc: 0.9116 - auc: 0.9668 - val_loss: 0.2256 - val_acc: 0.9001 - val_auc: 0.9698 - 12s/epoch - 54ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2267 - acc: 0.9080 - auc: 0.9673 - val_loss: 0.2168 - val_acc: 0.9063 - val_auc: 0.9712 - 12s/epoch - 54ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2235 - acc: 0.9103 - auc: 0.9684 - val_loss: 0.2174 - val_acc: 0.9038 - val_auc: 0.9717 - 12s/epoch - 54ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2287 - acc: 0.9073 - auc: 0.9664 - val_loss: 0.2166 - val_acc: 0.9162 - val_auc: 0.9729 - 12s/epoch - 54ms/step
Epoch 18/100
229/229 - 13s - loss: 0.2208 - acc: 0.9105 - auc: 0.9680 - val_loss: 0.2192 - val_acc: 0.9026 - val_auc: 0.9713 - 13s/epoch - 55ms/step
Epoch 19/100
229/229 - 12s - loss: 0.2173 - acc: 0.9109 - auc: 0.9696 - val_loss: 0.2253 - val_acc: 0.8977 - val_auc: 0.9698 - 12s/epoch - 54ms/step
Epoch 20/100
229/229 - 13s - loss: 0.2174 - acc: 0.9123 - auc: 0.9703 - val_loss: 0.2187 - val_acc: 0.9112 - val_auc: 0.9709 - 13s/epoch - 56ms/step
Epoch 21/100
229/229 - 12s - loss: 0.2167 - acc: 0.9147 - auc: 0.9701 - val_loss: 0.2164 - val_acc: 0.9075 - val_auc: 0.9718 - 12s/epoch - 54ms/step
Epoch 22/100
229/229 - 12s - loss: 0.2125 - acc: 0.9170 - auc: 0.9715 - val_loss: 0.2110 - val_acc: 0.9186 - val_auc: 0.9721 - 12s/epoch - 54ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2119 - acc: 0.9150 - auc: 0.9714 - val_loss: 0.2153 - val_acc: 0.9137 - val_auc: 0.9719 - 13s/epoch - 55ms/step
Epoch 24/100
229/229 - 12s - loss: 0.2059 - acc: 0.9172 - auc: 0.9727 - val_loss: 0.2264 - val_acc: 0.9038 - val_auc: 0.9696 - 12s/epoch - 55ms/step
Epoch 25/100
229/229 - 12s - loss: 0.2066 - acc: 0.9153 - auc: 0.9732 - val_loss: 0.2283 - val_acc: 0.9026 - val_auc: 0.9689 - 12s/epoch - 54ms/step
Epoch 26/100
229/229 - 12s - loss: 0.2016 - acc: 0.9199 - auc: 0.9742 - val_loss: 0.2325 - val_acc: 0.9026 - val_auc: 0.9667 - 12s/epoch - 54ms/step
Epoch 27/100
229/229 - 12s - loss: 0.1978 - acc: 0.9206 - auc: 0.9748 - val_loss: 0.2226 - val_acc: 0.9063 - val_auc: 0.9696 - 12s/epoch - 54ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
26/26 - 1s - 642ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.90      0.85      0.88       284
           1       0.92      0.95      0.94       527

    accuracy                           0.92       811
   macro avg       0.91      0.90      0.91       811
weighted avg       0.92      0.92      0.92       811

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.34      0.51      0.41       284
           1       0.64      0.46      0.54       527

    accuracy                           0.48       811
   macro avg       0.49      0.49      0.47       811
weighted avg       0.53      0.48      0.49       811

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 15s - loss: 0.3037 - acc: 0.8676 - auc: 0.9419 - val_loss: 0.2425 - val_acc: 0.9014 - val_auc: 0.9630 - 15s/epoch - 65ms/step
Epoch 2/100
229/229 - 12s - loss: 0.2572 - acc: 0.8951 - auc: 0.9567 - val_loss: 0.2428 - val_acc: 0.9075 - val_auc: 0.9626 - 12s/epoch - 54ms/step
Epoch 3/100
229/229 - 12s - loss: 0.2560 - acc: 0.8973 - auc: 0.9569 - val_loss: 0.2526 - val_acc: 0.8940 - val_auc: 0.9587 - 12s/epoch - 54ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2471 - acc: 0.8994 - auc: 0.9606 - val_loss: 0.2455 - val_acc: 0.9112 - val_auc: 0.9644 - 13s/epoch - 55ms/step
Epoch 5/100
229/229 - 12s - loss: 0.2443 - acc: 0.9020 - auc: 0.9610 - val_loss: 0.2430 - val_acc: 0.9051 - val_auc: 0.9632 - 12s/epoch - 54ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2394 - acc: 0.9036 - auc: 0.9627 - val_loss: 0.2653 - val_acc: 0.8804 - val_auc: 0.9570 - 13s/epoch - 55ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2394 - acc: 0.9031 - auc: 0.9628 - val_loss: 0.2306 - val_acc: 0.9075 - val_auc: 0.9659 - 12s/epoch - 55ms/step
Epoch 8/100
229/229 - 12s - loss: 0.2380 - acc: 0.9047 - auc: 0.9631 - val_loss: 0.2467 - val_acc: 0.8952 - val_auc: 0.9616 - 12s/epoch - 54ms/step
Epoch 9/100
229/229 - 12s - loss: 0.2365 - acc: 0.9043 - auc: 0.9640 - val_loss: 0.2341 - val_acc: 0.9026 - val_auc: 0.9648 - 12s/epoch - 54ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2357 - acc: 0.9016 - auc: 0.9643 - val_loss: 0.2313 - val_acc: 0.9137 - val_auc: 0.9671 - 12s/epoch - 54ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2307 - acc: 0.9081 - auc: 0.9656 - val_loss: 0.2283 - val_acc: 0.9088 - val_auc: 0.9666 - 12s/epoch - 54ms/step
Epoch 12/100
229/229 - 12s - loss: 0.2291 - acc: 0.9053 - auc: 0.9664 - val_loss: 0.2406 - val_acc: 0.9088 - val_auc: 0.9626 - 12s/epoch - 54ms/step
Epoch 13/100
229/229 - 12s - loss: 0.2290 - acc: 0.9065 - auc: 0.9667 - val_loss: 0.2227 - val_acc: 0.9088 - val_auc: 0.9696 - 12s/epoch - 54ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2241 - acc: 0.9090 - auc: 0.9679 - val_loss: 0.2244 - val_acc: 0.9100 - val_auc: 0.9684 - 12s/epoch - 54ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2231 - acc: 0.9083 - auc: 0.9685 - val_loss: 0.2277 - val_acc: 0.9063 - val_auc: 0.9651 - 12s/epoch - 54ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2187 - acc: 0.9101 - auc: 0.9700 - val_loss: 0.2334 - val_acc: 0.9100 - val_auc: 0.9680 - 12s/epoch - 54ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2163 - acc: 0.9149 - auc: 0.9700 - val_loss: 0.2345 - val_acc: 0.9088 - val_auc: 0.9655 - 12s/epoch - 54ms/step
Epoch 18/100
229/229 - 12s - loss: 0.2156 - acc: 0.9124 - auc: 0.9709 - val_loss: 0.2336 - val_acc: 0.8952 - val_auc: 0.9648 - 12s/epoch - 54ms/step
Epoch 19/100
229/229 - 12s - loss: 0.2106 - acc: 0.9166 - auc: 0.9722 - val_loss: 0.2284 - val_acc: 0.9038 - val_auc: 0.9692 - 12s/epoch - 54ms/step
Epoch 20/100
229/229 - 12s - loss: 0.2112 - acc: 0.9147 - auc: 0.9727 - val_loss: 0.2576 - val_acc: 0.8915 - val_auc: 0.9571 - 12s/epoch - 54ms/step
Epoch 21/100
229/229 - 12s - loss: 0.2073 - acc: 0.9155 - auc: 0.9731 - val_loss: 0.2333 - val_acc: 0.9014 - val_auc: 0.9666 - 12s/epoch - 54ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2038 - acc: 0.9154 - auc: 0.9748 - val_loss: 0.2256 - val_acc: 0.9100 - val_auc: 0.9681 - 13s/epoch - 55ms/step
Epoch 23/100
229/229 - 13s - loss: 0.1985 - acc: 0.9192 - auc: 0.9759 - val_loss: 0.2253 - val_acc: 0.9112 - val_auc: 0.9663 - 13s/epoch - 55ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
26/26 - 1s - 685ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.90      0.84      0.87       285
           1       0.91      0.95      0.93       526

    accuracy                           0.91       811
   macro avg       0.91      0.89      0.90       811
weighted avg       0.91      0.91      0.91       811

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.34      0.48      0.40       285
           1       0.64      0.49      0.56       526

    accuracy                           0.49       811
   macro avg       0.49      0.49      0.48       811
weighted avg       0.53      0.49      0.50       811

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
229/229 - 15s - loss: 0.2982 - acc: 0.8719 - auc: 0.9436 - val_loss: 0.2753 - val_acc: 0.8866 - val_auc: 0.9538 - 15s/epoch - 65ms/step
Epoch 2/100
229/229 - 13s - loss: 0.2556 - acc: 0.8938 - auc: 0.9570 - val_loss: 0.2895 - val_acc: 0.8804 - val_auc: 0.9509 - 13s/epoch - 55ms/step
Epoch 3/100
229/229 - 13s - loss: 0.2495 - acc: 0.8984 - auc: 0.9587 - val_loss: 0.2730 - val_acc: 0.8952 - val_auc: 0.9538 - 13s/epoch - 55ms/step
Epoch 4/100
229/229 - 13s - loss: 0.2436 - acc: 0.9003 - auc: 0.9614 - val_loss: 0.2525 - val_acc: 0.8927 - val_auc: 0.9603 - 13s/epoch - 56ms/step
Epoch 5/100
229/229 - 13s - loss: 0.2432 - acc: 0.8997 - auc: 0.9610 - val_loss: 0.2601 - val_acc: 0.8927 - val_auc: 0.9581 - 13s/epoch - 55ms/step
Epoch 6/100
229/229 - 13s - loss: 0.2353 - acc: 0.9044 - auc: 0.9635 - val_loss: 0.2562 - val_acc: 0.8952 - val_auc: 0.9587 - 13s/epoch - 55ms/step
Epoch 7/100
229/229 - 12s - loss: 0.2340 - acc: 0.9057 - auc: 0.9646 - val_loss: 0.2585 - val_acc: 0.8952 - val_auc: 0.9560 - 12s/epoch - 54ms/step
Epoch 8/100
229/229 - 12s - loss: 0.2312 - acc: 0.9088 - auc: 0.9654 - val_loss: 0.2495 - val_acc: 0.8977 - val_auc: 0.9616 - 12s/epoch - 54ms/step
Epoch 9/100
229/229 - 12s - loss: 0.2316 - acc: 0.9075 - auc: 0.9650 - val_loss: 0.2535 - val_acc: 0.8940 - val_auc: 0.9601 - 12s/epoch - 54ms/step
Epoch 10/100
229/229 - 12s - loss: 0.2244 - acc: 0.9070 - auc: 0.9677 - val_loss: 0.2521 - val_acc: 0.8940 - val_auc: 0.9608 - 12s/epoch - 54ms/step
Epoch 11/100
229/229 - 12s - loss: 0.2279 - acc: 0.9068 - auc: 0.9668 - val_loss: 0.2547 - val_acc: 0.8890 - val_auc: 0.9611 - 12s/epoch - 54ms/step
Epoch 12/100
229/229 - 13s - loss: 0.2224 - acc: 0.9114 - auc: 0.9676 - val_loss: 0.2792 - val_acc: 0.8878 - val_auc: 0.9554 - 13s/epoch - 56ms/step
Epoch 13/100
229/229 - 13s - loss: 0.2234 - acc: 0.9080 - auc: 0.9677 - val_loss: 0.2579 - val_acc: 0.8890 - val_auc: 0.9599 - 13s/epoch - 55ms/step
Epoch 14/100
229/229 - 12s - loss: 0.2247 - acc: 0.9092 - auc: 0.9673 - val_loss: 0.2492 - val_acc: 0.8952 - val_auc: 0.9628 - 12s/epoch - 55ms/step
Epoch 15/100
229/229 - 12s - loss: 0.2183 - acc: 0.9133 - auc: 0.9695 - val_loss: 0.2545 - val_acc: 0.8890 - val_auc: 0.9598 - 12s/epoch - 54ms/step
Epoch 16/100
229/229 - 12s - loss: 0.2168 - acc: 0.9124 - auc: 0.9704 - val_loss: 0.2550 - val_acc: 0.8940 - val_auc: 0.9623 - 12s/epoch - 54ms/step
Epoch 17/100
229/229 - 12s - loss: 0.2139 - acc: 0.9151 - auc: 0.9705 - val_loss: 0.2518 - val_acc: 0.8903 - val_auc: 0.9621 - 12s/epoch - 54ms/step
Epoch 18/100
229/229 - 12s - loss: 0.2136 - acc: 0.9143 - auc: 0.9713 - val_loss: 0.2572 - val_acc: 0.8903 - val_auc: 0.9614 - 12s/epoch - 54ms/step
Epoch 19/100
229/229 - 12s - loss: 0.2133 - acc: 0.9147 - auc: 0.9708 - val_loss: 0.2749 - val_acc: 0.8816 - val_auc: 0.9547 - 12s/epoch - 54ms/step
Epoch 20/100
229/229 - 12s - loss: 0.2090 - acc: 0.9165 - auc: 0.9718 - val_loss: 0.2642 - val_acc: 0.8816 - val_auc: 0.9584 - 12s/epoch - 54ms/step
Epoch 21/100
229/229 - 12s - loss: 0.2086 - acc: 0.9170 - auc: 0.9722 - val_loss: 0.2528 - val_acc: 0.8915 - val_auc: 0.9628 - 12s/epoch - 54ms/step
Epoch 22/100
229/229 - 13s - loss: 0.2038 - acc: 0.9176 - auc: 0.9739 - val_loss: 0.2602 - val_acc: 0.8940 - val_auc: 0.9611 - 13s/epoch - 55ms/step
Epoch 23/100
229/229 - 13s - loss: 0.2017 - acc: 0.9205 - auc: 0.9738 - val_loss: 0.2846 - val_acc: 0.8829 - val_auc: 0.9535 - 13s/epoch - 55ms/step
Epoch 24/100
229/229 - 13s - loss: 0.1992 - acc: 0.9183 - auc: 0.9748 - val_loss: 0.2614 - val_acc: 0.8878 - val_auc: 0.9608 - 13s/epoch - 55ms/step
Epoch 25/100
229/229 - 13s - loss: 0.1963 - acc: 0.9228 - auc: 0.9754 - val_loss: 0.2683 - val_acc: 0.8792 - val_auc: 0.9576 - 13s/epoch - 55ms/step
Epoch 26/100
229/229 - 12s - loss: 0.1917 - acc: 0.9239 - auc: 0.9765 - val_loss: 0.2647 - val_acc: 0.8964 - val_auc: 0.9593 - 12s/epoch - 54ms/step
Epoch 27/100
229/229 - 12s - loss: 0.1888 - acc: 0.9261 - auc: 0.9772 - val_loss: 0.2758 - val_acc: 0.8866 - val_auc: 0.9560 - 12s/epoch - 54ms/step
Epoch 28/100
229/229 - 12s - loss: 0.1864 - acc: 0.9255 - auc: 0.9782 - val_loss: 0.2548 - val_acc: 0.8952 - val_auc: 0.9618 - 12s/epoch - 54ms/step
Epoch 29/100
229/229 - 12s - loss: 0.1800 - acc: 0.9290 - auc: 0.9791 - val_loss: 0.2628 - val_acc: 0.8940 - val_auc: 0.9593 - 12s/epoch - 54ms/step
Epoch 30/100
229/229 - 12s - loss: 0.1753 - acc: 0.9333 - auc: 0.9795 - val_loss: 0.2929 - val_acc: 0.8903 - val_auc: 0.9518 - 12s/epoch - 54ms/step
Epoch 31/100
229/229 - 12s - loss: 0.1759 - acc: 0.9310 - auc: 0.9799 - val_loss: 0.2828 - val_acc: 0.8915 - val_auc: 0.9537 - 12s/epoch - 54ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
26/26 - 1s - 649ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.88      0.80      0.84       285
           1       0.90      0.94      0.92       526

    accuracy                           0.89       811
   macro avg       0.89      0.87      0.88       811
weighted avg       0.89      0.89      0.89       811

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.35      0.50      0.42       285
           1       0.65      0.51      0.57       526

    accuracy                           0.50       811
   macro avg       0.50      0.50      0.49       811
weighted avg       0.55      0.50      0.52       811

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.9055] IC [0.8995, 0.9115]
Mean Recall[0.8904] IC [0.8832, 0.8976]
Mean F1[0.8950] IC [0.8883, 0.9016]
Median Accuracy[0.9094]
Median Recall[0.8925]
Median F1[0.8999]
********************txid196627********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.93      0.81      0.86      1182
           1       0.82      0.93      0.87      1077

    accuracy                           0.87      2259
   macro avg       0.87      0.87      0.87      2259
weighted avg       0.87      0.87      0.87      2259

Predicted   0.0   1.0   All
True                       
0           956   226  1182
1            74  1003  1077
All        1030  1229  2259
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3188 - acc: 0.8667 - auc: 0.9352 - val_loss: 0.2651 - val_acc: 0.8994 - val_auc: 0.9541 - 13s/epoch - 68ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2831 - acc: 0.8828 - auc: 0.9492 - val_loss: 0.2514 - val_acc: 0.9146 - val_auc: 0.9572 - 10s/epoch - 57ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2725 - acc: 0.8928 - auc: 0.9526 - val_loss: 0.2606 - val_acc: 0.9085 - val_auc: 0.9528 - 10s/epoch - 52ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2657 - acc: 0.8950 - auc: 0.9550 - val_loss: 0.2390 - val_acc: 0.9177 - val_auc: 0.9598 - 10s/epoch - 54ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2604 - acc: 0.8970 - auc: 0.9559 - val_loss: 0.2366 - val_acc: 0.9192 - val_auc: 0.9632 - 10s/epoch - 55ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2591 - acc: 0.8970 - auc: 0.9576 - val_loss: 0.2299 - val_acc: 0.9146 - val_auc: 0.9648 - 10s/epoch - 55ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2574 - acc: 0.8982 - auc: 0.9579 - val_loss: 0.2257 - val_acc: 0.9207 - val_auc: 0.9653 - 10s/epoch - 56ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2498 - acc: 0.8994 - auc: 0.9607 - val_loss: 0.2287 - val_acc: 0.9207 - val_auc: 0.9650 - 10s/epoch - 56ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2446 - acc: 0.9025 - auc: 0.9623 - val_loss: 0.2263 - val_acc: 0.9207 - val_auc: 0.9642 - 10s/epoch - 56ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2451 - acc: 0.9021 - auc: 0.9618 - val_loss: 0.2272 - val_acc: 0.9207 - val_auc: 0.9644 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2427 - acc: 0.9020 - auc: 0.9627 - val_loss: 0.2252 - val_acc: 0.9223 - val_auc: 0.9667 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2403 - acc: 0.9028 - auc: 0.9631 - val_loss: 0.2501 - val_acc: 0.9085 - val_auc: 0.9576 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2350 - acc: 0.9055 - auc: 0.9655 - val_loss: 0.2411 - val_acc: 0.9101 - val_auc: 0.9614 - 10s/epoch - 55ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2349 - acc: 0.9048 - auc: 0.9656 - val_loss: 0.2412 - val_acc: 0.9101 - val_auc: 0.9643 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2291 - acc: 0.9074 - auc: 0.9671 - val_loss: 0.2372 - val_acc: 0.9146 - val_auc: 0.9633 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2321 - acc: 0.9074 - auc: 0.9659 - val_loss: 0.2195 - val_acc: 0.9238 - val_auc: 0.9673 - 10s/epoch - 55ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2271 - acc: 0.9072 - auc: 0.9664 - val_loss: 0.2304 - val_acc: 0.9131 - val_auc: 0.9657 - 10s/epoch - 55ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2268 - acc: 0.9086 - auc: 0.9677 - val_loss: 0.2226 - val_acc: 0.9238 - val_auc: 0.9653 - 10s/epoch - 55ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2263 - acc: 0.9069 - auc: 0.9688 - val_loss: 0.2215 - val_acc: 0.9101 - val_auc: 0.9681 - 10s/epoch - 55ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2200 - acc: 0.9118 - auc: 0.9701 - val_loss: 0.2228 - val_acc: 0.9192 - val_auc: 0.9669 - 10s/epoch - 55ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2149 - acc: 0.9120 - auc: 0.9712 - val_loss: 0.2300 - val_acc: 0.9116 - val_auc: 0.9657 - 10s/epoch - 56ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2232 - acc: 0.9086 - auc: 0.9693 - val_loss: 0.2297 - val_acc: 0.9131 - val_auc: 0.9670 - 10s/epoch - 55ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2141 - acc: 0.9121 - auc: 0.9718 - val_loss: 0.2207 - val_acc: 0.9146 - val_auc: 0.9683 - 10s/epoch - 56ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2094 - acc: 0.9137 - auc: 0.9722 - val_loss: 0.2239 - val_acc: 0.9192 - val_auc: 0.9660 - 10s/epoch - 55ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2077 - acc: 0.9170 - auc: 0.9727 - val_loss: 0.2305 - val_acc: 0.9040 - val_auc: 0.9672 - 10s/epoch - 56ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2111 - acc: 0.9145 - auc: 0.9725 - val_loss: 0.2266 - val_acc: 0.9101 - val_auc: 0.9675 - 10s/epoch - 56ms/step
Epoch 27/100
185/185 - 10s - loss: 0.2008 - acc: 0.9250 - auc: 0.9738 - val_loss: 0.2347 - val_acc: 0.9146 - val_auc: 0.9636 - 10s/epoch - 56ms/step
Epoch 28/100
185/185 - 10s - loss: 0.2022 - acc: 0.9199 - auc: 0.9743 - val_loss: 0.2376 - val_acc: 0.9055 - val_auc: 0.9670 - 10s/epoch - 56ms/step
Epoch 29/100
185/185 - 10s - loss: 0.2034 - acc: 0.9218 - auc: 0.9738 - val_loss: 0.2194 - val_acc: 0.9253 - val_auc: 0.9695 - 10s/epoch - 57ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1889 - acc: 0.9250 - auc: 0.9775 - val_loss: 0.2412 - val_acc: 0.9070 - val_auc: 0.9641 - 10s/epoch - 56ms/step
Epoch 31/100
185/185 - 10s - loss: 0.1881 - acc: 0.9264 - auc: 0.9776 - val_loss: 0.2436 - val_acc: 0.9116 - val_auc: 0.9637 - 10s/epoch - 56ms/step
Epoch 32/100
185/185 - 10s - loss: 0.1847 - acc: 0.9259 - auc: 0.9788 - val_loss: 0.2392 - val_acc: 0.9207 - val_auc: 0.9656 - 10s/epoch - 56ms/step
Epoch 33/100
185/185 - 10s - loss: 0.1743 - acc: 0.9303 - auc: 0.9812 - val_loss: 0.2441 - val_acc: 0.9070 - val_auc: 0.9637 - 10s/epoch - 56ms/step
Epoch 34/100
185/185 - 10s - loss: 0.1750 - acc: 0.9349 - auc: 0.9809 - val_loss: 0.2385 - val_acc: 0.9146 - val_auc: 0.9631 - 10s/epoch - 56ms/step
Epoch 35/100
185/185 - 10s - loss: 0.1779 - acc: 0.9303 - auc: 0.9802 - val_loss: 0.2333 - val_acc: 0.9192 - val_auc: 0.9682 - 10s/epoch - 56ms/step
Epoch 36/100
185/185 - 10s - loss: 0.1667 - acc: 0.9352 - auc: 0.9826 - val_loss: 0.2369 - val_acc: 0.9116 - val_auc: 0.9670 - 10s/epoch - 56ms/step
Epoch 37/100
185/185 - 10s - loss: 0.1689 - acc: 0.9333 - auc: 0.9820 - val_loss: 0.2467 - val_acc: 0.9131 - val_auc: 0.9644 - 10s/epoch - 56ms/step
Epoch 38/100
185/185 - 10s - loss: 0.1533 - acc: 0.9403 - auc: 0.9849 - val_loss: 0.2506 - val_acc: 0.9253 - val_auc: 0.9627 - 10s/epoch - 56ms/step
Epoch 39/100
185/185 - 10s - loss: 0.1541 - acc: 0.9393 - auc: 0.9850 - val_loss: 0.2508 - val_acc: 0.9177 - val_auc: 0.9614 - 10s/epoch - 56ms/step
Early stopping epoch: 38
******Evaluating TEST set*********
21/21 - 1s - 622ms/epoch - 30ms/step
              precision    recall  f1-score   support

           0       0.91      0.84      0.87       194
           1       0.93      0.96      0.95       462

    accuracy                           0.93       656
   macro avg       0.92      0.90      0.91       656
weighted avg       0.92      0.93      0.92       656

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.31      0.50      0.38       194
           1       0.71      0.53      0.61       462

    accuracy                           0.52       656
   macro avg       0.51      0.51      0.49       656
weighted avg       0.59      0.52      0.54       656

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3096 - acc: 0.8733 - auc: 0.9387 - val_loss: 0.3351 - val_acc: 0.8779 - val_auc: 0.9344 - 13s/epoch - 68ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2793 - acc: 0.8916 - auc: 0.9490 - val_loss: 0.3052 - val_acc: 0.8794 - val_auc: 0.9418 - 10s/epoch - 56ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2644 - acc: 0.8986 - auc: 0.9548 - val_loss: 0.3272 - val_acc: 0.8534 - val_auc: 0.9347 - 10s/epoch - 56ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2620 - acc: 0.8970 - auc: 0.9561 - val_loss: 0.3009 - val_acc: 0.8748 - val_auc: 0.9458 - 10s/epoch - 56ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2547 - acc: 0.9008 - auc: 0.9588 - val_loss: 0.2765 - val_acc: 0.8870 - val_auc: 0.9526 - 10s/epoch - 56ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2503 - acc: 0.9009 - auc: 0.9607 - val_loss: 0.2724 - val_acc: 0.8809 - val_auc: 0.9535 - 10s/epoch - 56ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2449 - acc: 0.9033 - auc: 0.9621 - val_loss: 0.2733 - val_acc: 0.8962 - val_auc: 0.9553 - 10s/epoch - 56ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2436 - acc: 0.9040 - auc: 0.9627 - val_loss: 0.2686 - val_acc: 0.8947 - val_auc: 0.9541 - 10s/epoch - 56ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2392 - acc: 0.9064 - auc: 0.9635 - val_loss: 0.2611 - val_acc: 0.8901 - val_auc: 0.9548 - 10s/epoch - 56ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2410 - acc: 0.9057 - auc: 0.9637 - val_loss: 0.2666 - val_acc: 0.8947 - val_auc: 0.9550 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2360 - acc: 0.9071 - auc: 0.9648 - val_loss: 0.2744 - val_acc: 0.8916 - val_auc: 0.9561 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2411 - acc: 0.9033 - auc: 0.9633 - val_loss: 0.2675 - val_acc: 0.8901 - val_auc: 0.9527 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2339 - acc: 0.9074 - auc: 0.9653 - val_loss: 0.2646 - val_acc: 0.8885 - val_auc: 0.9569 - 10s/epoch - 55ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2320 - acc: 0.9082 - auc: 0.9658 - val_loss: 0.2725 - val_acc: 0.8931 - val_auc: 0.9564 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2275 - acc: 0.9059 - auc: 0.9683 - val_loss: 0.2605 - val_acc: 0.8931 - val_auc: 0.9574 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2243 - acc: 0.9123 - auc: 0.9683 - val_loss: 0.2757 - val_acc: 0.8901 - val_auc: 0.9511 - 10s/epoch - 55ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2261 - acc: 0.9115 - auc: 0.9681 - val_loss: 0.2657 - val_acc: 0.8977 - val_auc: 0.9568 - 10s/epoch - 55ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2208 - acc: 0.9140 - auc: 0.9696 - val_loss: 0.2737 - val_acc: 0.8885 - val_auc: 0.9532 - 10s/epoch - 55ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2235 - acc: 0.9132 - auc: 0.9684 - val_loss: 0.2721 - val_acc: 0.8901 - val_auc: 0.9537 - 10s/epoch - 55ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2169 - acc: 0.9157 - auc: 0.9703 - val_loss: 0.2750 - val_acc: 0.8977 - val_auc: 0.9564 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2103 - acc: 0.9191 - auc: 0.9727 - val_loss: 0.2717 - val_acc: 0.8885 - val_auc: 0.9548 - 10s/epoch - 53ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2108 - acc: 0.9152 - auc: 0.9727 - val_loss: 0.2705 - val_acc: 0.8962 - val_auc: 0.9545 - 10s/epoch - 55ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2060 - acc: 0.9188 - auc: 0.9737 - val_loss: 0.2693 - val_acc: 0.8855 - val_auc: 0.9543 - 10s/epoch - 55ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2027 - acc: 0.9215 - auc: 0.9737 - val_loss: 0.2745 - val_acc: 0.8947 - val_auc: 0.9558 - 10s/epoch - 55ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2004 - acc: 0.9196 - auc: 0.9752 - val_loss: 0.2755 - val_acc: 0.8855 - val_auc: 0.9542 - 10s/epoch - 55ms/step
Early stopping epoch: 24
******Evaluating TEST set*********
21/21 - 1s - 590ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.84      0.78      0.81       193
           1       0.91      0.94      0.93       462

    accuracy                           0.89       655
   macro avg       0.88      0.86      0.87       655
weighted avg       0.89      0.89      0.89       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.26      0.46      0.33       193
           1       0.66      0.45      0.53       462

    accuracy                           0.45       655
   macro avg       0.46      0.45      0.43       655
weighted avg       0.54      0.45      0.47       655

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3171 - acc: 0.8730 - auc: 0.9361 - val_loss: 0.3548 - val_acc: 0.8702 - val_auc: 0.9357 - 13s/epoch - 69ms/step
Epoch 2/100
185/185 - 11s - loss: 0.2766 - acc: 0.8857 - auc: 0.9514 - val_loss: 0.2987 - val_acc: 0.8840 - val_auc: 0.9431 - 11s/epoch - 57ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2683 - acc: 0.8969 - auc: 0.9535 - val_loss: 0.2687 - val_acc: 0.9008 - val_auc: 0.9539 - 10s/epoch - 56ms/step
Epoch 4/100
185/185 - 11s - loss: 0.2593 - acc: 0.8969 - auc: 0.9569 - val_loss: 0.2745 - val_acc: 0.8901 - val_auc: 0.9531 - 11s/epoch - 57ms/step
Epoch 5/100
185/185 - 11s - loss: 0.2544 - acc: 0.9016 - auc: 0.9584 - val_loss: 0.2672 - val_acc: 0.8916 - val_auc: 0.9551 - 11s/epoch - 57ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2553 - acc: 0.9033 - auc: 0.9583 - val_loss: 0.2964 - val_acc: 0.8672 - val_auc: 0.9485 - 10s/epoch - 57ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2506 - acc: 0.9011 - auc: 0.9603 - val_loss: 0.2754 - val_acc: 0.8947 - val_auc: 0.9541 - 10s/epoch - 55ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2451 - acc: 0.9040 - auc: 0.9620 - val_loss: 0.2689 - val_acc: 0.8916 - val_auc: 0.9562 - 10s/epoch - 54ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2409 - acc: 0.9052 - auc: 0.9628 - val_loss: 0.2729 - val_acc: 0.8992 - val_auc: 0.9554 - 10s/epoch - 54ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2415 - acc: 0.9028 - auc: 0.9637 - val_loss: 0.2664 - val_acc: 0.8931 - val_auc: 0.9580 - 10s/epoch - 54ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2393 - acc: 0.9043 - auc: 0.9642 - val_loss: 0.2704 - val_acc: 0.9008 - val_auc: 0.9537 - 10s/epoch - 54ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2349 - acc: 0.9050 - auc: 0.9652 - val_loss: 0.2607 - val_acc: 0.8947 - val_auc: 0.9580 - 10s/epoch - 54ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2385 - acc: 0.9071 - auc: 0.9636 - val_loss: 0.2766 - val_acc: 0.8779 - val_auc: 0.9543 - 10s/epoch - 53ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2313 - acc: 0.9086 - auc: 0.9664 - val_loss: 0.2818 - val_acc: 0.8962 - val_auc: 0.9549 - 10s/epoch - 54ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2286 - acc: 0.9118 - auc: 0.9667 - val_loss: 0.2646 - val_acc: 0.8931 - val_auc: 0.9585 - 10s/epoch - 54ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2279 - acc: 0.9099 - auc: 0.9675 - val_loss: 0.2912 - val_acc: 0.8702 - val_auc: 0.9513 - 10s/epoch - 53ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2235 - acc: 0.9106 - auc: 0.9687 - val_loss: 0.2645 - val_acc: 0.8931 - val_auc: 0.9568 - 10s/epoch - 53ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2199 - acc: 0.9108 - auc: 0.9690 - val_loss: 0.2750 - val_acc: 0.8901 - val_auc: 0.9537 - 10s/epoch - 53ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2221 - acc: 0.9123 - auc: 0.9686 - val_loss: 0.2645 - val_acc: 0.8901 - val_auc: 0.9570 - 10s/epoch - 53ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2175 - acc: 0.9137 - auc: 0.9705 - val_loss: 0.2548 - val_acc: 0.8855 - val_auc: 0.9587 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2187 - acc: 0.9091 - auc: 0.9698 - val_loss: 0.2685 - val_acc: 0.8947 - val_auc: 0.9586 - 10s/epoch - 55ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2135 - acc: 0.9154 - auc: 0.9711 - val_loss: 0.2641 - val_acc: 0.8840 - val_auc: 0.9555 - 10s/epoch - 56ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2114 - acc: 0.9164 - auc: 0.9720 - val_loss: 0.2589 - val_acc: 0.8870 - val_auc: 0.9575 - 10s/epoch - 55ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2097 - acc: 0.9182 - auc: 0.9725 - val_loss: 0.2752 - val_acc: 0.8855 - val_auc: 0.9576 - 10s/epoch - 56ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2077 - acc: 0.9167 - auc: 0.9725 - val_loss: 0.2743 - val_acc: 0.8962 - val_auc: 0.9588 - 10s/epoch - 56ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2012 - acc: 0.9199 - auc: 0.9747 - val_loss: 0.2650 - val_acc: 0.8840 - val_auc: 0.9573 - 10s/epoch - 56ms/step
Epoch 27/100
185/185 - 10s - loss: 0.1997 - acc: 0.9205 - auc: 0.9751 - val_loss: 0.2722 - val_acc: 0.8885 - val_auc: 0.9543 - 10s/epoch - 56ms/step
Epoch 28/100
185/185 - 10s - loss: 0.2029 - acc: 0.9216 - auc: 0.9734 - val_loss: 0.2693 - val_acc: 0.8931 - val_auc: 0.9567 - 10s/epoch - 56ms/step
Epoch 29/100
185/185 - 10s - loss: 0.1954 - acc: 0.9250 - auc: 0.9752 - val_loss: 0.2697 - val_acc: 0.8916 - val_auc: 0.9580 - 10s/epoch - 56ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1851 - acc: 0.9269 - auc: 0.9780 - val_loss: 0.2779 - val_acc: 0.8901 - val_auc: 0.9546 - 10s/epoch - 56ms/step
Epoch 31/100
185/185 - 10s - loss: 0.1859 - acc: 0.9291 - auc: 0.9781 - val_loss: 0.2772 - val_acc: 0.8931 - val_auc: 0.9566 - 10s/epoch - 54ms/step
Epoch 32/100
185/185 - 10s - loss: 0.1731 - acc: 0.9293 - auc: 0.9810 - val_loss: 0.2871 - val_acc: 0.8870 - val_auc: 0.9474 - 10s/epoch - 54ms/step
Epoch 33/100
185/185 - 10s - loss: 0.1726 - acc: 0.9315 - auc: 0.9804 - val_loss: 0.3054 - val_acc: 0.8824 - val_auc: 0.9517 - 10s/epoch - 56ms/step
Epoch 34/100
185/185 - 10s - loss: 0.1722 - acc: 0.9318 - auc: 0.9806 - val_loss: 0.2970 - val_acc: 0.8855 - val_auc: 0.9478 - 10s/epoch - 56ms/step
Epoch 35/100
185/185 - 10s - loss: 0.1706 - acc: 0.9306 - auc: 0.9813 - val_loss: 0.3040 - val_acc: 0.8870 - val_auc: 0.9536 - 10s/epoch - 56ms/step
Early stopping epoch: 34
******Evaluating TEST set*********
21/21 - 1s - 611ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.91      0.72      0.80       193
           1       0.89      0.97      0.93       462

    accuracy                           0.90       655
   macro avg       0.90      0.84      0.87       655
weighted avg       0.90      0.90      0.89       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.29      0.50      0.36       193
           1       0.70      0.48      0.57       462

    accuracy                           0.49       655
   macro avg       0.49      0.49      0.47       655
weighted avg       0.58      0.49      0.51       655

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3166 - acc: 0.8694 - auc: 0.9360 - val_loss: 0.2870 - val_acc: 0.8748 - val_auc: 0.9501 - 13s/epoch - 68ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2847 - acc: 0.8898 - auc: 0.9465 - val_loss: 0.2782 - val_acc: 0.8840 - val_auc: 0.9539 - 10s/epoch - 56ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2731 - acc: 0.8984 - auc: 0.9512 - val_loss: 0.2555 - val_acc: 0.8962 - val_auc: 0.9587 - 10s/epoch - 56ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2637 - acc: 0.8986 - auc: 0.9548 - val_loss: 0.2482 - val_acc: 0.8947 - val_auc: 0.9620 - 10s/epoch - 56ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2627 - acc: 0.8981 - auc: 0.9557 - val_loss: 0.2467 - val_acc: 0.9069 - val_auc: 0.9644 - 10s/epoch - 57ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2558 - acc: 0.9009 - auc: 0.9587 - val_loss: 0.2382 - val_acc: 0.9008 - val_auc: 0.9660 - 10s/epoch - 55ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2532 - acc: 0.9003 - auc: 0.9594 - val_loss: 0.2393 - val_acc: 0.9038 - val_auc: 0.9656 - 10s/epoch - 55ms/step
Epoch 8/100
185/185 - 11s - loss: 0.2515 - acc: 0.9032 - auc: 0.9603 - val_loss: 0.2371 - val_acc: 0.9038 - val_auc: 0.9660 - 11s/epoch - 58ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2468 - acc: 0.9020 - auc: 0.9615 - val_loss: 0.2436 - val_acc: 0.9008 - val_auc: 0.9674 - 10s/epoch - 55ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2420 - acc: 0.9081 - auc: 0.9635 - val_loss: 0.2341 - val_acc: 0.9038 - val_auc: 0.9676 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2409 - acc: 0.9057 - auc: 0.9639 - val_loss: 0.2357 - val_acc: 0.9069 - val_auc: 0.9668 - 10s/epoch - 54ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2368 - acc: 0.9084 - auc: 0.9648 - val_loss: 0.2390 - val_acc: 0.8977 - val_auc: 0.9652 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2388 - acc: 0.9055 - auc: 0.9633 - val_loss: 0.2360 - val_acc: 0.9099 - val_auc: 0.9659 - 10s/epoch - 54ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2342 - acc: 0.9074 - auc: 0.9656 - val_loss: 0.2442 - val_acc: 0.8962 - val_auc: 0.9646 - 10s/epoch - 54ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2326 - acc: 0.9088 - auc: 0.9662 - val_loss: 0.2235 - val_acc: 0.9069 - val_auc: 0.9694 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2305 - acc: 0.9094 - auc: 0.9658 - val_loss: 0.2241 - val_acc: 0.9160 - val_auc: 0.9693 - 10s/epoch - 54ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2286 - acc: 0.9091 - auc: 0.9671 - val_loss: 0.2280 - val_acc: 0.9053 - val_auc: 0.9685 - 10s/epoch - 54ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2213 - acc: 0.9125 - auc: 0.9693 - val_loss: 0.2276 - val_acc: 0.9130 - val_auc: 0.9676 - 10s/epoch - 54ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2233 - acc: 0.9125 - auc: 0.9684 - val_loss: 0.2527 - val_acc: 0.8947 - val_auc: 0.9609 - 10s/epoch - 54ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2212 - acc: 0.9130 - auc: 0.9691 - val_loss: 0.2198 - val_acc: 0.9069 - val_auc: 0.9705 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2215 - acc: 0.9123 - auc: 0.9694 - val_loss: 0.2521 - val_acc: 0.8947 - val_auc: 0.9640 - 10s/epoch - 54ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2190 - acc: 0.9137 - auc: 0.9696 - val_loss: 0.2275 - val_acc: 0.9038 - val_auc: 0.9691 - 10s/epoch - 54ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2138 - acc: 0.9140 - auc: 0.9714 - val_loss: 0.2318 - val_acc: 0.9115 - val_auc: 0.9678 - 10s/epoch - 53ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2091 - acc: 0.9159 - auc: 0.9724 - val_loss: 0.2200 - val_acc: 0.9008 - val_auc: 0.9704 - 10s/epoch - 53ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2100 - acc: 0.9155 - auc: 0.9715 - val_loss: 0.2236 - val_acc: 0.9099 - val_auc: 0.9684 - 10s/epoch - 54ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2049 - acc: 0.9220 - auc: 0.9735 - val_loss: 0.2344 - val_acc: 0.9023 - val_auc: 0.9672 - 10s/epoch - 54ms/step
Epoch 27/100
185/185 - 10s - loss: 0.1998 - acc: 0.9179 - auc: 0.9748 - val_loss: 0.2403 - val_acc: 0.9008 - val_auc: 0.9657 - 10s/epoch - 54ms/step
Epoch 28/100
185/185 - 10s - loss: 0.1983 - acc: 0.9227 - auc: 0.9744 - val_loss: 0.2336 - val_acc: 0.9038 - val_auc: 0.9682 - 10s/epoch - 54ms/step
Epoch 29/100
185/185 - 10s - loss: 0.1957 - acc: 0.9230 - auc: 0.9760 - val_loss: 0.2306 - val_acc: 0.9053 - val_auc: 0.9685 - 10s/epoch - 54ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1881 - acc: 0.9261 - auc: 0.9771 - val_loss: 0.2563 - val_acc: 0.8885 - val_auc: 0.9608 - 10s/epoch - 54ms/step
Early stopping epoch: 29
******Evaluating TEST set*********
21/21 - 1s - 581ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.90      0.77      0.83       193
           1       0.91      0.96      0.94       462

    accuracy                           0.91       655
   macro avg       0.90      0.87      0.88       655
weighted avg       0.91      0.91      0.90       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.28      0.48      0.35       193
           1       0.69      0.48      0.56       462

    accuracy                           0.48       655
   macro avg       0.48      0.48      0.46       655
weighted avg       0.57      0.48      0.50       655

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3226 - acc: 0.8687 - auc: 0.9338 - val_loss: 0.2542 - val_acc: 0.8962 - val_auc: 0.9614 - 13s/epoch - 68ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2837 - acc: 0.8857 - auc: 0.9488 - val_loss: 0.2363 - val_acc: 0.9008 - val_auc: 0.9647 - 10s/epoch - 56ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2723 - acc: 0.8915 - auc: 0.9525 - val_loss: 0.2326 - val_acc: 0.9084 - val_auc: 0.9658 - 10s/epoch - 54ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2629 - acc: 0.8933 - auc: 0.9555 - val_loss: 0.2357 - val_acc: 0.9084 - val_auc: 0.9663 - 10s/epoch - 54ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2595 - acc: 0.8974 - auc: 0.9572 - val_loss: 0.2447 - val_acc: 0.9084 - val_auc: 0.9658 - 10s/epoch - 54ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2584 - acc: 0.8969 - auc: 0.9578 - val_loss: 0.2371 - val_acc: 0.9069 - val_auc: 0.9651 - 10s/epoch - 54ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2533 - acc: 0.9003 - auc: 0.9589 - val_loss: 0.2357 - val_acc: 0.9069 - val_auc: 0.9667 - 10s/epoch - 54ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2501 - acc: 0.9013 - auc: 0.9599 - val_loss: 0.2271 - val_acc: 0.9145 - val_auc: 0.9695 - 10s/epoch - 54ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2440 - acc: 0.9043 - auc: 0.9618 - val_loss: 0.2310 - val_acc: 0.9069 - val_auc: 0.9692 - 10s/epoch - 54ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2401 - acc: 0.9035 - auc: 0.9632 - val_loss: 0.2356 - val_acc: 0.9053 - val_auc: 0.9662 - 10s/epoch - 54ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2459 - acc: 0.9033 - auc: 0.9618 - val_loss: 0.2385 - val_acc: 0.9053 - val_auc: 0.9670 - 10s/epoch - 54ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2382 - acc: 0.9067 - auc: 0.9638 - val_loss: 0.2278 - val_acc: 0.9115 - val_auc: 0.9692 - 10s/epoch - 54ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2341 - acc: 0.9067 - auc: 0.9655 - val_loss: 0.2326 - val_acc: 0.9145 - val_auc: 0.9678 - 10s/epoch - 54ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2332 - acc: 0.9089 - auc: 0.9658 - val_loss: 0.2291 - val_acc: 0.9099 - val_auc: 0.9683 - 10s/epoch - 54ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2293 - acc: 0.9079 - auc: 0.9677 - val_loss: 0.2372 - val_acc: 0.9038 - val_auc: 0.9661 - 10s/epoch - 57ms/step
Epoch 16/100
185/185 - 11s - loss: 0.2265 - acc: 0.9106 - auc: 0.9670 - val_loss: 0.2232 - val_acc: 0.9145 - val_auc: 0.9706 - 11s/epoch - 57ms/step
Epoch 17/100
185/185 - 11s - loss: 0.2286 - acc: 0.9103 - auc: 0.9672 - val_loss: 0.2318 - val_acc: 0.9008 - val_auc: 0.9682 - 11s/epoch - 58ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2235 - acc: 0.9123 - auc: 0.9681 - val_loss: 0.2444 - val_acc: 0.8947 - val_auc: 0.9647 - 10s/epoch - 56ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2190 - acc: 0.9140 - auc: 0.9692 - val_loss: 0.2335 - val_acc: 0.9038 - val_auc: 0.9663 - 10s/epoch - 54ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2203 - acc: 0.9130 - auc: 0.9694 - val_loss: 0.2426 - val_acc: 0.9084 - val_auc: 0.9649 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2127 - acc: 0.9194 - auc: 0.9710 - val_loss: 0.2379 - val_acc: 0.9023 - val_auc: 0.9662 - 10s/epoch - 54ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2130 - acc: 0.9154 - auc: 0.9708 - val_loss: 0.2356 - val_acc: 0.8916 - val_auc: 0.9684 - 10s/epoch - 54ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2084 - acc: 0.9193 - auc: 0.9726 - val_loss: 0.2296 - val_acc: 0.9084 - val_auc: 0.9684 - 10s/epoch - 54ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2025 - acc: 0.9235 - auc: 0.9740 - val_loss: 0.2435 - val_acc: 0.9023 - val_auc: 0.9638 - 10s/epoch - 54ms/step
Epoch 25/100
185/185 - 10s - loss: 0.1981 - acc: 0.9218 - auc: 0.9751 - val_loss: 0.2378 - val_acc: 0.9069 - val_auc: 0.9654 - 10s/epoch - 54ms/step
Epoch 26/100
185/185 - 10s - loss: 0.1968 - acc: 0.9222 - auc: 0.9759 - val_loss: 0.2269 - val_acc: 0.9008 - val_auc: 0.9703 - 10s/epoch - 54ms/step
Early stopping epoch: 25
******Evaluating TEST set*********
21/21 - 1s - 583ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.90      0.80      0.85       193
           1       0.92      0.96      0.94       462

    accuracy                           0.91       655
   macro avg       0.91      0.88      0.89       655
weighted avg       0.91      0.91      0.91       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.27      0.45      0.34       193
           1       0.68      0.49      0.57       462

    accuracy                           0.48       655
   macro avg       0.48      0.47      0.46       655
weighted avg       0.56      0.48      0.50       655

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 12s - loss: 0.3208 - acc: 0.8682 - auc: 0.9332 - val_loss: 0.2714 - val_acc: 0.9023 - val_auc: 0.9536 - 12s/epoch - 66ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2863 - acc: 0.8860 - auc: 0.9463 - val_loss: 0.2559 - val_acc: 0.9038 - val_auc: 0.9557 - 10s/epoch - 54ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2711 - acc: 0.8920 - auc: 0.9533 - val_loss: 0.2695 - val_acc: 0.8901 - val_auc: 0.9536 - 10s/epoch - 54ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2676 - acc: 0.8942 - auc: 0.9547 - val_loss: 0.2473 - val_acc: 0.9115 - val_auc: 0.9607 - 10s/epoch - 54ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2577 - acc: 0.9003 - auc: 0.9575 - val_loss: 0.2389 - val_acc: 0.9069 - val_auc: 0.9607 - 10s/epoch - 54ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2576 - acc: 0.8981 - auc: 0.9586 - val_loss: 0.2425 - val_acc: 0.9053 - val_auc: 0.9623 - 10s/epoch - 54ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2495 - acc: 0.8991 - auc: 0.9607 - val_loss: 0.2422 - val_acc: 0.9099 - val_auc: 0.9628 - 10s/epoch - 54ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2505 - acc: 0.9006 - auc: 0.9600 - val_loss: 0.2331 - val_acc: 0.9145 - val_auc: 0.9672 - 10s/epoch - 54ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2449 - acc: 0.9025 - auc: 0.9620 - val_loss: 0.2392 - val_acc: 0.9008 - val_auc: 0.9633 - 10s/epoch - 54ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2512 - acc: 0.9008 - auc: 0.9589 - val_loss: 0.2306 - val_acc: 0.9115 - val_auc: 0.9654 - 10s/epoch - 54ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2406 - acc: 0.9057 - auc: 0.9626 - val_loss: 0.2251 - val_acc: 0.9191 - val_auc: 0.9683 - 10s/epoch - 54ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2468 - acc: 0.9020 - auc: 0.9618 - val_loss: 0.2243 - val_acc: 0.9191 - val_auc: 0.9673 - 10s/epoch - 54ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2407 - acc: 0.9037 - auc: 0.9632 - val_loss: 0.2205 - val_acc: 0.9252 - val_auc: 0.9695 - 10s/epoch - 56ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2354 - acc: 0.9040 - auc: 0.9648 - val_loss: 0.2165 - val_acc: 0.9221 - val_auc: 0.9704 - 10s/epoch - 54ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2347 - acc: 0.9057 - auc: 0.9648 - val_loss: 0.2303 - val_acc: 0.9191 - val_auc: 0.9653 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2303 - acc: 0.9094 - auc: 0.9667 - val_loss: 0.2159 - val_acc: 0.9206 - val_auc: 0.9707 - 10s/epoch - 55ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2291 - acc: 0.9096 - auc: 0.9662 - val_loss: 0.2186 - val_acc: 0.9176 - val_auc: 0.9706 - 10s/epoch - 54ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2276 - acc: 0.9094 - auc: 0.9678 - val_loss: 0.2329 - val_acc: 0.9206 - val_auc: 0.9689 - 10s/epoch - 54ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2244 - acc: 0.9094 - auc: 0.9682 - val_loss: 0.2236 - val_acc: 0.9221 - val_auc: 0.9684 - 10s/epoch - 55ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2242 - acc: 0.9113 - auc: 0.9679 - val_loss: 0.2275 - val_acc: 0.9191 - val_auc: 0.9665 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2180 - acc: 0.9121 - auc: 0.9700 - val_loss: 0.2179 - val_acc: 0.9206 - val_auc: 0.9701 - 10s/epoch - 54ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2113 - acc: 0.9167 - auc: 0.9719 - val_loss: 0.2231 - val_acc: 0.9145 - val_auc: 0.9694 - 10s/epoch - 54ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2103 - acc: 0.9154 - auc: 0.9718 - val_loss: 0.2146 - val_acc: 0.9221 - val_auc: 0.9711 - 10s/epoch - 54ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2120 - acc: 0.9128 - auc: 0.9719 - val_loss: 0.2396 - val_acc: 0.9069 - val_auc: 0.9644 - 10s/epoch - 54ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2036 - acc: 0.9181 - auc: 0.9741 - val_loss: 0.2133 - val_acc: 0.9313 - val_auc: 0.9720 - 10s/epoch - 54ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2010 - acc: 0.9181 - auc: 0.9752 - val_loss: 0.2212 - val_acc: 0.9252 - val_auc: 0.9698 - 10s/epoch - 54ms/step
Epoch 27/100
185/185 - 10s - loss: 0.1977 - acc: 0.9223 - auc: 0.9748 - val_loss: 0.2510 - val_acc: 0.9084 - val_auc: 0.9606 - 10s/epoch - 54ms/step
Epoch 28/100
185/185 - 10s - loss: 0.1975 - acc: 0.9222 - auc: 0.9755 - val_loss: 0.2177 - val_acc: 0.9221 - val_auc: 0.9694 - 10s/epoch - 54ms/step
Epoch 29/100
185/185 - 10s - loss: 0.1862 - acc: 0.9286 - auc: 0.9785 - val_loss: 0.2221 - val_acc: 0.9176 - val_auc: 0.9674 - 10s/epoch - 54ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1864 - acc: 0.9264 - auc: 0.9779 - val_loss: 0.2310 - val_acc: 0.9145 - val_auc: 0.9647 - 10s/epoch - 54ms/step
Epoch 31/100
185/185 - 10s - loss: 0.1815 - acc: 0.9293 - auc: 0.9791 - val_loss: 0.2385 - val_acc: 0.9069 - val_auc: 0.9611 - 10s/epoch - 54ms/step
Epoch 32/100
185/185 - 10s - loss: 0.1710 - acc: 0.9315 - auc: 0.9823 - val_loss: 0.2447 - val_acc: 0.9191 - val_auc: 0.9607 - 10s/epoch - 54ms/step
Epoch 33/100
185/185 - 10s - loss: 0.1703 - acc: 0.9328 - auc: 0.9818 - val_loss: 0.2351 - val_acc: 0.9130 - val_auc: 0.9618 - 10s/epoch - 55ms/step
Epoch 34/100
185/185 - 11s - loss: 0.1607 - acc: 0.9391 - auc: 0.9831 - val_loss: 0.2646 - val_acc: 0.9023 - val_auc: 0.9556 - 11s/epoch - 59ms/step
Epoch 35/100
185/185 - 11s - loss: 0.1562 - acc: 0.9393 - auc: 0.9847 - val_loss: 0.2800 - val_acc: 0.9008 - val_auc: 0.9576 - 11s/epoch - 58ms/step
Early stopping epoch: 34
******Evaluating TEST set*********
21/21 - 1s - 575ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.92      0.84      0.88       193
           1       0.94      0.97      0.95       462

    accuracy                           0.93       655
   macro avg       0.93      0.90      0.92       655
weighted avg       0.93      0.93      0.93       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.27      0.44      0.34       193
           1       0.69      0.51      0.58       462

    accuracy                           0.49       655
   macro avg       0.48      0.47      0.46       655
weighted avg       0.56      0.49      0.51       655

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 14s - loss: 0.3201 - acc: 0.8675 - auc: 0.9351 - val_loss: 0.2629 - val_acc: 0.8931 - val_auc: 0.9584 - 14s/epoch - 73ms/step
Epoch 2/100
185/185 - 11s - loss: 0.2783 - acc: 0.8913 - auc: 0.9502 - val_loss: 0.2570 - val_acc: 0.9038 - val_auc: 0.9597 - 11s/epoch - 59ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2708 - acc: 0.8925 - auc: 0.9525 - val_loss: 0.2489 - val_acc: 0.9069 - val_auc: 0.9621 - 10s/epoch - 55ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2658 - acc: 0.8977 - auc: 0.9548 - val_loss: 0.2474 - val_acc: 0.9160 - val_auc: 0.9605 - 10s/epoch - 54ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2568 - acc: 0.8996 - auc: 0.9572 - val_loss: 0.2549 - val_acc: 0.9145 - val_auc: 0.9582 - 10s/epoch - 54ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2582 - acc: 0.8987 - auc: 0.9575 - val_loss: 0.2579 - val_acc: 0.9130 - val_auc: 0.9578 - 10s/epoch - 54ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2536 - acc: 0.8984 - auc: 0.9600 - val_loss: 0.2438 - val_acc: 0.9084 - val_auc: 0.9622 - 10s/epoch - 54ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2506 - acc: 0.9020 - auc: 0.9599 - val_loss: 0.2518 - val_acc: 0.8977 - val_auc: 0.9593 - 10s/epoch - 54ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2451 - acc: 0.9025 - auc: 0.9622 - val_loss: 0.2495 - val_acc: 0.9069 - val_auc: 0.9613 - 10s/epoch - 54ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2444 - acc: 0.9015 - auc: 0.9627 - val_loss: 0.2440 - val_acc: 0.9115 - val_auc: 0.9617 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2420 - acc: 0.9038 - auc: 0.9633 - val_loss: 0.2409 - val_acc: 0.9145 - val_auc: 0.9633 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2388 - acc: 0.9054 - auc: 0.9635 - val_loss: 0.2331 - val_acc: 0.9130 - val_auc: 0.9663 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2400 - acc: 0.9030 - auc: 0.9646 - val_loss: 0.2302 - val_acc: 0.9115 - val_auc: 0.9676 - 10s/epoch - 56ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2354 - acc: 0.9059 - auc: 0.9654 - val_loss: 0.2473 - val_acc: 0.9084 - val_auc: 0.9606 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2382 - acc: 0.9052 - auc: 0.9641 - val_loss: 0.2293 - val_acc: 0.9160 - val_auc: 0.9674 - 10s/epoch - 54ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2340 - acc: 0.9043 - auc: 0.9661 - val_loss: 0.2309 - val_acc: 0.9099 - val_auc: 0.9664 - 10s/epoch - 55ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2313 - acc: 0.9082 - auc: 0.9664 - val_loss: 0.2340 - val_acc: 0.9069 - val_auc: 0.9661 - 10s/epoch - 54ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2251 - acc: 0.9111 - auc: 0.9681 - val_loss: 0.2317 - val_acc: 0.9099 - val_auc: 0.9656 - 10s/epoch - 55ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2282 - acc: 0.9069 - auc: 0.9680 - val_loss: 0.2255 - val_acc: 0.9115 - val_auc: 0.9692 - 10s/epoch - 54ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2260 - acc: 0.9099 - auc: 0.9682 - val_loss: 0.2274 - val_acc: 0.9130 - val_auc: 0.9690 - 10s/epoch - 54ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2206 - acc: 0.9177 - auc: 0.9689 - val_loss: 0.2570 - val_acc: 0.8977 - val_auc: 0.9629 - 10s/epoch - 55ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2186 - acc: 0.9142 - auc: 0.9700 - val_loss: 0.2221 - val_acc: 0.9084 - val_auc: 0.9706 - 10s/epoch - 55ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2152 - acc: 0.9162 - auc: 0.9712 - val_loss: 0.2364 - val_acc: 0.8947 - val_auc: 0.9672 - 10s/epoch - 55ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2097 - acc: 0.9179 - auc: 0.9725 - val_loss: 0.2275 - val_acc: 0.9084 - val_auc: 0.9676 - 10s/epoch - 54ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2107 - acc: 0.9177 - auc: 0.9718 - val_loss: 0.2314 - val_acc: 0.9038 - val_auc: 0.9683 - 10s/epoch - 55ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2076 - acc: 0.9201 - auc: 0.9733 - val_loss: 0.2392 - val_acc: 0.9053 - val_auc: 0.9646 - 10s/epoch - 54ms/step
Epoch 27/100
185/185 - 10s - loss: 0.1975 - acc: 0.9238 - auc: 0.9751 - val_loss: 0.2406 - val_acc: 0.8977 - val_auc: 0.9654 - 10s/epoch - 54ms/step
Epoch 28/100
185/185 - 10s - loss: 0.1974 - acc: 0.9233 - auc: 0.9760 - val_loss: 0.2328 - val_acc: 0.9008 - val_auc: 0.9691 - 10s/epoch - 55ms/step
Epoch 29/100
185/185 - 10s - loss: 0.1932 - acc: 0.9250 - auc: 0.9766 - val_loss: 0.2300 - val_acc: 0.9176 - val_auc: 0.9681 - 10s/epoch - 54ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1852 - acc: 0.9303 - auc: 0.9782 - val_loss: 0.2389 - val_acc: 0.9069 - val_auc: 0.9673 - 10s/epoch - 54ms/step
Epoch 31/100
185/185 - 10s - loss: 0.1880 - acc: 0.9266 - auc: 0.9775 - val_loss: 0.2198 - val_acc: 0.9084 - val_auc: 0.9707 - 10s/epoch - 55ms/step
Epoch 32/100
185/185 - 10s - loss: 0.1758 - acc: 0.9340 - auc: 0.9810 - val_loss: 0.2342 - val_acc: 0.9099 - val_auc: 0.9681 - 10s/epoch - 54ms/step
Epoch 33/100
185/185 - 10s - loss: 0.1790 - acc: 0.9301 - auc: 0.9798 - val_loss: 0.2410 - val_acc: 0.9038 - val_auc: 0.9647 - 10s/epoch - 54ms/step
Epoch 34/100
185/185 - 10s - loss: 0.1729 - acc: 0.9339 - auc: 0.9812 - val_loss: 0.2481 - val_acc: 0.9038 - val_auc: 0.9645 - 10s/epoch - 54ms/step
Epoch 35/100
185/185 - 10s - loss: 0.1690 - acc: 0.9379 - auc: 0.9818 - val_loss: 0.2427 - val_acc: 0.9115 - val_auc: 0.9674 - 10s/epoch - 54ms/step
Epoch 36/100
185/185 - 10s - loss: 0.1639 - acc: 0.9398 - auc: 0.9828 - val_loss: 0.2534 - val_acc: 0.8992 - val_auc: 0.9659 - 10s/epoch - 54ms/step
Epoch 37/100
185/185 - 10s - loss: 0.1586 - acc: 0.9434 - auc: 0.9831 - val_loss: 0.2732 - val_acc: 0.8977 - val_auc: 0.9620 - 10s/epoch - 54ms/step
Epoch 38/100
185/185 - 10s - loss: 0.1556 - acc: 0.9403 - auc: 0.9848 - val_loss: 0.2613 - val_acc: 0.8947 - val_auc: 0.9595 - 10s/epoch - 55ms/step
Epoch 39/100
185/185 - 10s - loss: 0.1505 - acc: 0.9444 - auc: 0.9850 - val_loss: 0.2535 - val_acc: 0.9023 - val_auc: 0.9596 - 10s/epoch - 54ms/step
Epoch 40/100
185/185 - 10s - loss: 0.1460 - acc: 0.9457 - auc: 0.9866 - val_loss: 0.2560 - val_acc: 0.8977 - val_auc: 0.9658 - 10s/epoch - 55ms/step
Epoch 41/100
185/185 - 10s - loss: 0.1388 - acc: 0.9493 - auc: 0.9871 - val_loss: 0.2599 - val_acc: 0.8992 - val_auc: 0.9619 - 10s/epoch - 54ms/step
Early stopping epoch: 40
******Evaluating TEST set*********
21/21 - 1s - 587ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.89      0.78      0.83       193
           1       0.91      0.96      0.94       462

    accuracy                           0.91       655
   macro avg       0.90      0.87      0.89       655
weighted avg       0.91      0.91      0.91       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.28      0.46      0.35       193
           1       0.69      0.51      0.58       462

    accuracy                           0.49       655
   macro avg       0.49      0.48      0.47       655
weighted avg       0.57      0.49      0.52       655

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 12s - loss: 0.3132 - acc: 0.8731 - auc: 0.9375 - val_loss: 0.2980 - val_acc: 0.8824 - val_auc: 0.9468 - 12s/epoch - 67ms/step
Epoch 2/100
185/185 - 11s - loss: 0.2769 - acc: 0.8892 - auc: 0.9509 - val_loss: 0.2722 - val_acc: 0.8840 - val_auc: 0.9555 - 11s/epoch - 57ms/step
Epoch 3/100
185/185 - 11s - loss: 0.2725 - acc: 0.8918 - auc: 0.9521 - val_loss: 0.2667 - val_acc: 0.9008 - val_auc: 0.9563 - 11s/epoch - 59ms/step
Epoch 4/100
185/185 - 11s - loss: 0.2658 - acc: 0.8967 - auc: 0.9545 - val_loss: 0.2540 - val_acc: 0.8885 - val_auc: 0.9602 - 11s/epoch - 59ms/step
Epoch 5/100
185/185 - 11s - loss: 0.2581 - acc: 0.8993 - auc: 0.9572 - val_loss: 0.2570 - val_acc: 0.8901 - val_auc: 0.9600 - 11s/epoch - 58ms/step
Epoch 6/100
185/185 - 11s - loss: 0.2582 - acc: 0.8998 - auc: 0.9576 - val_loss: 0.2773 - val_acc: 0.8947 - val_auc: 0.9525 - 11s/epoch - 59ms/step
Epoch 7/100
185/185 - 11s - loss: 0.2543 - acc: 0.8996 - auc: 0.9587 - val_loss: 0.2547 - val_acc: 0.8992 - val_auc: 0.9584 - 11s/epoch - 59ms/step
Epoch 8/100
185/185 - 11s - loss: 0.2487 - acc: 0.9023 - auc: 0.9605 - val_loss: 0.2515 - val_acc: 0.9053 - val_auc: 0.9601 - 11s/epoch - 59ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2438 - acc: 0.9062 - auc: 0.9623 - val_loss: 0.2515 - val_acc: 0.8901 - val_auc: 0.9630 - 10s/epoch - 55ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2449 - acc: 0.9009 - auc: 0.9632 - val_loss: 0.2351 - val_acc: 0.8962 - val_auc: 0.9663 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2398 - acc: 0.9065 - auc: 0.9639 - val_loss: 0.2479 - val_acc: 0.8977 - val_auc: 0.9613 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2370 - acc: 0.9081 - auc: 0.9653 - val_loss: 0.2360 - val_acc: 0.8916 - val_auc: 0.9652 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2383 - acc: 0.9088 - auc: 0.9643 - val_loss: 0.2550 - val_acc: 0.8901 - val_auc: 0.9617 - 10s/epoch - 55ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2354 - acc: 0.9077 - auc: 0.9657 - val_loss: 0.2882 - val_acc: 0.8809 - val_auc: 0.9599 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2325 - acc: 0.9050 - auc: 0.9668 - val_loss: 0.2390 - val_acc: 0.9023 - val_auc: 0.9646 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2308 - acc: 0.9089 - auc: 0.9670 - val_loss: 0.2333 - val_acc: 0.8870 - val_auc: 0.9667 - 10s/epoch - 55ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2299 - acc: 0.9086 - auc: 0.9669 - val_loss: 0.2330 - val_acc: 0.9023 - val_auc: 0.9662 - 10s/epoch - 55ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2272 - acc: 0.9116 - auc: 0.9677 - val_loss: 0.2311 - val_acc: 0.9069 - val_auc: 0.9656 - 10s/epoch - 55ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2201 - acc: 0.9127 - auc: 0.9700 - val_loss: 0.2312 - val_acc: 0.9099 - val_auc: 0.9668 - 10s/epoch - 55ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2215 - acc: 0.9132 - auc: 0.9702 - val_loss: 0.2412 - val_acc: 0.9038 - val_auc: 0.9653 - 10s/epoch - 55ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2178 - acc: 0.9149 - auc: 0.9704 - val_loss: 0.2403 - val_acc: 0.8901 - val_auc: 0.9673 - 10s/epoch - 55ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2181 - acc: 0.9169 - auc: 0.9710 - val_loss: 0.2513 - val_acc: 0.8870 - val_auc: 0.9612 - 10s/epoch - 55ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2142 - acc: 0.9166 - auc: 0.9714 - val_loss: 0.2490 - val_acc: 0.8870 - val_auc: 0.9641 - 10s/epoch - 55ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2099 - acc: 0.9160 - auc: 0.9721 - val_loss: 0.2428 - val_acc: 0.8931 - val_auc: 0.9665 - 10s/epoch - 55ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2070 - acc: 0.9201 - auc: 0.9730 - val_loss: 0.2365 - val_acc: 0.9069 - val_auc: 0.9675 - 10s/epoch - 55ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2040 - acc: 0.9206 - auc: 0.9736 - val_loss: 0.2544 - val_acc: 0.8931 - val_auc: 0.9653 - 10s/epoch - 55ms/step
Epoch 27/100
185/185 - 10s - loss: 0.2017 - acc: 0.9162 - auc: 0.9751 - val_loss: 0.2313 - val_acc: 0.9145 - val_auc: 0.9656 - 10s/epoch - 55ms/step
Epoch 28/100
185/185 - 10s - loss: 0.1936 - acc: 0.9237 - auc: 0.9768 - val_loss: 0.2407 - val_acc: 0.9038 - val_auc: 0.9636 - 10s/epoch - 55ms/step
Epoch 29/100
185/185 - 10s - loss: 0.1895 - acc: 0.9279 - auc: 0.9768 - val_loss: 0.2563 - val_acc: 0.8885 - val_auc: 0.9606 - 10s/epoch - 55ms/step
Epoch 30/100
185/185 - 10s - loss: 0.1866 - acc: 0.9283 - auc: 0.9780 - val_loss: 0.2452 - val_acc: 0.9008 - val_auc: 0.9646 - 10s/epoch - 55ms/step
Epoch 31/100
185/185 - 10s - loss: 0.1854 - acc: 0.9300 - auc: 0.9780 - val_loss: 0.2609 - val_acc: 0.8962 - val_auc: 0.9629 - 10s/epoch - 55ms/step
Epoch 32/100
185/185 - 10s - loss: 0.1799 - acc: 0.9308 - auc: 0.9791 - val_loss: 0.2639 - val_acc: 0.8931 - val_auc: 0.9626 - 10s/epoch - 55ms/step
Epoch 33/100
185/185 - 10s - loss: 0.1703 - acc: 0.9355 - auc: 0.9808 - val_loss: 0.2938 - val_acc: 0.8885 - val_auc: 0.9455 - 10s/epoch - 55ms/step
Epoch 34/100
185/185 - 10s - loss: 0.1702 - acc: 0.9337 - auc: 0.9810 - val_loss: 0.2874 - val_acc: 0.8870 - val_auc: 0.9509 - 10s/epoch - 55ms/step
Epoch 35/100
185/185 - 10s - loss: 0.1672 - acc: 0.9342 - auc: 0.9815 - val_loss: 0.2660 - val_acc: 0.8992 - val_auc: 0.9577 - 10s/epoch - 55ms/step
Early stopping epoch: 34
******Evaluating TEST set*********
21/21 - 1s - 579ms/epoch - 28ms/step
              precision    recall  f1-score   support

           0       0.90      0.77      0.83       193
           1       0.91      0.96      0.94       462

    accuracy                           0.91       655
   macro avg       0.90      0.87      0.88       655
weighted avg       0.91      0.91      0.90       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.30      0.54      0.38       193
           1       0.71      0.47      0.57       462

    accuracy                           0.49       655
   macro avg       0.50      0.51      0.48       655
weighted avg       0.59      0.49      0.51       655

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3111 - acc: 0.8692 - auc: 0.9386 - val_loss: 0.3213 - val_acc: 0.8672 - val_auc: 0.9322 - 13s/epoch - 69ms/step
Epoch 2/100
185/185 - 10s - loss: 0.2760 - acc: 0.8943 - auc: 0.9508 - val_loss: 0.3134 - val_acc: 0.8794 - val_auc: 0.9380 - 10s/epoch - 55ms/step
Epoch 3/100
185/185 - 10s - loss: 0.2632 - acc: 0.8974 - auc: 0.9556 - val_loss: 0.3108 - val_acc: 0.8779 - val_auc: 0.9408 - 10s/epoch - 55ms/step
Epoch 4/100
185/185 - 10s - loss: 0.2561 - acc: 0.8965 - auc: 0.9591 - val_loss: 0.3096 - val_acc: 0.8809 - val_auc: 0.9399 - 10s/epoch - 55ms/step
Epoch 5/100
185/185 - 10s - loss: 0.2479 - acc: 0.9016 - auc: 0.9609 - val_loss: 0.3002 - val_acc: 0.8916 - val_auc: 0.9418 - 10s/epoch - 55ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2475 - acc: 0.9026 - auc: 0.9611 - val_loss: 0.3186 - val_acc: 0.8824 - val_auc: 0.9396 - 10s/epoch - 55ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2440 - acc: 0.9047 - auc: 0.9633 - val_loss: 0.3205 - val_acc: 0.8840 - val_auc: 0.9431 - 10s/epoch - 55ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2405 - acc: 0.9042 - auc: 0.9636 - val_loss: 0.2981 - val_acc: 0.8794 - val_auc: 0.9421 - 10s/epoch - 55ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2385 - acc: 0.9072 - auc: 0.9635 - val_loss: 0.3297 - val_acc: 0.8718 - val_auc: 0.9397 - 10s/epoch - 55ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2344 - acc: 0.9079 - auc: 0.9657 - val_loss: 0.2975 - val_acc: 0.8824 - val_auc: 0.9432 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2322 - acc: 0.9096 - auc: 0.9661 - val_loss: 0.2997 - val_acc: 0.8824 - val_auc: 0.9440 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2310 - acc: 0.9038 - auc: 0.9667 - val_loss: 0.2977 - val_acc: 0.8901 - val_auc: 0.9425 - 10s/epoch - 54ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2272 - acc: 0.9125 - auc: 0.9673 - val_loss: 0.3024 - val_acc: 0.8901 - val_auc: 0.9462 - 10s/epoch - 55ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2254 - acc: 0.9123 - auc: 0.9674 - val_loss: 0.3040 - val_acc: 0.8947 - val_auc: 0.9429 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2247 - acc: 0.9118 - auc: 0.9684 - val_loss: 0.3072 - val_acc: 0.8962 - val_auc: 0.9432 - 10s/epoch - 55ms/step
Epoch 16/100
185/185 - 11s - loss: 0.2189 - acc: 0.9127 - auc: 0.9701 - val_loss: 0.2988 - val_acc: 0.8855 - val_auc: 0.9442 - 11s/epoch - 61ms/step
Epoch 17/100
185/185 - 11s - loss: 0.2226 - acc: 0.9123 - auc: 0.9693 - val_loss: 0.2873 - val_acc: 0.8901 - val_auc: 0.9458 - 11s/epoch - 59ms/step
Epoch 18/100
185/185 - 11s - loss: 0.2157 - acc: 0.9138 - auc: 0.9707 - val_loss: 0.3073 - val_acc: 0.8901 - val_auc: 0.9395 - 11s/epoch - 59ms/step
Epoch 19/100
185/185 - 11s - loss: 0.2145 - acc: 0.9171 - auc: 0.9714 - val_loss: 0.3077 - val_acc: 0.8931 - val_auc: 0.9414 - 11s/epoch - 58ms/step
Epoch 20/100
185/185 - 11s - loss: 0.2129 - acc: 0.9164 - auc: 0.9718 - val_loss: 0.3130 - val_acc: 0.8901 - val_auc: 0.9368 - 11s/epoch - 59ms/step
Epoch 21/100
185/185 - 11s - loss: 0.2082 - acc: 0.9203 - auc: 0.9725 - val_loss: 0.3585 - val_acc: 0.8794 - val_auc: 0.9337 - 11s/epoch - 59ms/step
Epoch 22/100
185/185 - 11s - loss: 0.2054 - acc: 0.9208 - auc: 0.9736 - val_loss: 0.3186 - val_acc: 0.8824 - val_auc: 0.9338 - 11s/epoch - 58ms/step
Epoch 23/100
185/185 - 11s - loss: 0.2008 - acc: 0.9233 - auc: 0.9747 - val_loss: 0.3023 - val_acc: 0.8931 - val_auc: 0.9414 - 11s/epoch - 59ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
21/21 - 1s - 617ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.92      0.68      0.79       193
           1       0.88      0.98      0.93       462

    accuracy                           0.89       655
   macro avg       0.90      0.83      0.86       655
weighted avg       0.89      0.89      0.88       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.26      0.46      0.34       193
           1       0.67      0.47      0.55       462

    accuracy                           0.46       655
   macro avg       0.47      0.46      0.44       655
weighted avg       0.55      0.46      0.49       655

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
185/185 - 13s - loss: 0.3178 - acc: 0.8694 - auc: 0.9345 - val_loss: 0.2565 - val_acc: 0.8962 - val_auc: 0.9620 - 13s/epoch - 71ms/step
Epoch 2/100
185/185 - 11s - loss: 0.2801 - acc: 0.8886 - auc: 0.9491 - val_loss: 0.2617 - val_acc: 0.8947 - val_auc: 0.9603 - 11s/epoch - 58ms/step
Epoch 3/100
185/185 - 11s - loss: 0.2750 - acc: 0.8928 - auc: 0.9499 - val_loss: 0.2449 - val_acc: 0.8947 - val_auc: 0.9668 - 11s/epoch - 58ms/step
Epoch 4/100
185/185 - 11s - loss: 0.2655 - acc: 0.8964 - auc: 0.9537 - val_loss: 0.2434 - val_acc: 0.9008 - val_auc: 0.9659 - 11s/epoch - 59ms/step
Epoch 5/100
185/185 - 11s - loss: 0.2570 - acc: 0.8996 - auc: 0.9577 - val_loss: 0.2404 - val_acc: 0.8962 - val_auc: 0.9648 - 11s/epoch - 58ms/step
Epoch 6/100
185/185 - 10s - loss: 0.2592 - acc: 0.8964 - auc: 0.9574 - val_loss: 0.2524 - val_acc: 0.8962 - val_auc: 0.9606 - 10s/epoch - 55ms/step
Epoch 7/100
185/185 - 10s - loss: 0.2523 - acc: 0.8987 - auc: 0.9593 - val_loss: 0.2445 - val_acc: 0.8977 - val_auc: 0.9630 - 10s/epoch - 54ms/step
Epoch 8/100
185/185 - 10s - loss: 0.2497 - acc: 0.9033 - auc: 0.9602 - val_loss: 0.2312 - val_acc: 0.9053 - val_auc: 0.9699 - 10s/epoch - 55ms/step
Epoch 9/100
185/185 - 10s - loss: 0.2453 - acc: 0.9018 - auc: 0.9618 - val_loss: 0.2385 - val_acc: 0.9008 - val_auc: 0.9649 - 10s/epoch - 55ms/step
Epoch 10/100
185/185 - 10s - loss: 0.2416 - acc: 0.9045 - auc: 0.9636 - val_loss: 0.2293 - val_acc: 0.9023 - val_auc: 0.9683 - 10s/epoch - 55ms/step
Epoch 11/100
185/185 - 10s - loss: 0.2410 - acc: 0.9040 - auc: 0.9634 - val_loss: 0.2216 - val_acc: 0.9084 - val_auc: 0.9723 - 10s/epoch - 55ms/step
Epoch 12/100
185/185 - 10s - loss: 0.2409 - acc: 0.9026 - auc: 0.9628 - val_loss: 0.2329 - val_acc: 0.8977 - val_auc: 0.9670 - 10s/epoch - 55ms/step
Epoch 13/100
185/185 - 10s - loss: 0.2368 - acc: 0.9072 - auc: 0.9648 - val_loss: 0.2276 - val_acc: 0.9084 - val_auc: 0.9695 - 10s/epoch - 54ms/step
Epoch 14/100
185/185 - 10s - loss: 0.2359 - acc: 0.9064 - auc: 0.9648 - val_loss: 0.2292 - val_acc: 0.8977 - val_auc: 0.9715 - 10s/epoch - 55ms/step
Epoch 15/100
185/185 - 10s - loss: 0.2337 - acc: 0.9042 - auc: 0.9662 - val_loss: 0.2295 - val_acc: 0.9069 - val_auc: 0.9681 - 10s/epoch - 54ms/step
Epoch 16/100
185/185 - 10s - loss: 0.2309 - acc: 0.9074 - auc: 0.9667 - val_loss: 0.2242 - val_acc: 0.9008 - val_auc: 0.9698 - 10s/epoch - 54ms/step
Epoch 17/100
185/185 - 10s - loss: 0.2305 - acc: 0.9093 - auc: 0.9667 - val_loss: 0.2161 - val_acc: 0.9008 - val_auc: 0.9727 - 10s/epoch - 55ms/step
Epoch 18/100
185/185 - 10s - loss: 0.2283 - acc: 0.9101 - auc: 0.9670 - val_loss: 0.2144 - val_acc: 0.9069 - val_auc: 0.9734 - 10s/epoch - 55ms/step
Epoch 19/100
185/185 - 10s - loss: 0.2236 - acc: 0.9115 - auc: 0.9684 - val_loss: 0.2242 - val_acc: 0.9053 - val_auc: 0.9699 - 10s/epoch - 55ms/step
Epoch 20/100
185/185 - 10s - loss: 0.2268 - acc: 0.9098 - auc: 0.9677 - val_loss: 0.2209 - val_acc: 0.9053 - val_auc: 0.9705 - 10s/epoch - 55ms/step
Epoch 21/100
185/185 - 10s - loss: 0.2163 - acc: 0.9150 - auc: 0.9706 - val_loss: 0.2180 - val_acc: 0.9115 - val_auc: 0.9727 - 10s/epoch - 55ms/step
Epoch 22/100
185/185 - 10s - loss: 0.2172 - acc: 0.9147 - auc: 0.9702 - val_loss: 0.2211 - val_acc: 0.9115 - val_auc: 0.9701 - 10s/epoch - 55ms/step
Epoch 23/100
185/185 - 10s - loss: 0.2150 - acc: 0.9160 - auc: 0.9702 - val_loss: 0.2341 - val_acc: 0.9115 - val_auc: 0.9664 - 10s/epoch - 54ms/step
Epoch 24/100
185/185 - 10s - loss: 0.2144 - acc: 0.9164 - auc: 0.9715 - val_loss: 0.2196 - val_acc: 0.9115 - val_auc: 0.9710 - 10s/epoch - 54ms/step
Epoch 25/100
185/185 - 10s - loss: 0.2077 - acc: 0.9179 - auc: 0.9731 - val_loss: 0.2279 - val_acc: 0.9176 - val_auc: 0.9729 - 10s/epoch - 55ms/step
Epoch 26/100
185/185 - 10s - loss: 0.2061 - acc: 0.9201 - auc: 0.9730 - val_loss: 0.2457 - val_acc: 0.8977 - val_auc: 0.9629 - 10s/epoch - 54ms/step
Epoch 27/100
185/185 - 10s - loss: 0.2046 - acc: 0.9238 - auc: 0.9740 - val_loss: 0.2240 - val_acc: 0.9084 - val_auc: 0.9693 - 10s/epoch - 54ms/step
Epoch 28/100
185/185 - 10s - loss: 0.1958 - acc: 0.9247 - auc: 0.9754 - val_loss: 0.2286 - val_acc: 0.9160 - val_auc: 0.9711 - 10s/epoch - 54ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
21/21 - 1s - 617ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.89      0.78      0.83       194
           1       0.91      0.96      0.94       461

    accuracy                           0.91       655
   macro avg       0.90      0.87      0.88       655
weighted avg       0.91      0.91      0.90       655

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.27      0.49      0.35       194
           1       0.68      0.45      0.54       461

    accuracy                           0.46       655
   macro avg       0.48      0.47      0.45       655
weighted avg       0.56      0.46      0.49       655

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.9080] IC [0.9003, 0.9156]
Mean Recall[0.8695] IC [0.8565, 0.8826]
Mean F1[0.8842] IC [0.8737, 0.8948]
Median Accuracy[0.9069]
Median Recall[0.8686]
Median F1[0.8834]
********************txid511145********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.94      0.74      0.83      2098
           1       0.75      0.94      0.84      1726

    accuracy                           0.83      3824
   macro avg       0.85      0.84      0.83      3824
weighted avg       0.86      0.83      0.83      3824

Predicted   0.0   1.0   All
True                       
0          1558   540  2098
1            95  1631  1726
All        1653  2171  3824
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3316 - acc: 0.8555 - auc: 0.9310 - val_loss: 0.2907 - val_acc: 0.8750 - val_auc: 0.9479 - 17s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2948 - acc: 0.8789 - auc: 0.9442 - val_loss: 0.2841 - val_acc: 0.8782 - val_auc: 0.9511 - 15s/epoch - 55ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2809 - acc: 0.8879 - auc: 0.9494 - val_loss: 0.2765 - val_acc: 0.8929 - val_auc: 0.9529 - 15s/epoch - 55ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2809 - acc: 0.8863 - auc: 0.9497 - val_loss: 0.2793 - val_acc: 0.8845 - val_auc: 0.9528 - 15s/epoch - 55ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2735 - acc: 0.8912 - auc: 0.9522 - val_loss: 0.2792 - val_acc: 0.8834 - val_auc: 0.9527 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2719 - acc: 0.8900 - auc: 0.9531 - val_loss: 0.2929 - val_acc: 0.8761 - val_auc: 0.9468 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2706 - acc: 0.8910 - auc: 0.9537 - val_loss: 0.2768 - val_acc: 0.8897 - val_auc: 0.9523 - 15s/epoch - 55ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2652 - acc: 0.8911 - auc: 0.9558 - val_loss: 0.2764 - val_acc: 0.8866 - val_auc: 0.9525 - 15s/epoch - 55ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2624 - acc: 0.8939 - auc: 0.9568 - val_loss: 0.2786 - val_acc: 0.8908 - val_auc: 0.9530 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2599 - acc: 0.8934 - auc: 0.9577 - val_loss: 0.2844 - val_acc: 0.8750 - val_auc: 0.9516 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2604 - acc: 0.8947 - auc: 0.9577 - val_loss: 0.2736 - val_acc: 0.8866 - val_auc: 0.9561 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2576 - acc: 0.8969 - auc: 0.9590 - val_loss: 0.2779 - val_acc: 0.8855 - val_auc: 0.9537 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2553 - acc: 0.8967 - auc: 0.9601 - val_loss: 0.2704 - val_acc: 0.8887 - val_auc: 0.9557 - 15s/epoch - 56ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2536 - acc: 0.8949 - auc: 0.9603 - val_loss: 0.2769 - val_acc: 0.8929 - val_auc: 0.9530 - 15s/epoch - 56ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2508 - acc: 0.9005 - auc: 0.9612 - val_loss: 0.2698 - val_acc: 0.8866 - val_auc: 0.9560 - 15s/epoch - 56ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2454 - acc: 0.8990 - auc: 0.9629 - val_loss: 0.2834 - val_acc: 0.8803 - val_auc: 0.9531 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2439 - acc: 0.9022 - auc: 0.9633 - val_loss: 0.2928 - val_acc: 0.8782 - val_auc: 0.9489 - 15s/epoch - 56ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2437 - acc: 0.8995 - auc: 0.9634 - val_loss: 0.2790 - val_acc: 0.8813 - val_auc: 0.9526 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2364 - acc: 0.9024 - auc: 0.9655 - val_loss: 0.2672 - val_acc: 0.8897 - val_auc: 0.9569 - 15s/epoch - 56ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2322 - acc: 0.9048 - auc: 0.9669 - val_loss: 0.2924 - val_acc: 0.8813 - val_auc: 0.9517 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2312 - acc: 0.9083 - auc: 0.9667 - val_loss: 0.2726 - val_acc: 0.8897 - val_auc: 0.9544 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2287 - acc: 0.9079 - auc: 0.9678 - val_loss: 0.2825 - val_acc: 0.8897 - val_auc: 0.9511 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2250 - acc: 0.9086 - auc: 0.9691 - val_loss: 0.3027 - val_acc: 0.8792 - val_auc: 0.9475 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2195 - acc: 0.9095 - auc: 0.9706 - val_loss: 0.3033 - val_acc: 0.8824 - val_auc: 0.9477 - 15s/epoch - 56ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2171 - acc: 0.9097 - auc: 0.9716 - val_loss: 0.2909 - val_acc: 0.8929 - val_auc: 0.9504 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2138 - acc: 0.9116 - auc: 0.9722 - val_loss: 0.2979 - val_acc: 0.8876 - val_auc: 0.9473 - 15s/epoch - 56ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2076 - acc: 0.9154 - auc: 0.9735 - val_loss: 0.2820 - val_acc: 0.8834 - val_auc: 0.9537 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2020 - acc: 0.9185 - auc: 0.9749 - val_loss: 0.2940 - val_acc: 0.8866 - val_auc: 0.9480 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.1999 - acc: 0.9166 - auc: 0.9753 - val_loss: 0.3033 - val_acc: 0.8792 - val_auc: 0.9474 - 15s/epoch - 56ms/step
Early stopping epoch: 28
******Evaluating TEST set*********
30/30 - 1s - 789ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.89      0.84      0.86       391
           1       0.89      0.93      0.91       561

    accuracy                           0.89       952
   macro avg       0.89      0.88      0.88       952
weighted avg       0.89      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.47      0.42       391
           1       0.57      0.49      0.52       561

    accuracy                           0.48       952
   macro avg       0.48      0.48      0.47       952
weighted avg       0.49      0.48      0.48       952

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3309 - acc: 0.8551 - auc: 0.9315 - val_loss: 0.3106 - val_acc: 0.8676 - val_auc: 0.9404 - 17s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2949 - acc: 0.8788 - auc: 0.9442 - val_loss: 0.2957 - val_acc: 0.8739 - val_auc: 0.9451 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2815 - acc: 0.8874 - auc: 0.9489 - val_loss: 0.3016 - val_acc: 0.8613 - val_auc: 0.9441 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2800 - acc: 0.8869 - auc: 0.9497 - val_loss: 0.3125 - val_acc: 0.8550 - val_auc: 0.9423 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2739 - acc: 0.8908 - auc: 0.9521 - val_loss: 0.2806 - val_acc: 0.8771 - val_auc: 0.9528 - 15s/epoch - 56ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2698 - acc: 0.8917 - auc: 0.9539 - val_loss: 0.2718 - val_acc: 0.8929 - val_auc: 0.9543 - 15s/epoch - 56ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2678 - acc: 0.8934 - auc: 0.9548 - val_loss: 0.2726 - val_acc: 0.8824 - val_auc: 0.9544 - 15s/epoch - 56ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2625 - acc: 0.8963 - auc: 0.9568 - val_loss: 0.2969 - val_acc: 0.8792 - val_auc: 0.9504 - 15s/epoch - 56ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2680 - acc: 0.8910 - auc: 0.9549 - val_loss: 0.3165 - val_acc: 0.8687 - val_auc: 0.9432 - 15s/epoch - 56ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2612 - acc: 0.8954 - auc: 0.9569 - val_loss: 0.2740 - val_acc: 0.8887 - val_auc: 0.9548 - 15s/epoch - 56ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2586 - acc: 0.8960 - auc: 0.9583 - val_loss: 0.2807 - val_acc: 0.8792 - val_auc: 0.9531 - 15s/epoch - 56ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2582 - acc: 0.8961 - auc: 0.9590 - val_loss: 0.2769 - val_acc: 0.8845 - val_auc: 0.9538 - 15s/epoch - 56ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2521 - acc: 0.8967 - auc: 0.9604 - val_loss: 0.2711 - val_acc: 0.8950 - val_auc: 0.9539 - 15s/epoch - 55ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2504 - acc: 0.8969 - auc: 0.9615 - val_loss: 0.2675 - val_acc: 0.8908 - val_auc: 0.9552 - 15s/epoch - 56ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2462 - acc: 0.9013 - auc: 0.9626 - val_loss: 0.3036 - val_acc: 0.8729 - val_auc: 0.9492 - 15s/epoch - 56ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2453 - acc: 0.9001 - auc: 0.9632 - val_loss: 0.2700 - val_acc: 0.8855 - val_auc: 0.9540 - 15s/epoch - 56ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2437 - acc: 0.9023 - auc: 0.9636 - val_loss: 0.2679 - val_acc: 0.8876 - val_auc: 0.9547 - 15s/epoch - 56ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2406 - acc: 0.9012 - auc: 0.9645 - val_loss: 0.2711 - val_acc: 0.8855 - val_auc: 0.9554 - 15s/epoch - 56ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2347 - acc: 0.9040 - auc: 0.9664 - val_loss: 0.2750 - val_acc: 0.8782 - val_auc: 0.9552 - 15s/epoch - 56ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2344 - acc: 0.9066 - auc: 0.9663 - val_loss: 0.2777 - val_acc: 0.8908 - val_auc: 0.9526 - 15s/epoch - 56ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2350 - acc: 0.9068 - auc: 0.9661 - val_loss: 0.2645 - val_acc: 0.8939 - val_auc: 0.9563 - 15s/epoch - 56ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2254 - acc: 0.9090 - auc: 0.9688 - val_loss: 0.2749 - val_acc: 0.8929 - val_auc: 0.9576 - 15s/epoch - 56ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2256 - acc: 0.9102 - auc: 0.9688 - val_loss: 0.2652 - val_acc: 0.8908 - val_auc: 0.9562 - 15s/epoch - 56ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2216 - acc: 0.9128 - auc: 0.9699 - val_loss: 0.2690 - val_acc: 0.8908 - val_auc: 0.9545 - 15s/epoch - 56ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2157 - acc: 0.9148 - auc: 0.9717 - val_loss: 0.2745 - val_acc: 0.8918 - val_auc: 0.9573 - 15s/epoch - 56ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2143 - acc: 0.9143 - auc: 0.9717 - val_loss: 0.2610 - val_acc: 0.8918 - val_auc: 0.9589 - 15s/epoch - 57ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2050 - acc: 0.9186 - auc: 0.9741 - val_loss: 0.2723 - val_acc: 0.8887 - val_auc: 0.9561 - 15s/epoch - 56ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2016 - acc: 0.9211 - auc: 0.9751 - val_loss: 0.2778 - val_acc: 0.8876 - val_auc: 0.9548 - 15s/epoch - 56ms/step
Epoch 29/100
268/268 - 15s - loss: 0.1971 - acc: 0.9222 - auc: 0.9765 - val_loss: 0.2839 - val_acc: 0.8824 - val_auc: 0.9529 - 15s/epoch - 56ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1924 - acc: 0.9232 - auc: 0.9772 - val_loss: 0.2894 - val_acc: 0.8761 - val_auc: 0.9525 - 15s/epoch - 56ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1846 - acc: 0.9250 - auc: 0.9791 - val_loss: 0.2785 - val_acc: 0.8960 - val_auc: 0.9533 - 15s/epoch - 56ms/step
Epoch 32/100
268/268 - 15s - loss: 0.1775 - acc: 0.9295 - auc: 0.9804 - val_loss: 0.2948 - val_acc: 0.8876 - val_auc: 0.9517 - 15s/epoch - 56ms/step
Epoch 33/100
268/268 - 15s - loss: 0.1756 - acc: 0.9305 - auc: 0.9809 - val_loss: 0.3164 - val_acc: 0.8813 - val_auc: 0.9446 - 15s/epoch - 56ms/step
Epoch 34/100
268/268 - 15s - loss: 0.1672 - acc: 0.9344 - auc: 0.9829 - val_loss: 0.2930 - val_acc: 0.8813 - val_auc: 0.9528 - 15s/epoch - 56ms/step
Epoch 35/100
268/268 - 15s - loss: 0.1581 - acc: 0.9387 - auc: 0.9847 - val_loss: 0.3182 - val_acc: 0.8782 - val_auc: 0.9455 - 15s/epoch - 56ms/step
Epoch 36/100
268/268 - 15s - loss: 0.1547 - acc: 0.9414 - auc: 0.9850 - val_loss: 0.3001 - val_acc: 0.8897 - val_auc: 0.9500 - 15s/epoch - 56ms/step
Early stopping epoch: 35
******Evaluating TEST set*********
30/30 - 1s - 799ms/epoch - 27ms/step
              precision    recall  f1-score   support

           0       0.92      0.81      0.86       392
           1       0.88      0.95      0.91       560

    accuracy                           0.89       952
   macro avg       0.90      0.88      0.89       952
weighted avg       0.89      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.50      0.44       392
           1       0.57      0.47      0.52       560

    accuracy                           0.48       952
   macro avg       0.49      0.49      0.48       952
weighted avg       0.50      0.48      0.49       952

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3302 - acc: 0.8566 - auc: 0.9324 - val_loss: 0.2754 - val_acc: 0.8929 - val_auc: 0.9478 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2954 - acc: 0.8809 - auc: 0.9443 - val_loss: 0.2608 - val_acc: 0.8992 - val_auc: 0.9538 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2868 - acc: 0.8830 - auc: 0.9477 - val_loss: 0.2603 - val_acc: 0.8876 - val_auc: 0.9528 - 15s/epoch - 56ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2825 - acc: 0.8859 - auc: 0.9491 - val_loss: 0.2518 - val_acc: 0.8992 - val_auc: 0.9587 - 15s/epoch - 57ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2755 - acc: 0.8867 - auc: 0.9520 - val_loss: 0.2518 - val_acc: 0.9023 - val_auc: 0.9545 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 14s - loss: 0.2718 - acc: 0.8908 - auc: 0.9533 - val_loss: 0.2438 - val_acc: 0.9002 - val_auc: 0.9594 - 14s/epoch - 54ms/step
Epoch 7/100
268/268 - 14s - loss: 0.2714 - acc: 0.8886 - auc: 0.9539 - val_loss: 0.2501 - val_acc: 0.9128 - val_auc: 0.9590 - 14s/epoch - 54ms/step
Epoch 8/100
268/268 - 14s - loss: 0.2704 - acc: 0.8921 - auc: 0.9540 - val_loss: 0.2412 - val_acc: 0.9044 - val_auc: 0.9617 - 14s/epoch - 54ms/step
Epoch 9/100
268/268 - 14s - loss: 0.2624 - acc: 0.8905 - auc: 0.9579 - val_loss: 0.2413 - val_acc: 0.9055 - val_auc: 0.9612 - 14s/epoch - 53ms/step
Epoch 10/100
268/268 - 14s - loss: 0.2645 - acc: 0.8912 - auc: 0.9568 - val_loss: 0.2448 - val_acc: 0.9097 - val_auc: 0.9606 - 14s/epoch - 53ms/step
Epoch 11/100
268/268 - 14s - loss: 0.2598 - acc: 0.8927 - auc: 0.9580 - val_loss: 0.2460 - val_acc: 0.9013 - val_auc: 0.9602 - 14s/epoch - 53ms/step
Epoch 12/100
268/268 - 14s - loss: 0.2566 - acc: 0.8947 - auc: 0.9593 - val_loss: 0.2365 - val_acc: 0.9023 - val_auc: 0.9644 - 14s/epoch - 53ms/step
Epoch 13/100
268/268 - 14s - loss: 0.2552 - acc: 0.8954 - auc: 0.9594 - val_loss: 0.2392 - val_acc: 0.9065 - val_auc: 0.9651 - 14s/epoch - 53ms/step
Epoch 14/100
268/268 - 14s - loss: 0.2526 - acc: 0.8974 - auc: 0.9607 - val_loss: 0.2313 - val_acc: 0.9055 - val_auc: 0.9667 - 14s/epoch - 53ms/step
Epoch 15/100
268/268 - 14s - loss: 0.2495 - acc: 0.8971 - auc: 0.9614 - val_loss: 0.2294 - val_acc: 0.9044 - val_auc: 0.9664 - 14s/epoch - 53ms/step
Epoch 16/100
268/268 - 14s - loss: 0.2448 - acc: 0.8990 - auc: 0.9631 - val_loss: 0.2352 - val_acc: 0.9107 - val_auc: 0.9649 - 14s/epoch - 53ms/step
Epoch 17/100
268/268 - 14s - loss: 0.2428 - acc: 0.9010 - auc: 0.9637 - val_loss: 0.2380 - val_acc: 0.9076 - val_auc: 0.9637 - 14s/epoch - 54ms/step
Epoch 18/100
268/268 - 14s - loss: 0.2413 - acc: 0.9019 - auc: 0.9638 - val_loss: 0.2365 - val_acc: 0.9034 - val_auc: 0.9651 - 14s/epoch - 54ms/step
Epoch 19/100
268/268 - 14s - loss: 0.2408 - acc: 0.9025 - auc: 0.9647 - val_loss: 0.2308 - val_acc: 0.9076 - val_auc: 0.9670 - 14s/epoch - 53ms/step
Epoch 20/100
268/268 - 14s - loss: 0.2336 - acc: 0.9036 - auc: 0.9663 - val_loss: 0.2413 - val_acc: 0.9002 - val_auc: 0.9633 - 14s/epoch - 54ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2294 - acc: 0.9051 - auc: 0.9678 - val_loss: 0.2416 - val_acc: 0.9086 - val_auc: 0.9647 - 15s/epoch - 54ms/step
Epoch 22/100
268/268 - 14s - loss: 0.2250 - acc: 0.9088 - auc: 0.9693 - val_loss: 0.2433 - val_acc: 0.9023 - val_auc: 0.9626 - 14s/epoch - 54ms/step
Epoch 23/100
268/268 - 14s - loss: 0.2202 - acc: 0.9094 - auc: 0.9707 - val_loss: 0.2499 - val_acc: 0.9065 - val_auc: 0.9597 - 14s/epoch - 53ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2189 - acc: 0.9100 - auc: 0.9709 - val_loss: 0.2365 - val_acc: 0.9065 - val_auc: 0.9653 - 15s/epoch - 54ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2170 - acc: 0.9106 - auc: 0.9711 - val_loss: 0.2407 - val_acc: 0.9044 - val_auc: 0.9614 - 15s/epoch - 55ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2116 - acc: 0.9147 - auc: 0.9729 - val_loss: 0.2455 - val_acc: 0.9044 - val_auc: 0.9625 - 15s/epoch - 54ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2036 - acc: 0.9186 - auc: 0.9749 - val_loss: 0.2413 - val_acc: 0.9044 - val_auc: 0.9648 - 15s/epoch - 54ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2019 - acc: 0.9168 - auc: 0.9753 - val_loss: 0.2510 - val_acc: 0.9034 - val_auc: 0.9641 - 15s/epoch - 55ms/step
Epoch 29/100
268/268 - 15s - loss: 0.1945 - acc: 0.9228 - auc: 0.9766 - val_loss: 0.2621 - val_acc: 0.9065 - val_auc: 0.9563 - 15s/epoch - 54ms/step
Early stopping epoch: 28
******Evaluating TEST set*********
30/30 - 1s - 714ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.90      0.87      0.89       392
           1       0.91      0.94      0.92       560

    accuracy                           0.91       952
   macro avg       0.91      0.90      0.90       952
weighted avg       0.91      0.91      0.91       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.52      0.46       392
           1       0.60      0.50      0.55       560

    accuracy                           0.51       952
   macro avg       0.51      0.51      0.50       952
weighted avg       0.52      0.51      0.51       952

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3174 - acc: 0.8641 - auc: 0.9369 - val_loss: 0.3187 - val_acc: 0.8834 - val_auc: 0.9335 - 17s/epoch - 63ms/step
Epoch 2/100
268/268 - 14s - loss: 0.2899 - acc: 0.8834 - auc: 0.9469 - val_loss: 0.3171 - val_acc: 0.8761 - val_auc: 0.9357 - 14s/epoch - 54ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2795 - acc: 0.8846 - auc: 0.9506 - val_loss: 0.3284 - val_acc: 0.8761 - val_auc: 0.9370 - 15s/epoch - 54ms/step
Epoch 4/100
268/268 - 14s - loss: 0.2729 - acc: 0.8880 - auc: 0.9528 - val_loss: 0.3108 - val_acc: 0.8803 - val_auc: 0.9351 - 14s/epoch - 53ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2683 - acc: 0.8901 - auc: 0.9545 - val_loss: 0.3100 - val_acc: 0.8908 - val_auc: 0.9385 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2678 - acc: 0.8931 - auc: 0.9548 - val_loss: 0.2983 - val_acc: 0.8887 - val_auc: 0.9419 - 15s/epoch - 54ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2634 - acc: 0.8943 - auc: 0.9568 - val_loss: 0.3075 - val_acc: 0.8782 - val_auc: 0.9378 - 15s/epoch - 54ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2643 - acc: 0.8926 - auc: 0.9562 - val_loss: 0.2954 - val_acc: 0.8897 - val_auc: 0.9427 - 15s/epoch - 54ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2609 - acc: 0.8946 - auc: 0.9578 - val_loss: 0.3010 - val_acc: 0.8845 - val_auc: 0.9427 - 15s/epoch - 54ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2573 - acc: 0.8973 - auc: 0.9587 - val_loss: 0.2917 - val_acc: 0.8929 - val_auc: 0.9462 - 15s/epoch - 55ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2573 - acc: 0.8961 - auc: 0.9591 - val_loss: 0.2955 - val_acc: 0.8981 - val_auc: 0.9458 - 15s/epoch - 54ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2518 - acc: 0.8989 - auc: 0.9610 - val_loss: 0.2910 - val_acc: 0.8855 - val_auc: 0.9446 - 15s/epoch - 54ms/step
Epoch 13/100
268/268 - 14s - loss: 0.2530 - acc: 0.8985 - auc: 0.9599 - val_loss: 0.3055 - val_acc: 0.8771 - val_auc: 0.9414 - 14s/epoch - 54ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2516 - acc: 0.8987 - auc: 0.9608 - val_loss: 0.2854 - val_acc: 0.8897 - val_auc: 0.9480 - 15s/epoch - 55ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2481 - acc: 0.8977 - auc: 0.9623 - val_loss: 0.3016 - val_acc: 0.8813 - val_auc: 0.9446 - 15s/epoch - 54ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2459 - acc: 0.9022 - auc: 0.9625 - val_loss: 0.2911 - val_acc: 0.8876 - val_auc: 0.9462 - 15s/epoch - 54ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2475 - acc: 0.8976 - auc: 0.9625 - val_loss: 0.3086 - val_acc: 0.8792 - val_auc: 0.9394 - 15s/epoch - 54ms/step
Epoch 18/100
268/268 - 15s - loss: 0.2419 - acc: 0.9025 - auc: 0.9644 - val_loss: 0.2973 - val_acc: 0.8866 - val_auc: 0.9459 - 15s/epoch - 55ms/step
Epoch 19/100
268/268 - 15s - loss: 0.2391 - acc: 0.9023 - auc: 0.9647 - val_loss: 0.3054 - val_acc: 0.8824 - val_auc: 0.9418 - 15s/epoch - 54ms/step
Epoch 20/100
268/268 - 15s - loss: 0.2341 - acc: 0.9072 - auc: 0.9666 - val_loss: 0.2916 - val_acc: 0.8908 - val_auc: 0.9488 - 15s/epoch - 55ms/step
Epoch 21/100
268/268 - 15s - loss: 0.2318 - acc: 0.9050 - auc: 0.9670 - val_loss: 0.2919 - val_acc: 0.8792 - val_auc: 0.9469 - 15s/epoch - 54ms/step
Epoch 22/100
268/268 - 15s - loss: 0.2288 - acc: 0.9071 - auc: 0.9681 - val_loss: 0.3016 - val_acc: 0.8771 - val_auc: 0.9440 - 15s/epoch - 54ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2277 - acc: 0.9082 - auc: 0.9685 - val_loss: 0.3020 - val_acc: 0.8813 - val_auc: 0.9438 - 15s/epoch - 54ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2233 - acc: 0.9113 - auc: 0.9692 - val_loss: 0.2992 - val_acc: 0.8782 - val_auc: 0.9446 - 15s/epoch - 55ms/step
Epoch 25/100
268/268 - 14s - loss: 0.2185 - acc: 0.9140 - auc: 0.9710 - val_loss: 0.2986 - val_acc: 0.8908 - val_auc: 0.9464 - 14s/epoch - 54ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2151 - acc: 0.9115 - auc: 0.9722 - val_loss: 0.3436 - val_acc: 0.8771 - val_auc: 0.9346 - 15s/epoch - 55ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2129 - acc: 0.9147 - auc: 0.9720 - val_loss: 0.3140 - val_acc: 0.8750 - val_auc: 0.9456 - 15s/epoch - 55ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2078 - acc: 0.9180 - auc: 0.9739 - val_loss: 0.3083 - val_acc: 0.8866 - val_auc: 0.9445 - 15s/epoch - 55ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2020 - acc: 0.9194 - auc: 0.9754 - val_loss: 0.3183 - val_acc: 0.8803 - val_auc: 0.9447 - 15s/epoch - 54ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1990 - acc: 0.9194 - auc: 0.9763 - val_loss: 0.3065 - val_acc: 0.8834 - val_auc: 0.9456 - 15s/epoch - 55ms/step
Early stopping epoch: 29
******Evaluating TEST set*********
30/30 - 1s - 717ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.92      0.81      0.86       392
           1       0.88      0.95      0.91       560

    accuracy                           0.89       952
   macro avg       0.90      0.88      0.88       952
weighted avg       0.89      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.49      0.45       392
           1       0.60      0.52      0.56       560

    accuracy                           0.51       952
   macro avg       0.51      0.51      0.51       952
weighted avg       0.52      0.51      0.51       952

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 18s - loss: 0.3359 - acc: 0.8515 - auc: 0.9292 - val_loss: 0.2602 - val_acc: 0.8897 - val_auc: 0.9572 - 18s/epoch - 69ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2967 - acc: 0.8794 - auc: 0.9439 - val_loss: 0.2733 - val_acc: 0.8782 - val_auc: 0.9554 - 15s/epoch - 56ms/step
Epoch 3/100
268/268 - 16s - loss: 0.2930 - acc: 0.8796 - auc: 0.9458 - val_loss: 0.2498 - val_acc: 0.8992 - val_auc: 0.9623 - 16s/epoch - 60ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2793 - acc: 0.8870 - auc: 0.9504 - val_loss: 0.2699 - val_acc: 0.8824 - val_auc: 0.9548 - 15s/epoch - 56ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2803 - acc: 0.8867 - auc: 0.9502 - val_loss: 0.2525 - val_acc: 0.8939 - val_auc: 0.9613 - 15s/epoch - 55ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2733 - acc: 0.8887 - auc: 0.9531 - val_loss: 0.2388 - val_acc: 0.9055 - val_auc: 0.9651 - 15s/epoch - 54ms/step
Epoch 7/100
268/268 - 14s - loss: 0.2724 - acc: 0.8897 - auc: 0.9536 - val_loss: 0.2454 - val_acc: 0.9076 - val_auc: 0.9654 - 14s/epoch - 54ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2712 - acc: 0.8881 - auc: 0.9542 - val_loss: 0.2501 - val_acc: 0.9002 - val_auc: 0.9606 - 15s/epoch - 54ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2646 - acc: 0.8924 - auc: 0.9561 - val_loss: 0.2368 - val_acc: 0.9002 - val_auc: 0.9646 - 15s/epoch - 54ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2672 - acc: 0.8940 - auc: 0.9551 - val_loss: 0.2422 - val_acc: 0.9013 - val_auc: 0.9626 - 15s/epoch - 54ms/step
Epoch 11/100
268/268 - 14s - loss: 0.2632 - acc: 0.8931 - auc: 0.9567 - val_loss: 0.2564 - val_acc: 0.8897 - val_auc: 0.9599 - 14s/epoch - 54ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2603 - acc: 0.8939 - auc: 0.9580 - val_loss: 0.2650 - val_acc: 0.9002 - val_auc: 0.9566 - 15s/epoch - 54ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2626 - acc: 0.8940 - auc: 0.9574 - val_loss: 0.2369 - val_acc: 0.9013 - val_auc: 0.9642 - 15s/epoch - 54ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2582 - acc: 0.8931 - auc: 0.9591 - val_loss: 0.2428 - val_acc: 0.9065 - val_auc: 0.9645 - 15s/epoch - 54ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2529 - acc: 0.8964 - auc: 0.9603 - val_loss: 0.2430 - val_acc: 0.9023 - val_auc: 0.9618 - 15s/epoch - 54ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2508 - acc: 0.8966 - auc: 0.9616 - val_loss: 0.2365 - val_acc: 0.9055 - val_auc: 0.9641 - 15s/epoch - 54ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2489 - acc: 0.8995 - auc: 0.9618 - val_loss: 0.2421 - val_acc: 0.8992 - val_auc: 0.9621 - 15s/epoch - 54ms/step
Early stopping epoch: 16
******Evaluating TEST set*********
30/30 - 1s - 753ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.90      0.88      0.89       392
           1       0.91      0.93      0.92       560

    accuracy                           0.91       952
   macro avg       0.91      0.90      0.90       952
weighted avg       0.91      0.91      0.91       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.45      0.41       392
           1       0.55      0.47      0.51       560

    accuracy                           0.46       952
   macro avg       0.46      0.46      0.46       952
weighted avg       0.48      0.46      0.47       952

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3286 - acc: 0.8593 - auc: 0.9323 - val_loss: 0.3072 - val_acc: 0.8739 - val_auc: 0.9417 - 17s/epoch - 64ms/step
Epoch 2/100
268/268 - 16s - loss: 0.2922 - acc: 0.8827 - auc: 0.9449 - val_loss: 0.3036 - val_acc: 0.8782 - val_auc: 0.9432 - 16s/epoch - 59ms/step
Epoch 3/100
268/268 - 14s - loss: 0.2855 - acc: 0.8845 - auc: 0.9484 - val_loss: 0.2887 - val_acc: 0.8792 - val_auc: 0.9469 - 14s/epoch - 54ms/step
Epoch 4/100
268/268 - 14s - loss: 0.2795 - acc: 0.8878 - auc: 0.9502 - val_loss: 0.2809 - val_acc: 0.8855 - val_auc: 0.9494 - 14s/epoch - 54ms/step
Epoch 5/100
268/268 - 14s - loss: 0.2752 - acc: 0.8876 - auc: 0.9521 - val_loss: 0.2751 - val_acc: 0.8876 - val_auc: 0.9533 - 14s/epoch - 54ms/step
Epoch 6/100
268/268 - 16s - loss: 0.2700 - acc: 0.8908 - auc: 0.9544 - val_loss: 0.2719 - val_acc: 0.8897 - val_auc: 0.9548 - 16s/epoch - 60ms/step
Epoch 7/100
268/268 - 16s - loss: 0.2673 - acc: 0.8933 - auc: 0.9551 - val_loss: 0.2717 - val_acc: 0.8866 - val_auc: 0.9549 - 16s/epoch - 60ms/step
Epoch 8/100
268/268 - 16s - loss: 0.2653 - acc: 0.8939 - auc: 0.9554 - val_loss: 0.2813 - val_acc: 0.8824 - val_auc: 0.9522 - 16s/epoch - 58ms/step
Epoch 9/100
268/268 - 16s - loss: 0.2644 - acc: 0.8924 - auc: 0.9565 - val_loss: 0.2762 - val_acc: 0.8908 - val_auc: 0.9535 - 16s/epoch - 58ms/step
Epoch 10/100
268/268 - 16s - loss: 0.2611 - acc: 0.8938 - auc: 0.9576 - val_loss: 0.2790 - val_acc: 0.8918 - val_auc: 0.9534 - 16s/epoch - 58ms/step
Epoch 11/100
268/268 - 16s - loss: 0.2596 - acc: 0.8945 - auc: 0.9576 - val_loss: 0.2726 - val_acc: 0.8950 - val_auc: 0.9551 - 16s/epoch - 58ms/step
Epoch 12/100
268/268 - 14s - loss: 0.2557 - acc: 0.8963 - auc: 0.9594 - val_loss: 0.2798 - val_acc: 0.8887 - val_auc: 0.9538 - 14s/epoch - 54ms/step
Epoch 13/100
268/268 - 14s - loss: 0.2511 - acc: 0.8970 - auc: 0.9607 - val_loss: 0.2849 - val_acc: 0.8834 - val_auc: 0.9525 - 14s/epoch - 53ms/step
Epoch 14/100
268/268 - 14s - loss: 0.2512 - acc: 0.8955 - auc: 0.9614 - val_loss: 0.2693 - val_acc: 0.8897 - val_auc: 0.9558 - 14s/epoch - 53ms/step
Epoch 15/100
268/268 - 14s - loss: 0.2468 - acc: 0.9001 - auc: 0.9624 - val_loss: 0.2789 - val_acc: 0.8876 - val_auc: 0.9542 - 14s/epoch - 53ms/step
Epoch 16/100
268/268 - 14s - loss: 0.2427 - acc: 0.9011 - auc: 0.9639 - val_loss: 0.3004 - val_acc: 0.8761 - val_auc: 0.9472 - 14s/epoch - 53ms/step
Epoch 17/100
268/268 - 14s - loss: 0.2425 - acc: 0.9016 - auc: 0.9639 - val_loss: 0.2814 - val_acc: 0.8855 - val_auc: 0.9543 - 14s/epoch - 53ms/step
Epoch 18/100
268/268 - 14s - loss: 0.2391 - acc: 0.9023 - auc: 0.9649 - val_loss: 0.2754 - val_acc: 0.8897 - val_auc: 0.9545 - 14s/epoch - 53ms/step
Epoch 19/100
268/268 - 14s - loss: 0.2341 - acc: 0.9054 - auc: 0.9666 - val_loss: 0.2835 - val_acc: 0.8771 - val_auc: 0.9507 - 14s/epoch - 53ms/step
Epoch 20/100
268/268 - 14s - loss: 0.2312 - acc: 0.9061 - auc: 0.9671 - val_loss: 0.2824 - val_acc: 0.8855 - val_auc: 0.9550 - 14s/epoch - 53ms/step
Epoch 21/100
268/268 - 14s - loss: 0.2278 - acc: 0.9071 - auc: 0.9683 - val_loss: 0.2809 - val_acc: 0.8782 - val_auc: 0.9528 - 14s/epoch - 53ms/step
Epoch 22/100
268/268 - 14s - loss: 0.2232 - acc: 0.9099 - auc: 0.9696 - val_loss: 0.2884 - val_acc: 0.8887 - val_auc: 0.9541 - 14s/epoch - 53ms/step
Epoch 23/100
268/268 - 14s - loss: 0.2158 - acc: 0.9127 - auc: 0.9716 - val_loss: 0.2815 - val_acc: 0.8887 - val_auc: 0.9537 - 14s/epoch - 53ms/step
Epoch 24/100
268/268 - 14s - loss: 0.2110 - acc: 0.9150 - auc: 0.9730 - val_loss: 0.3008 - val_acc: 0.8792 - val_auc: 0.9483 - 14s/epoch - 53ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
30/30 - 1s - 706ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.88      0.85      0.86       392
           1       0.90      0.92      0.91       560

    accuracy                           0.89       952
   macro avg       0.89      0.88      0.89       952
weighted avg       0.89      0.89      0.89       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.52      0.47       392
           1       0.60      0.50      0.55       560

    accuracy                           0.51       952
   macro avg       0.51      0.51      0.51       952
weighted avg       0.53      0.51      0.51       952

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3258 - acc: 0.8600 - auc: 0.9336 - val_loss: 0.3111 - val_acc: 0.8750 - val_auc: 0.9388 - 17s/epoch - 63ms/step
Epoch 2/100
268/268 - 14s - loss: 0.2926 - acc: 0.8792 - auc: 0.9455 - val_loss: 0.2975 - val_acc: 0.8718 - val_auc: 0.9445 - 14s/epoch - 54ms/step
Epoch 3/100
268/268 - 16s - loss: 0.2870 - acc: 0.8831 - auc: 0.9478 - val_loss: 0.2903 - val_acc: 0.8739 - val_auc: 0.9468 - 16s/epoch - 60ms/step
Epoch 4/100
268/268 - 16s - loss: 0.2804 - acc: 0.8873 - auc: 0.9502 - val_loss: 0.2826 - val_acc: 0.8876 - val_auc: 0.9504 - 16s/epoch - 60ms/step
Epoch 5/100
268/268 - 16s - loss: 0.2755 - acc: 0.8913 - auc: 0.9520 - val_loss: 0.2805 - val_acc: 0.8813 - val_auc: 0.9521 - 16s/epoch - 60ms/step
Epoch 6/100
268/268 - 16s - loss: 0.2721 - acc: 0.8927 - auc: 0.9529 - val_loss: 0.2741 - val_acc: 0.8897 - val_auc: 0.9533 - 16s/epoch - 60ms/step
Epoch 7/100
268/268 - 16s - loss: 0.2685 - acc: 0.8910 - auc: 0.9546 - val_loss: 0.2728 - val_acc: 0.8845 - val_auc: 0.9543 - 16s/epoch - 60ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2686 - acc: 0.8904 - auc: 0.9548 - val_loss: 0.2819 - val_acc: 0.8845 - val_auc: 0.9515 - 15s/epoch - 54ms/step
Epoch 9/100
268/268 - 14s - loss: 0.2657 - acc: 0.8931 - auc: 0.9557 - val_loss: 0.2774 - val_acc: 0.8792 - val_auc: 0.9514 - 14s/epoch - 54ms/step
Epoch 10/100
268/268 - 14s - loss: 0.2596 - acc: 0.8939 - auc: 0.9580 - val_loss: 0.2673 - val_acc: 0.8824 - val_auc: 0.9555 - 14s/epoch - 54ms/step
Epoch 11/100
268/268 - 14s - loss: 0.2581 - acc: 0.8956 - auc: 0.9585 - val_loss: 0.2687 - val_acc: 0.8824 - val_auc: 0.9564 - 14s/epoch - 54ms/step
Epoch 12/100
268/268 - 14s - loss: 0.2573 - acc: 0.8971 - auc: 0.9587 - val_loss: 0.2762 - val_acc: 0.8855 - val_auc: 0.9546 - 14s/epoch - 54ms/step
Epoch 13/100
268/268 - 14s - loss: 0.2516 - acc: 0.8987 - auc: 0.9604 - val_loss: 0.2913 - val_acc: 0.8676 - val_auc: 0.9492 - 14s/epoch - 54ms/step
Epoch 14/100
268/268 - 14s - loss: 0.2499 - acc: 0.8975 - auc: 0.9614 - val_loss: 0.2872 - val_acc: 0.8813 - val_auc: 0.9510 - 14s/epoch - 54ms/step
Epoch 15/100
268/268 - 14s - loss: 0.2482 - acc: 0.9003 - auc: 0.9622 - val_loss: 0.2773 - val_acc: 0.8929 - val_auc: 0.9551 - 14s/epoch - 54ms/step
Epoch 16/100
268/268 - 14s - loss: 0.2485 - acc: 0.9002 - auc: 0.9622 - val_loss: 0.2771 - val_acc: 0.8845 - val_auc: 0.9551 - 14s/epoch - 54ms/step
Epoch 17/100
268/268 - 14s - loss: 0.2440 - acc: 0.9026 - auc: 0.9631 - val_loss: 0.2827 - val_acc: 0.8918 - val_auc: 0.9503 - 14s/epoch - 54ms/step
Epoch 18/100
268/268 - 14s - loss: 0.2409 - acc: 0.9047 - auc: 0.9643 - val_loss: 0.2689 - val_acc: 0.8929 - val_auc: 0.9555 - 14s/epoch - 54ms/step
Epoch 19/100
268/268 - 14s - loss: 0.2373 - acc: 0.9022 - auc: 0.9656 - val_loss: 0.2786 - val_acc: 0.8876 - val_auc: 0.9508 - 14s/epoch - 54ms/step
Epoch 20/100
268/268 - 14s - loss: 0.2355 - acc: 0.9065 - auc: 0.9659 - val_loss: 0.2736 - val_acc: 0.8887 - val_auc: 0.9531 - 14s/epoch - 54ms/step
Epoch 21/100
268/268 - 14s - loss: 0.2336 - acc: 0.9036 - auc: 0.9668 - val_loss: 0.2841 - val_acc: 0.8824 - val_auc: 0.9531 - 14s/epoch - 54ms/step
Early stopping epoch: 20
******Evaluating TEST set*********
30/30 - 1s - 714ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.87      0.84      0.85       392
           1       0.89      0.91      0.90       560

    accuracy                           0.88       952
   macro avg       0.88      0.88      0.88       952
weighted avg       0.88      0.88      0.88       952

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.50      0.44       392
           1       0.56      0.45      0.50       560

    accuracy                           0.47       952
   macro avg       0.47      0.47      0.47       952
weighted avg       0.49      0.47      0.47       952

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 17s - loss: 0.3262 - acc: 0.8565 - auc: 0.9339 - val_loss: 0.2962 - val_acc: 0.8843 - val_auc: 0.9435 - 17s/epoch - 62ms/step
Epoch 2/100
268/268 - 14s - loss: 0.2932 - acc: 0.8814 - auc: 0.9454 - val_loss: 0.2884 - val_acc: 0.8906 - val_auc: 0.9467 - 14s/epoch - 54ms/step
Epoch 3/100
268/268 - 14s - loss: 0.2844 - acc: 0.8829 - auc: 0.9488 - val_loss: 0.3045 - val_acc: 0.8749 - val_auc: 0.9423 - 14s/epoch - 53ms/step
Epoch 4/100
268/268 - 14s - loss: 0.2821 - acc: 0.8854 - auc: 0.9496 - val_loss: 0.2780 - val_acc: 0.8948 - val_auc: 0.9514 - 14s/epoch - 54ms/step
Epoch 5/100
268/268 - 14s - loss: 0.2728 - acc: 0.8894 - auc: 0.9524 - val_loss: 0.2771 - val_acc: 0.8885 - val_auc: 0.9527 - 14s/epoch - 54ms/step
Epoch 6/100
268/268 - 15s - loss: 0.2728 - acc: 0.8884 - auc: 0.9526 - val_loss: 0.2835 - val_acc: 0.8864 - val_auc: 0.9510 - 15s/epoch - 54ms/step
Epoch 7/100
268/268 - 16s - loss: 0.2688 - acc: 0.8922 - auc: 0.9545 - val_loss: 0.2719 - val_acc: 0.8970 - val_auc: 0.9543 - 16s/epoch - 60ms/step
Epoch 8/100
268/268 - 16s - loss: 0.2655 - acc: 0.8904 - auc: 0.9557 - val_loss: 0.2730 - val_acc: 0.8948 - val_auc: 0.9543 - 16s/epoch - 60ms/step
Epoch 9/100
268/268 - 16s - loss: 0.2664 - acc: 0.8919 - auc: 0.9550 - val_loss: 0.2690 - val_acc: 0.8906 - val_auc: 0.9560 - 16s/epoch - 60ms/step
Epoch 10/100
268/268 - 16s - loss: 0.2641 - acc: 0.8946 - auc: 0.9559 - val_loss: 0.2810 - val_acc: 0.8854 - val_auc: 0.9519 - 16s/epoch - 60ms/step
Epoch 11/100
268/268 - 16s - loss: 0.2585 - acc: 0.8939 - auc: 0.9586 - val_loss: 0.2706 - val_acc: 0.8970 - val_auc: 0.9553 - 16s/epoch - 60ms/step
Epoch 12/100
268/268 - 16s - loss: 0.2617 - acc: 0.8913 - auc: 0.9570 - val_loss: 0.2697 - val_acc: 0.8970 - val_auc: 0.9561 - 16s/epoch - 60ms/step
Epoch 13/100
268/268 - 16s - loss: 0.2570 - acc: 0.8933 - auc: 0.9589 - val_loss: 0.2701 - val_acc: 0.8948 - val_auc: 0.9553 - 16s/epoch - 60ms/step
Epoch 14/100
268/268 - 16s - loss: 0.2538 - acc: 0.8948 - auc: 0.9607 - val_loss: 0.2669 - val_acc: 0.8991 - val_auc: 0.9571 - 16s/epoch - 61ms/step
Epoch 15/100
268/268 - 16s - loss: 0.2526 - acc: 0.8975 - auc: 0.9608 - val_loss: 0.2637 - val_acc: 0.8959 - val_auc: 0.9584 - 16s/epoch - 60ms/step
Epoch 16/100
268/268 - 16s - loss: 0.2486 - acc: 0.8973 - auc: 0.9619 - val_loss: 0.2700 - val_acc: 0.8896 - val_auc: 0.9557 - 16s/epoch - 59ms/step
Epoch 17/100
268/268 - 16s - loss: 0.2463 - acc: 0.8991 - auc: 0.9628 - val_loss: 0.2711 - val_acc: 0.9012 - val_auc: 0.9573 - 16s/epoch - 60ms/step
Epoch 18/100
268/268 - 16s - loss: 0.2460 - acc: 0.8993 - auc: 0.9631 - val_loss: 0.2743 - val_acc: 0.8906 - val_auc: 0.9563 - 16s/epoch - 60ms/step
Epoch 19/100
268/268 - 16s - loss: 0.2422 - acc: 0.8980 - auc: 0.9638 - val_loss: 0.2747 - val_acc: 0.8927 - val_auc: 0.9559 - 16s/epoch - 60ms/step
Epoch 20/100
268/268 - 16s - loss: 0.2390 - acc: 0.9003 - auc: 0.9651 - val_loss: 0.2786 - val_acc: 0.8885 - val_auc: 0.9539 - 16s/epoch - 60ms/step
Epoch 21/100
268/268 - 16s - loss: 0.2378 - acc: 0.9007 - auc: 0.9652 - val_loss: 0.2698 - val_acc: 0.8959 - val_auc: 0.9566 - 16s/epoch - 59ms/step
Epoch 22/100
268/268 - 16s - loss: 0.2341 - acc: 0.9037 - auc: 0.9669 - val_loss: 0.2687 - val_acc: 0.8927 - val_auc: 0.9577 - 16s/epoch - 60ms/step
Epoch 23/100
268/268 - 16s - loss: 0.2301 - acc: 0.9070 - auc: 0.9680 - val_loss: 0.2767 - val_acc: 0.8896 - val_auc: 0.9534 - 16s/epoch - 60ms/step
Epoch 24/100
268/268 - 16s - loss: 0.2243 - acc: 0.9070 - auc: 0.9700 - val_loss: 0.2816 - val_acc: 0.8875 - val_auc: 0.9524 - 16s/epoch - 60ms/step
Epoch 25/100
268/268 - 16s - loss: 0.2197 - acc: 0.9114 - auc: 0.9708 - val_loss: 0.2895 - val_acc: 0.8749 - val_auc: 0.9511 - 16s/epoch - 60ms/step
Early stopping epoch: 24
******Evaluating TEST set*********
30/30 - 1s - 771ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.90      0.84      0.87       391
           1       0.89      0.93      0.91       560

    accuracy                           0.90       951
   macro avg       0.90      0.89      0.89       951
weighted avg       0.90      0.90      0.90       951

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.48      0.44       391
           1       0.59      0.51      0.55       560

    accuracy                           0.50       951
   macro avg       0.50      0.50      0.49       951
weighted avg       0.51      0.50      0.50       951

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 19s - loss: 0.3292 - acc: 0.8552 - auc: 0.9323 - val_loss: 0.3084 - val_acc: 0.8696 - val_auc: 0.9422 - 19s/epoch - 71ms/step
Epoch 2/100
268/268 - 16s - loss: 0.2903 - acc: 0.8815 - auc: 0.9456 - val_loss: 0.2840 - val_acc: 0.8854 - val_auc: 0.9525 - 16s/epoch - 61ms/step
Epoch 3/100
268/268 - 16s - loss: 0.2854 - acc: 0.8859 - auc: 0.9474 - val_loss: 0.3048 - val_acc: 0.8654 - val_auc: 0.9449 - 16s/epoch - 60ms/step
Epoch 4/100
268/268 - 16s - loss: 0.2760 - acc: 0.8899 - auc: 0.9511 - val_loss: 0.2775 - val_acc: 0.8864 - val_auc: 0.9543 - 16s/epoch - 60ms/step
Epoch 5/100
268/268 - 16s - loss: 0.2744 - acc: 0.8877 - auc: 0.9525 - val_loss: 0.2842 - val_acc: 0.8780 - val_auc: 0.9521 - 16s/epoch - 61ms/step
Epoch 6/100
268/268 - 16s - loss: 0.2677 - acc: 0.8926 - auc: 0.9543 - val_loss: 0.2750 - val_acc: 0.8980 - val_auc: 0.9559 - 16s/epoch - 61ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2681 - acc: 0.8910 - auc: 0.9546 - val_loss: 0.2803 - val_acc: 0.8927 - val_auc: 0.9550 - 15s/epoch - 55ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2666 - acc: 0.8941 - auc: 0.9555 - val_loss: 0.2752 - val_acc: 0.8812 - val_auc: 0.9555 - 15s/epoch - 55ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2638 - acc: 0.8955 - auc: 0.9560 - val_loss: 0.2779 - val_acc: 0.8749 - val_auc: 0.9549 - 15s/epoch - 55ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2615 - acc: 0.8954 - auc: 0.9567 - val_loss: 0.2725 - val_acc: 0.8812 - val_auc: 0.9575 - 15s/epoch - 55ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2583 - acc: 0.8980 - auc: 0.9577 - val_loss: 0.2633 - val_acc: 0.8917 - val_auc: 0.9595 - 15s/epoch - 54ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2579 - acc: 0.8966 - auc: 0.9583 - val_loss: 0.2868 - val_acc: 0.8717 - val_auc: 0.9527 - 15s/epoch - 55ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2555 - acc: 0.8998 - auc: 0.9589 - val_loss: 0.2669 - val_acc: 0.8906 - val_auc: 0.9597 - 15s/epoch - 55ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2539 - acc: 0.8997 - auc: 0.9599 - val_loss: 0.2564 - val_acc: 0.8980 - val_auc: 0.9620 - 15s/epoch - 55ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2507 - acc: 0.9000 - auc: 0.9607 - val_loss: 0.2549 - val_acc: 0.8927 - val_auc: 0.9623 - 15s/epoch - 55ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2479 - acc: 0.8994 - auc: 0.9619 - val_loss: 0.2531 - val_acc: 0.8938 - val_auc: 0.9626 - 15s/epoch - 54ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2470 - acc: 0.9005 - auc: 0.9622 - val_loss: 0.2545 - val_acc: 0.8906 - val_auc: 0.9628 - 15s/epoch - 54ms/step
Epoch 18/100
268/268 - 14s - loss: 0.2444 - acc: 0.9026 - auc: 0.9635 - val_loss: 0.2600 - val_acc: 0.8875 - val_auc: 0.9610 - 14s/epoch - 54ms/step
Epoch 19/100
268/268 - 14s - loss: 0.2432 - acc: 0.9021 - auc: 0.9637 - val_loss: 0.2653 - val_acc: 0.8833 - val_auc: 0.9603 - 14s/epoch - 54ms/step
Epoch 20/100
268/268 - 14s - loss: 0.2421 - acc: 0.9046 - auc: 0.9638 - val_loss: 0.2558 - val_acc: 0.9012 - val_auc: 0.9622 - 14s/epoch - 54ms/step
Epoch 21/100
268/268 - 14s - loss: 0.2375 - acc: 0.9039 - auc: 0.9654 - val_loss: 0.2545 - val_acc: 0.8885 - val_auc: 0.9633 - 14s/epoch - 54ms/step
Epoch 22/100
268/268 - 14s - loss: 0.2333 - acc: 0.9072 - auc: 0.9664 - val_loss: 0.3164 - val_acc: 0.8738 - val_auc: 0.9474 - 14s/epoch - 54ms/step
Epoch 23/100
268/268 - 14s - loss: 0.2301 - acc: 0.9085 - auc: 0.9673 - val_loss: 0.2864 - val_acc: 0.8801 - val_auc: 0.9566 - 14s/epoch - 54ms/step
Epoch 24/100
268/268 - 14s - loss: 0.2267 - acc: 0.9079 - auc: 0.9688 - val_loss: 0.2554 - val_acc: 0.8927 - val_auc: 0.9613 - 14s/epoch - 53ms/step
Epoch 25/100
268/268 - 14s - loss: 0.2289 - acc: 0.9073 - auc: 0.9681 - val_loss: 0.2819 - val_acc: 0.8822 - val_auc: 0.9564 - 14s/epoch - 53ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2196 - acc: 0.9137 - auc: 0.9709 - val_loss: 0.2668 - val_acc: 0.8854 - val_auc: 0.9587 - 15s/epoch - 57ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2170 - acc: 0.9136 - auc: 0.9713 - val_loss: 0.2841 - val_acc: 0.8864 - val_auc: 0.9558 - 15s/epoch - 55ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2152 - acc: 0.9114 - auc: 0.9721 - val_loss: 0.2754 - val_acc: 0.8833 - val_auc: 0.9570 - 15s/epoch - 57ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2093 - acc: 0.9143 - auc: 0.9738 - val_loss: 0.2715 - val_acc: 0.8948 - val_auc: 0.9587 - 15s/epoch - 58ms/step
Epoch 30/100
268/268 - 15s - loss: 0.2007 - acc: 0.9171 - auc: 0.9755 - val_loss: 0.2736 - val_acc: 0.8822 - val_auc: 0.9585 - 15s/epoch - 57ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1984 - acc: 0.9211 - auc: 0.9763 - val_loss: 0.2911 - val_acc: 0.8864 - val_auc: 0.9521 - 15s/epoch - 57ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
30/30 - 1s - 757ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.87      0.86      0.86       391
           1       0.90      0.91      0.91       560

    accuracy                           0.89       951
   macro avg       0.89      0.88      0.88       951
weighted avg       0.89      0.89      0.89       951

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.45      0.41       391
           1       0.56      0.49      0.52       560

    accuracy                           0.47       951
   macro avg       0.47      0.47      0.47       951
weighted avg       0.49      0.47      0.48       951

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
268/268 - 18s - loss: 0.3284 - acc: 0.8594 - auc: 0.9326 - val_loss: 0.2802 - val_acc: 0.8896 - val_auc: 0.9505 - 18s/epoch - 65ms/step
Epoch 2/100
268/268 - 15s - loss: 0.2950 - acc: 0.8798 - auc: 0.9449 - val_loss: 0.2747 - val_acc: 0.8885 - val_auc: 0.9519 - 15s/epoch - 57ms/step
Epoch 3/100
268/268 - 15s - loss: 0.2837 - acc: 0.8864 - auc: 0.9485 - val_loss: 0.2738 - val_acc: 0.8843 - val_auc: 0.9528 - 15s/epoch - 57ms/step
Epoch 4/100
268/268 - 15s - loss: 0.2794 - acc: 0.8884 - auc: 0.9504 - val_loss: 0.2637 - val_acc: 0.8938 - val_auc: 0.9558 - 15s/epoch - 57ms/step
Epoch 5/100
268/268 - 15s - loss: 0.2747 - acc: 0.8900 - auc: 0.9523 - val_loss: 0.2621 - val_acc: 0.8917 - val_auc: 0.9576 - 15s/epoch - 57ms/step
Epoch 6/100
268/268 - 14s - loss: 0.2721 - acc: 0.8899 - auc: 0.9534 - val_loss: 0.2629 - val_acc: 0.8938 - val_auc: 0.9563 - 14s/epoch - 53ms/step
Epoch 7/100
268/268 - 15s - loss: 0.2702 - acc: 0.8927 - auc: 0.9541 - val_loss: 0.2797 - val_acc: 0.8875 - val_auc: 0.9508 - 15s/epoch - 57ms/step
Epoch 8/100
268/268 - 15s - loss: 0.2718 - acc: 0.8910 - auc: 0.9538 - val_loss: 0.2716 - val_acc: 0.8927 - val_auc: 0.9553 - 15s/epoch - 57ms/step
Epoch 9/100
268/268 - 15s - loss: 0.2667 - acc: 0.8917 - auc: 0.9558 - val_loss: 0.2605 - val_acc: 0.8991 - val_auc: 0.9583 - 15s/epoch - 57ms/step
Epoch 10/100
268/268 - 15s - loss: 0.2632 - acc: 0.8926 - auc: 0.9571 - val_loss: 0.2581 - val_acc: 0.8959 - val_auc: 0.9588 - 15s/epoch - 57ms/step
Epoch 11/100
268/268 - 15s - loss: 0.2617 - acc: 0.8941 - auc: 0.9574 - val_loss: 0.2526 - val_acc: 0.9001 - val_auc: 0.9613 - 15s/epoch - 57ms/step
Epoch 12/100
268/268 - 15s - loss: 0.2599 - acc: 0.8931 - auc: 0.9581 - val_loss: 0.2531 - val_acc: 0.8991 - val_auc: 0.9616 - 15s/epoch - 57ms/step
Epoch 13/100
268/268 - 15s - loss: 0.2545 - acc: 0.8976 - auc: 0.9601 - val_loss: 0.2563 - val_acc: 0.9022 - val_auc: 0.9620 - 15s/epoch - 57ms/step
Epoch 14/100
268/268 - 15s - loss: 0.2544 - acc: 0.8946 - auc: 0.9600 - val_loss: 0.2479 - val_acc: 0.9012 - val_auc: 0.9628 - 15s/epoch - 57ms/step
Epoch 15/100
268/268 - 15s - loss: 0.2523 - acc: 0.8983 - auc: 0.9609 - val_loss: 0.2545 - val_acc: 0.9022 - val_auc: 0.9626 - 15s/epoch - 57ms/step
Epoch 16/100
268/268 - 15s - loss: 0.2503 - acc: 0.8996 - auc: 0.9612 - val_loss: 0.2514 - val_acc: 0.9022 - val_auc: 0.9615 - 15s/epoch - 57ms/step
Epoch 17/100
268/268 - 15s - loss: 0.2465 - acc: 0.9004 - auc: 0.9624 - val_loss: 0.2447 - val_acc: 0.9022 - val_auc: 0.9633 - 15s/epoch - 58ms/step
Epoch 18/100
268/268 - 14s - loss: 0.2436 - acc: 0.9025 - auc: 0.9640 - val_loss: 0.2488 - val_acc: 0.8959 - val_auc: 0.9616 - 14s/epoch - 53ms/step
Epoch 19/100
268/268 - 14s - loss: 0.2402 - acc: 0.9026 - auc: 0.9642 - val_loss: 0.2491 - val_acc: 0.9012 - val_auc: 0.9624 - 14s/epoch - 53ms/step
Epoch 20/100
268/268 - 14s - loss: 0.2421 - acc: 0.9014 - auc: 0.9642 - val_loss: 0.2686 - val_acc: 0.8917 - val_auc: 0.9585 - 14s/epoch - 53ms/step
Epoch 21/100
268/268 - 14s - loss: 0.2363 - acc: 0.9043 - auc: 0.9661 - val_loss: 0.2419 - val_acc: 0.9043 - val_auc: 0.9643 - 14s/epoch - 53ms/step
Epoch 22/100
268/268 - 14s - loss: 0.2314 - acc: 0.9070 - auc: 0.9671 - val_loss: 0.2410 - val_acc: 0.9096 - val_auc: 0.9646 - 14s/epoch - 54ms/step
Epoch 23/100
268/268 - 15s - loss: 0.2273 - acc: 0.9086 - auc: 0.9684 - val_loss: 0.2374 - val_acc: 0.9064 - val_auc: 0.9651 - 15s/epoch - 57ms/step
Epoch 24/100
268/268 - 15s - loss: 0.2289 - acc: 0.9052 - auc: 0.9687 - val_loss: 0.2493 - val_acc: 0.9054 - val_auc: 0.9640 - 15s/epoch - 57ms/step
Epoch 25/100
268/268 - 15s - loss: 0.2212 - acc: 0.9099 - auc: 0.9704 - val_loss: 0.2430 - val_acc: 0.9054 - val_auc: 0.9630 - 15s/epoch - 57ms/step
Epoch 26/100
268/268 - 15s - loss: 0.2162 - acc: 0.9113 - auc: 0.9717 - val_loss: 0.2447 - val_acc: 0.9054 - val_auc: 0.9626 - 15s/epoch - 57ms/step
Epoch 27/100
268/268 - 15s - loss: 0.2147 - acc: 0.9126 - auc: 0.9720 - val_loss: 0.2545 - val_acc: 0.8959 - val_auc: 0.9619 - 15s/epoch - 57ms/step
Epoch 28/100
268/268 - 15s - loss: 0.2063 - acc: 0.9173 - auc: 0.9739 - val_loss: 0.2628 - val_acc: 0.9033 - val_auc: 0.9623 - 15s/epoch - 57ms/step
Epoch 29/100
268/268 - 15s - loss: 0.2051 - acc: 0.9138 - auc: 0.9745 - val_loss: 0.2353 - val_acc: 0.9127 - val_auc: 0.9663 - 15s/epoch - 57ms/step
Epoch 30/100
268/268 - 15s - loss: 0.1984 - acc: 0.9168 - auc: 0.9762 - val_loss: 0.2665 - val_acc: 0.8980 - val_auc: 0.9559 - 15s/epoch - 57ms/step
Epoch 31/100
268/268 - 15s - loss: 0.1921 - acc: 0.9221 - auc: 0.9776 - val_loss: 0.2539 - val_acc: 0.9127 - val_auc: 0.9621 - 15s/epoch - 57ms/step
Epoch 32/100
268/268 - 15s - loss: 0.1863 - acc: 0.9268 - auc: 0.9788 - val_loss: 0.2610 - val_acc: 0.9085 - val_auc: 0.9583 - 15s/epoch - 57ms/step
Epoch 33/100
268/268 - 15s - loss: 0.1761 - acc: 0.9297 - auc: 0.9812 - val_loss: 0.2688 - val_acc: 0.9012 - val_auc: 0.9527 - 15s/epoch - 57ms/step
Epoch 34/100
268/268 - 15s - loss: 0.1741 - acc: 0.9304 - auc: 0.9812 - val_loss: 0.3094 - val_acc: 0.8843 - val_auc: 0.9421 - 15s/epoch - 57ms/step
Epoch 35/100
268/268 - 15s - loss: 0.1709 - acc: 0.9316 - auc: 0.9817 - val_loss: 0.2932 - val_acc: 0.8959 - val_auc: 0.9534 - 15s/epoch - 57ms/step
Epoch 36/100
268/268 - 15s - loss: 0.1627 - acc: 0.9349 - auc: 0.9837 - val_loss: 0.3036 - val_acc: 0.9001 - val_auc: 0.9517 - 15s/epoch - 57ms/step
Epoch 37/100
268/268 - 15s - loss: 0.1602 - acc: 0.9370 - auc: 0.9843 - val_loss: 0.2806 - val_acc: 0.9064 - val_auc: 0.9571 - 15s/epoch - 57ms/step
Epoch 38/100
268/268 - 15s - loss: 0.1549 - acc: 0.9380 - auc: 0.9853 - val_loss: 0.2801 - val_acc: 0.9022 - val_auc: 0.9586 - 15s/epoch - 57ms/step
Epoch 39/100
268/268 - 15s - loss: 0.1446 - acc: 0.9442 - auc: 0.9868 - val_loss: 0.3024 - val_acc: 0.8970 - val_auc: 0.9489 - 15s/epoch - 57ms/step
Early stopping epoch: 38
******Evaluating TEST set*********
30/30 - 1s - 860ms/epoch - 29ms/step
              precision    recall  f1-score   support

           0       0.92      0.86      0.89       391
           1       0.91      0.95      0.93       560

    accuracy                           0.91       951
   macro avg       0.91      0.91      0.91       951
weighted avg       0.91      0.91      0.91       951

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.52      0.46       391
           1       0.59      0.48      0.53       560

    accuracy                           0.50       951
   macro avg       0.50      0.50      0.50       951
weighted avg       0.52      0.50      0.50       951

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8957] IC [0.8898, 0.9015]
Mean Recall[0.8879] IC [0.8816, 0.8943]
Mean F1[0.8912] IC [0.8851, 0.8974]
Median Accuracy[0.8913]
Median Recall[0.8835]
Median F1[0.8858]
********************txid85962********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.88      0.75      0.81       114
           1       0.96      0.98      0.97       744

    accuracy                           0.95       858
   macro avg       0.92      0.87      0.89       858
weighted avg       0.95      0.95      0.95       858

Predicted  0.0  1.0  All
True                    
0           86   28  114
1           12  732  744
All         98  760  858
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 18s - loss: 0.3178 - acc: 0.8659 - auc: 0.9367 - val_loss: 0.2757 - val_acc: 0.8847 - val_auc: 0.9560 - 18s/epoch - 66ms/step
Epoch 2/100
264/264 - 14s - loss: 0.2819 - acc: 0.8858 - auc: 0.9482 - val_loss: 0.2580 - val_acc: 0.8965 - val_auc: 0.9602 - 14s/epoch - 54ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2733 - acc: 0.8891 - auc: 0.9518 - val_loss: 0.2710 - val_acc: 0.8901 - val_auc: 0.9567 - 15s/epoch - 55ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2743 - acc: 0.8889 - auc: 0.9520 - val_loss: 0.2537 - val_acc: 0.8965 - val_auc: 0.9632 - 15s/epoch - 56ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2655 - acc: 0.8931 - auc: 0.9558 - val_loss: 0.2528 - val_acc: 0.8954 - val_auc: 0.9632 - 15s/epoch - 56ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2622 - acc: 0.8953 - auc: 0.9567 - val_loss: 0.2552 - val_acc: 0.8879 - val_auc: 0.9643 - 15s/epoch - 56ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2596 - acc: 0.8938 - auc: 0.9576 - val_loss: 0.2577 - val_acc: 0.8901 - val_auc: 0.9631 - 15s/epoch - 56ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2591 - acc: 0.8946 - auc: 0.9577 - val_loss: 0.2570 - val_acc: 0.8879 - val_auc: 0.9645 - 15s/epoch - 56ms/step
Epoch 9/100
264/264 - 15s - loss: 0.2532 - acc: 0.8989 - auc: 0.9599 - val_loss: 0.2413 - val_acc: 0.9007 - val_auc: 0.9683 - 15s/epoch - 56ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2506 - acc: 0.8986 - auc: 0.9609 - val_loss: 0.2463 - val_acc: 0.8954 - val_auc: 0.9647 - 15s/epoch - 56ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2506 - acc: 0.9000 - auc: 0.9607 - val_loss: 0.2362 - val_acc: 0.9061 - val_auc: 0.9680 - 15s/epoch - 57ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2467 - acc: 0.9006 - auc: 0.9620 - val_loss: 0.2356 - val_acc: 0.9050 - val_auc: 0.9686 - 15s/epoch - 57ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2438 - acc: 0.9024 - auc: 0.9626 - val_loss: 0.2364 - val_acc: 0.8997 - val_auc: 0.9683 - 15s/epoch - 56ms/step
Epoch 14/100
264/264 - 15s - loss: 0.2411 - acc: 0.9059 - auc: 0.9639 - val_loss: 0.2359 - val_acc: 0.9029 - val_auc: 0.9677 - 15s/epoch - 57ms/step
Epoch 15/100
264/264 - 15s - loss: 0.2400 - acc: 0.9048 - auc: 0.9648 - val_loss: 0.2371 - val_acc: 0.8943 - val_auc: 0.9683 - 15s/epoch - 57ms/step
Epoch 16/100
264/264 - 15s - loss: 0.2363 - acc: 0.9063 - auc: 0.9652 - val_loss: 0.2453 - val_acc: 0.8943 - val_auc: 0.9649 - 15s/epoch - 57ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2341 - acc: 0.9077 - auc: 0.9665 - val_loss: 0.2412 - val_acc: 0.8965 - val_auc: 0.9668 - 15s/epoch - 56ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2299 - acc: 0.9070 - auc: 0.9675 - val_loss: 0.2339 - val_acc: 0.9018 - val_auc: 0.9668 - 15s/epoch - 56ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2294 - acc: 0.9064 - auc: 0.9677 - val_loss: 0.2732 - val_acc: 0.8975 - val_auc: 0.9621 - 15s/epoch - 57ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2232 - acc: 0.9097 - auc: 0.9693 - val_loss: 0.2506 - val_acc: 0.9007 - val_auc: 0.9666 - 15s/epoch - 57ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2187 - acc: 0.9120 - auc: 0.9706 - val_loss: 0.2528 - val_acc: 0.9018 - val_auc: 0.9627 - 15s/epoch - 57ms/step
Epoch 22/100
264/264 - 15s - loss: 0.2191 - acc: 0.9101 - auc: 0.9706 - val_loss: 0.2597 - val_acc: 0.8954 - val_auc: 0.9598 - 15s/epoch - 57ms/step
Early stopping epoch: 21
******Evaluating TEST set*********
30/30 - 1s - 715ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.92      0.85      0.88       390
           1       0.90      0.95      0.92       547

    accuracy                           0.91       937
   macro avg       0.91      0.90      0.90       937
weighted avg       0.91      0.91      0.90       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.47      0.43       390
           1       0.57      0.49      0.53       547

    accuracy                           0.48       937
   macro avg       0.48      0.48      0.48       937
weighted avg       0.50      0.48      0.49       937

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3213 - acc: 0.8626 - auc: 0.9354 - val_loss: 0.2383 - val_acc: 0.9018 - val_auc: 0.9636 - 17s/epoch - 65ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2865 - acc: 0.8828 - auc: 0.9482 - val_loss: 0.2144 - val_acc: 0.9221 - val_auc: 0.9686 - 15s/epoch - 57ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2771 - acc: 0.8845 - auc: 0.9517 - val_loss: 0.2121 - val_acc: 0.9221 - val_auc: 0.9699 - 15s/epoch - 57ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2754 - acc: 0.8852 - auc: 0.9527 - val_loss: 0.2137 - val_acc: 0.9210 - val_auc: 0.9692 - 15s/epoch - 57ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2709 - acc: 0.8904 - auc: 0.9538 - val_loss: 0.2280 - val_acc: 0.9168 - val_auc: 0.9682 - 15s/epoch - 57ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2676 - acc: 0.8916 - auc: 0.9554 - val_loss: 0.2205 - val_acc: 0.9200 - val_auc: 0.9690 - 15s/epoch - 57ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2630 - acc: 0.8922 - auc: 0.9572 - val_loss: 0.2176 - val_acc: 0.9157 - val_auc: 0.9692 - 15s/epoch - 57ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2596 - acc: 0.8947 - auc: 0.9590 - val_loss: 0.2166 - val_acc: 0.9242 - val_auc: 0.9719 - 15s/epoch - 57ms/step
Epoch 9/100
264/264 - 15s - loss: 0.2593 - acc: 0.8929 - auc: 0.9587 - val_loss: 0.2032 - val_acc: 0.9221 - val_auc: 0.9741 - 15s/epoch - 57ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2561 - acc: 0.8916 - auc: 0.9595 - val_loss: 0.2104 - val_acc: 0.9210 - val_auc: 0.9702 - 15s/epoch - 57ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2546 - acc: 0.8923 - auc: 0.9607 - val_loss: 0.2212 - val_acc: 0.9114 - val_auc: 0.9679 - 15s/epoch - 57ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2501 - acc: 0.8974 - auc: 0.9619 - val_loss: 0.2086 - val_acc: 0.9178 - val_auc: 0.9725 - 15s/epoch - 57ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2485 - acc: 0.8988 - auc: 0.9616 - val_loss: 0.2042 - val_acc: 0.9200 - val_auc: 0.9723 - 15s/epoch - 57ms/step
Epoch 14/100
264/264 - 15s - loss: 0.2474 - acc: 0.8973 - auc: 0.9629 - val_loss: 0.2144 - val_acc: 0.9157 - val_auc: 0.9716 - 15s/epoch - 57ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2423 - acc: 0.9004 - auc: 0.9642 - val_loss: 0.2065 - val_acc: 0.9210 - val_auc: 0.9714 - 14s/epoch - 52ms/step
Epoch 16/100
264/264 - 15s - loss: 0.2424 - acc: 0.9014 - auc: 0.9639 - val_loss: 0.2091 - val_acc: 0.9242 - val_auc: 0.9705 - 15s/epoch - 57ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2404 - acc: 0.9019 - auc: 0.9647 - val_loss: 0.1995 - val_acc: 0.9232 - val_auc: 0.9734 - 15s/epoch - 57ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2356 - acc: 0.9008 - auc: 0.9664 - val_loss: 0.2088 - val_acc: 0.9146 - val_auc: 0.9727 - 15s/epoch - 57ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2324 - acc: 0.9011 - auc: 0.9674 - val_loss: 0.2219 - val_acc: 0.9114 - val_auc: 0.9688 - 15s/epoch - 57ms/step
Early stopping epoch: 18
******Evaluating TEST set*********
30/30 - 1s - 777ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.90      0.91      0.91       390
           1       0.94      0.93      0.93       547

    accuracy                           0.92       937
   macro avg       0.92      0.92      0.92       937
weighted avg       0.92      0.92      0.92       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.51      0.45       390
           1       0.58      0.48      0.52       547

    accuracy                           0.49       937
   macro avg       0.49      0.49      0.49       937
weighted avg       0.51      0.49      0.49       937

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3098 - acc: 0.8680 - auc: 0.9408 - val_loss: 0.3339 - val_acc: 0.8634 - val_auc: 0.9300 - 17s/epoch - 66ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2817 - acc: 0.8859 - auc: 0.9499 - val_loss: 0.3256 - val_acc: 0.8709 - val_auc: 0.9313 - 15s/epoch - 57ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2689 - acc: 0.8938 - auc: 0.9541 - val_loss: 0.3345 - val_acc: 0.8741 - val_auc: 0.9348 - 15s/epoch - 57ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2648 - acc: 0.8916 - auc: 0.9556 - val_loss: 0.3088 - val_acc: 0.8773 - val_auc: 0.9391 - 15s/epoch - 57ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2602 - acc: 0.8944 - auc: 0.9571 - val_loss: 0.3132 - val_acc: 0.8709 - val_auc: 0.9390 - 15s/epoch - 57ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2589 - acc: 0.8944 - auc: 0.9581 - val_loss: 0.3145 - val_acc: 0.8730 - val_auc: 0.9407 - 15s/epoch - 57ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2558 - acc: 0.8967 - auc: 0.9588 - val_loss: 0.3098 - val_acc: 0.8773 - val_auc: 0.9422 - 15s/epoch - 56ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2534 - acc: 0.8985 - auc: 0.9598 - val_loss: 0.3156 - val_acc: 0.8762 - val_auc: 0.9399 - 15s/epoch - 56ms/step
Epoch 9/100
264/264 - 15s - loss: 0.2503 - acc: 0.8988 - auc: 0.9604 - val_loss: 0.3122 - val_acc: 0.8687 - val_auc: 0.9400 - 15s/epoch - 55ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2517 - acc: 0.8988 - auc: 0.9604 - val_loss: 0.3340 - val_acc: 0.8677 - val_auc: 0.9348 - 15s/epoch - 55ms/step
Epoch 11/100
264/264 - 14s - loss: 0.2498 - acc: 0.8976 - auc: 0.9611 - val_loss: 0.3076 - val_acc: 0.8762 - val_auc: 0.9415 - 14s/epoch - 54ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2455 - acc: 0.9014 - auc: 0.9627 - val_loss: 0.3099 - val_acc: 0.8773 - val_auc: 0.9408 - 15s/epoch - 56ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2433 - acc: 0.9030 - auc: 0.9632 - val_loss: 0.3033 - val_acc: 0.8794 - val_auc: 0.9453 - 15s/epoch - 56ms/step
Epoch 14/100
264/264 - 15s - loss: 0.2394 - acc: 0.9031 - auc: 0.9644 - val_loss: 0.3277 - val_acc: 0.8805 - val_auc: 0.9422 - 15s/epoch - 55ms/step
Epoch 15/100
264/264 - 15s - loss: 0.2379 - acc: 0.9046 - auc: 0.9653 - val_loss: 0.3012 - val_acc: 0.8826 - val_auc: 0.9437 - 15s/epoch - 56ms/step
Epoch 16/100
264/264 - 15s - loss: 0.2346 - acc: 0.9052 - auc: 0.9656 - val_loss: 0.3016 - val_acc: 0.8773 - val_auc: 0.9442 - 15s/epoch - 56ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2329 - acc: 0.9055 - auc: 0.9665 - val_loss: 0.2973 - val_acc: 0.8762 - val_auc: 0.9455 - 15s/epoch - 57ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2328 - acc: 0.9058 - auc: 0.9669 - val_loss: 0.3152 - val_acc: 0.8751 - val_auc: 0.9444 - 15s/epoch - 56ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2287 - acc: 0.9072 - auc: 0.9672 - val_loss: 0.3037 - val_acc: 0.8762 - val_auc: 0.9437 - 15s/epoch - 56ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2230 - acc: 0.9104 - auc: 0.9688 - val_loss: 0.3203 - val_acc: 0.8826 - val_auc: 0.9437 - 15s/epoch - 56ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2214 - acc: 0.9109 - auc: 0.9695 - val_loss: 0.3033 - val_acc: 0.8805 - val_auc: 0.9464 - 15s/epoch - 56ms/step
Epoch 22/100
264/264 - 15s - loss: 0.2183 - acc: 0.9088 - auc: 0.9707 - val_loss: 0.3126 - val_acc: 0.8837 - val_auc: 0.9451 - 15s/epoch - 56ms/step
Epoch 23/100
264/264 - 15s - loss: 0.2133 - acc: 0.9121 - auc: 0.9724 - val_loss: 0.3142 - val_acc: 0.8794 - val_auc: 0.9410 - 15s/epoch - 56ms/step
Epoch 24/100
264/264 - 15s - loss: 0.2122 - acc: 0.9150 - auc: 0.9729 - val_loss: 0.3064 - val_acc: 0.8837 - val_auc: 0.9473 - 15s/epoch - 56ms/step
Epoch 25/100
264/264 - 15s - loss: 0.2069 - acc: 0.9174 - auc: 0.9742 - val_loss: 0.3131 - val_acc: 0.8805 - val_auc: 0.9458 - 15s/epoch - 56ms/step
Epoch 26/100
264/264 - 15s - loss: 0.2023 - acc: 0.9160 - auc: 0.9751 - val_loss: 0.3226 - val_acc: 0.8815 - val_auc: 0.9388 - 15s/epoch - 57ms/step
Epoch 27/100
264/264 - 15s - loss: 0.1995 - acc: 0.9188 - auc: 0.9756 - val_loss: 0.3295 - val_acc: 0.8858 - val_auc: 0.9417 - 15s/epoch - 57ms/step
Epoch 28/100
264/264 - 15s - loss: 0.1944 - acc: 0.9203 - auc: 0.9774 - val_loss: 0.3367 - val_acc: 0.8794 - val_auc: 0.9432 - 15s/epoch - 57ms/step
Epoch 29/100
264/264 - 15s - loss: 0.1875 - acc: 0.9254 - auc: 0.9793 - val_loss: 0.3218 - val_acc: 0.8741 - val_auc: 0.9411 - 15s/epoch - 57ms/step
Epoch 30/100
264/264 - 15s - loss: 0.1829 - acc: 0.9267 - auc: 0.9797 - val_loss: 0.3316 - val_acc: 0.8751 - val_auc: 0.9381 - 15s/epoch - 56ms/step
Epoch 31/100
264/264 - 15s - loss: 0.1762 - acc: 0.9297 - auc: 0.9812 - val_loss: 0.3551 - val_acc: 0.8698 - val_auc: 0.9353 - 15s/epoch - 56ms/step
Epoch 32/100
264/264 - 15s - loss: 0.1720 - acc: 0.9320 - auc: 0.9821 - val_loss: 0.3605 - val_acc: 0.8719 - val_auc: 0.9318 - 15s/epoch - 56ms/step
Epoch 33/100
264/264 - 15s - loss: 0.1619 - acc: 0.9364 - auc: 0.9838 - val_loss: 0.4103 - val_acc: 0.8698 - val_auc: 0.9247 - 15s/epoch - 57ms/step
Epoch 34/100
264/264 - 15s - loss: 0.1546 - acc: 0.9369 - auc: 0.9857 - val_loss: 0.3711 - val_acc: 0.8517 - val_auc: 0.9336 - 15s/epoch - 57ms/step
Early stopping epoch: 33
******Evaluating TEST set*********
30/30 - 1s - 741ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.86      0.85      0.86       390
           1       0.90      0.90      0.90       547

    accuracy                           0.88       937
   macro avg       0.88      0.88      0.88       937
weighted avg       0.88      0.88      0.88       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.43      0.52      0.47       390
           1       0.59      0.50      0.55       547

    accuracy                           0.51       937
   macro avg       0.51      0.51      0.51       937
weighted avg       0.52      0.51      0.51       937

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3179 - acc: 0.8656 - auc: 0.9371 - val_loss: 0.3001 - val_acc: 0.8709 - val_auc: 0.9453 - 17s/epoch - 65ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2877 - acc: 0.8817 - auc: 0.9467 - val_loss: 0.2747 - val_acc: 0.8890 - val_auc: 0.9526 - 15s/epoch - 56ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2757 - acc: 0.8906 - auc: 0.9508 - val_loss: 0.2745 - val_acc: 0.8922 - val_auc: 0.9553 - 15s/epoch - 57ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2764 - acc: 0.8871 - auc: 0.9510 - val_loss: 0.2722 - val_acc: 0.8879 - val_auc: 0.9549 - 15s/epoch - 56ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2664 - acc: 0.8908 - auc: 0.9553 - val_loss: 0.2580 - val_acc: 0.8911 - val_auc: 0.9597 - 15s/epoch - 56ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2630 - acc: 0.8944 - auc: 0.9562 - val_loss: 0.2605 - val_acc: 0.8943 - val_auc: 0.9585 - 15s/epoch - 56ms/step
Epoch 7/100
264/264 - 16s - loss: 0.2608 - acc: 0.8937 - auc: 0.9572 - val_loss: 0.2579 - val_acc: 0.8954 - val_auc: 0.9613 - 16s/epoch - 59ms/step
Epoch 8/100
264/264 - 16s - loss: 0.2585 - acc: 0.8961 - auc: 0.9583 - val_loss: 0.2869 - val_acc: 0.8869 - val_auc: 0.9562 - 16s/epoch - 59ms/step
Epoch 9/100
264/264 - 16s - loss: 0.2602 - acc: 0.8930 - auc: 0.9576 - val_loss: 0.2550 - val_acc: 0.8954 - val_auc: 0.9607 - 16s/epoch - 59ms/step
Epoch 10/100
264/264 - 16s - loss: 0.2528 - acc: 0.8976 - auc: 0.9596 - val_loss: 0.2544 - val_acc: 0.8954 - val_auc: 0.9623 - 16s/epoch - 59ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2506 - acc: 0.8969 - auc: 0.9612 - val_loss: 0.2874 - val_acc: 0.8794 - val_auc: 0.9493 - 15s/epoch - 59ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2498 - acc: 0.8975 - auc: 0.9618 - val_loss: 0.2547 - val_acc: 0.8954 - val_auc: 0.9628 - 15s/epoch - 59ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2480 - acc: 0.8979 - auc: 0.9624 - val_loss: 0.2490 - val_acc: 0.8975 - val_auc: 0.9634 - 15s/epoch - 57ms/step
Epoch 14/100
264/264 - 14s - loss: 0.2444 - acc: 0.8991 - auc: 0.9627 - val_loss: 0.2636 - val_acc: 0.8954 - val_auc: 0.9606 - 14s/epoch - 54ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2436 - acc: 0.9019 - auc: 0.9633 - val_loss: 0.2429 - val_acc: 0.9018 - val_auc: 0.9660 - 14s/epoch - 54ms/step
Epoch 16/100
264/264 - 14s - loss: 0.2442 - acc: 0.9000 - auc: 0.9632 - val_loss: 0.2643 - val_acc: 0.8933 - val_auc: 0.9602 - 14s/epoch - 54ms/step
Epoch 17/100
264/264 - 14s - loss: 0.2395 - acc: 0.9012 - auc: 0.9648 - val_loss: 0.2460 - val_acc: 0.9029 - val_auc: 0.9644 - 14s/epoch - 54ms/step
Epoch 18/100
264/264 - 14s - loss: 0.2371 - acc: 0.9002 - auc: 0.9656 - val_loss: 0.2470 - val_acc: 0.9050 - val_auc: 0.9644 - 14s/epoch - 55ms/step
Epoch 19/100
264/264 - 14s - loss: 0.2351 - acc: 0.9029 - auc: 0.9665 - val_loss: 0.2650 - val_acc: 0.8879 - val_auc: 0.9587 - 14s/epoch - 54ms/step
Epoch 20/100
264/264 - 14s - loss: 0.2323 - acc: 0.9031 - auc: 0.9671 - val_loss: 0.2585 - val_acc: 0.8933 - val_auc: 0.9608 - 14s/epoch - 54ms/step
Epoch 21/100
264/264 - 14s - loss: 0.2285 - acc: 0.9081 - auc: 0.9680 - val_loss: 0.2511 - val_acc: 0.8954 - val_auc: 0.9621 - 14s/epoch - 54ms/step
Epoch 22/100
264/264 - 14s - loss: 0.2259 - acc: 0.9072 - auc: 0.9689 - val_loss: 0.2428 - val_acc: 0.9007 - val_auc: 0.9654 - 14s/epoch - 54ms/step
Epoch 23/100
264/264 - 14s - loss: 0.2206 - acc: 0.9075 - auc: 0.9704 - val_loss: 0.2457 - val_acc: 0.9018 - val_auc: 0.9636 - 14s/epoch - 54ms/step
Epoch 24/100
264/264 - 14s - loss: 0.2190 - acc: 0.9103 - auc: 0.9710 - val_loss: 0.2478 - val_acc: 0.9007 - val_auc: 0.9640 - 14s/epoch - 54ms/step
Epoch 25/100
264/264 - 14s - loss: 0.2172 - acc: 0.9128 - auc: 0.9713 - val_loss: 0.2473 - val_acc: 0.9007 - val_auc: 0.9639 - 14s/epoch - 54ms/step
Early stopping epoch: 24
******Evaluating TEST set*********
30/30 - 1s - 723ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.90      0.85      0.88       390
           1       0.90      0.94      0.92       547

    accuracy                           0.90       937
   macro avg       0.90      0.89      0.90       937
weighted avg       0.90      0.90      0.90       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.48      0.44       390
           1       0.58      0.51      0.54       547

    accuracy                           0.50       937
   macro avg       0.49      0.49      0.49       937
weighted avg       0.51      0.50      0.50       937

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3164 - acc: 0.8641 - auc: 0.9382 - val_loss: 0.3111 - val_acc: 0.8815 - val_auc: 0.9395 - 17s/epoch - 66ms/step
Epoch 2/100
264/264 - 14s - loss: 0.2813 - acc: 0.8890 - auc: 0.9493 - val_loss: 0.2980 - val_acc: 0.8847 - val_auc: 0.9428 - 14s/epoch - 55ms/step
Epoch 3/100
264/264 - 14s - loss: 0.2707 - acc: 0.8902 - auc: 0.9534 - val_loss: 0.3376 - val_acc: 0.8559 - val_auc: 0.9318 - 14s/epoch - 55ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2698 - acc: 0.8889 - auc: 0.9546 - val_loss: 0.3062 - val_acc: 0.8815 - val_auc: 0.9419 - 15s/epoch - 55ms/step
Epoch 5/100
264/264 - 14s - loss: 0.2651 - acc: 0.8943 - auc: 0.9561 - val_loss: 0.2960 - val_acc: 0.8773 - val_auc: 0.9437 - 14s/epoch - 55ms/step
Epoch 6/100
264/264 - 14s - loss: 0.2571 - acc: 0.8975 - auc: 0.9584 - val_loss: 0.2887 - val_acc: 0.8901 - val_auc: 0.9484 - 14s/epoch - 55ms/step
Epoch 7/100
264/264 - 14s - loss: 0.2560 - acc: 0.8984 - auc: 0.9587 - val_loss: 0.2937 - val_acc: 0.8751 - val_auc: 0.9470 - 14s/epoch - 55ms/step
Epoch 8/100
264/264 - 14s - loss: 0.2557 - acc: 0.8994 - auc: 0.9586 - val_loss: 0.2883 - val_acc: 0.8869 - val_auc: 0.9486 - 14s/epoch - 55ms/step
Epoch 9/100
264/264 - 14s - loss: 0.2517 - acc: 0.8989 - auc: 0.9607 - val_loss: 0.2896 - val_acc: 0.8730 - val_auc: 0.9478 - 14s/epoch - 55ms/step
Epoch 10/100
264/264 - 14s - loss: 0.2511 - acc: 0.8997 - auc: 0.9607 - val_loss: 0.2878 - val_acc: 0.8858 - val_auc: 0.9496 - 14s/epoch - 54ms/step
Epoch 11/100
264/264 - 14s - loss: 0.2468 - acc: 0.8982 - auc: 0.9619 - val_loss: 0.2878 - val_acc: 0.8847 - val_auc: 0.9479 - 14s/epoch - 55ms/step
Epoch 12/100
264/264 - 14s - loss: 0.2429 - acc: 0.9012 - auc: 0.9636 - val_loss: 0.2830 - val_acc: 0.8773 - val_auc: 0.9518 - 14s/epoch - 55ms/step
Epoch 13/100
264/264 - 14s - loss: 0.2415 - acc: 0.9013 - auc: 0.9640 - val_loss: 0.2857 - val_acc: 0.8858 - val_auc: 0.9511 - 14s/epoch - 54ms/step
Epoch 14/100
264/264 - 14s - loss: 0.2389 - acc: 0.9043 - auc: 0.9648 - val_loss: 0.2989 - val_acc: 0.8773 - val_auc: 0.9479 - 14s/epoch - 55ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2393 - acc: 0.9020 - auc: 0.9649 - val_loss: 0.2894 - val_acc: 0.8773 - val_auc: 0.9511 - 14s/epoch - 55ms/step
Epoch 16/100
264/264 - 14s - loss: 0.2317 - acc: 0.9043 - auc: 0.9670 - val_loss: 0.3004 - val_acc: 0.8847 - val_auc: 0.9491 - 14s/epoch - 55ms/step
Epoch 17/100
264/264 - 14s - loss: 0.2340 - acc: 0.9056 - auc: 0.9663 - val_loss: 0.2821 - val_acc: 0.8869 - val_auc: 0.9529 - 14s/epoch - 54ms/step
Epoch 18/100
264/264 - 14s - loss: 0.2303 - acc: 0.9039 - auc: 0.9675 - val_loss: 0.2808 - val_acc: 0.8751 - val_auc: 0.9525 - 14s/epoch - 54ms/step
Epoch 19/100
264/264 - 14s - loss: 0.2254 - acc: 0.9068 - auc: 0.9687 - val_loss: 0.2945 - val_acc: 0.8730 - val_auc: 0.9500 - 14s/epoch - 55ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2273 - acc: 0.9081 - auc: 0.9686 - val_loss: 0.2998 - val_acc: 0.8666 - val_auc: 0.9475 - 15s/epoch - 55ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2212 - acc: 0.9106 - auc: 0.9705 - val_loss: 0.2973 - val_acc: 0.8815 - val_auc: 0.9471 - 15s/epoch - 56ms/step
Epoch 22/100
264/264 - 15s - loss: 0.2180 - acc: 0.9109 - auc: 0.9712 - val_loss: 0.3041 - val_acc: 0.8762 - val_auc: 0.9437 - 15s/epoch - 56ms/step
Epoch 23/100
264/264 - 15s - loss: 0.2151 - acc: 0.9122 - auc: 0.9722 - val_loss: 0.2846 - val_acc: 0.8815 - val_auc: 0.9540 - 15s/epoch - 57ms/step
Epoch 24/100
264/264 - 15s - loss: 0.2115 - acc: 0.9133 - auc: 0.9731 - val_loss: 0.2851 - val_acc: 0.8858 - val_auc: 0.9518 - 15s/epoch - 58ms/step
Epoch 25/100
264/264 - 15s - loss: 0.2080 - acc: 0.9157 - auc: 0.9737 - val_loss: 0.3028 - val_acc: 0.8805 - val_auc: 0.9456 - 15s/epoch - 56ms/step
Epoch 26/100
264/264 - 15s - loss: 0.2035 - acc: 0.9158 - auc: 0.9754 - val_loss: 0.2807 - val_acc: 0.8954 - val_auc: 0.9532 - 15s/epoch - 57ms/step
Epoch 27/100
264/264 - 15s - loss: 0.2016 - acc: 0.9179 - auc: 0.9758 - val_loss: 0.3055 - val_acc: 0.8794 - val_auc: 0.9497 - 15s/epoch - 56ms/step
Epoch 28/100
264/264 - 14s - loss: 0.1969 - acc: 0.9196 - auc: 0.9764 - val_loss: 0.3186 - val_acc: 0.8858 - val_auc: 0.9425 - 14s/epoch - 55ms/step
Epoch 29/100
264/264 - 14s - loss: 0.1926 - acc: 0.9192 - auc: 0.9777 - val_loss: 0.3207 - val_acc: 0.8751 - val_auc: 0.9426 - 14s/epoch - 54ms/step
Epoch 30/100
264/264 - 14s - loss: 0.1861 - acc: 0.9247 - auc: 0.9789 - val_loss: 0.3475 - val_acc: 0.8751 - val_auc: 0.9363 - 14s/epoch - 54ms/step
Epoch 31/100
264/264 - 14s - loss: 0.1815 - acc: 0.9258 - auc: 0.9803 - val_loss: 0.3394 - val_acc: 0.8741 - val_auc: 0.9423 - 14s/epoch - 54ms/step
Epoch 32/100
264/264 - 14s - loss: 0.1797 - acc: 0.9236 - auc: 0.9808 - val_loss: 0.3409 - val_acc: 0.8773 - val_auc: 0.9406 - 14s/epoch - 54ms/step
Epoch 33/100
264/264 - 15s - loss: 0.1698 - acc: 0.9294 - auc: 0.9830 - val_loss: 0.3367 - val_acc: 0.8794 - val_auc: 0.9408 - 15s/epoch - 55ms/step
Early stopping epoch: 32
******Evaluating TEST set*********
30/30 - 1s - 694ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.88      0.82      0.85       390
           1       0.88      0.92      0.90       547

    accuracy                           0.88       937
   macro avg       0.88      0.87      0.88       937
weighted avg       0.88      0.88      0.88       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.46      0.42       390
           1       0.56      0.49      0.52       547

    accuracy                           0.47       937
   macro avg       0.47      0.47      0.47       937
weighted avg       0.49      0.47      0.48       937

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3221 - acc: 0.8584 - auc: 0.9357 - val_loss: 0.2879 - val_acc: 0.8623 - val_auc: 0.9485 - 17s/epoch - 63ms/step
Epoch 2/100
264/264 - 14s - loss: 0.2871 - acc: 0.8840 - auc: 0.9469 - val_loss: 0.2751 - val_acc: 0.8890 - val_auc: 0.9524 - 14s/epoch - 55ms/step
Epoch 3/100
264/264 - 14s - loss: 0.2734 - acc: 0.8915 - auc: 0.9518 - val_loss: 0.2673 - val_acc: 0.8879 - val_auc: 0.9557 - 14s/epoch - 54ms/step
Epoch 4/100
264/264 - 14s - loss: 0.2717 - acc: 0.8930 - auc: 0.9524 - val_loss: 0.2696 - val_acc: 0.8954 - val_auc: 0.9545 - 14s/epoch - 55ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2695 - acc: 0.8934 - auc: 0.9538 - val_loss: 0.2619 - val_acc: 0.8986 - val_auc: 0.9576 - 15s/epoch - 55ms/step
Epoch 6/100
264/264 - 14s - loss: 0.2619 - acc: 0.8953 - auc: 0.9565 - val_loss: 0.2613 - val_acc: 0.8975 - val_auc: 0.9584 - 14s/epoch - 54ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2607 - acc: 0.8949 - auc: 0.9571 - val_loss: 0.2621 - val_acc: 0.8933 - val_auc: 0.9566 - 15s/epoch - 56ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2572 - acc: 0.8962 - auc: 0.9587 - val_loss: 0.2636 - val_acc: 0.8901 - val_auc: 0.9581 - 15s/epoch - 55ms/step
Epoch 9/100
264/264 - 14s - loss: 0.2547 - acc: 0.8972 - auc: 0.9591 - val_loss: 0.2622 - val_acc: 0.8890 - val_auc: 0.9582 - 14s/epoch - 55ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2509 - acc: 0.8976 - auc: 0.9607 - val_loss: 0.2633 - val_acc: 0.8858 - val_auc: 0.9581 - 15s/epoch - 55ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2503 - acc: 0.8980 - auc: 0.9611 - val_loss: 0.2583 - val_acc: 0.8943 - val_auc: 0.9580 - 15s/epoch - 55ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2476 - acc: 0.9017 - auc: 0.9617 - val_loss: 0.2717 - val_acc: 0.8943 - val_auc: 0.9561 - 15s/epoch - 55ms/step
Epoch 13/100
264/264 - 14s - loss: 0.2439 - acc: 0.9010 - auc: 0.9634 - val_loss: 0.2536 - val_acc: 0.8986 - val_auc: 0.9622 - 14s/epoch - 55ms/step
Epoch 14/100
264/264 - 14s - loss: 0.2430 - acc: 0.9018 - auc: 0.9635 - val_loss: 0.2577 - val_acc: 0.8954 - val_auc: 0.9586 - 14s/epoch - 54ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2395 - acc: 0.9031 - auc: 0.9644 - val_loss: 0.2624 - val_acc: 0.8890 - val_auc: 0.9587 - 14s/epoch - 54ms/step
Epoch 16/100
264/264 - 14s - loss: 0.2351 - acc: 0.9043 - auc: 0.9653 - val_loss: 0.2682 - val_acc: 0.8837 - val_auc: 0.9584 - 14s/epoch - 54ms/step
Epoch 17/100
264/264 - 14s - loss: 0.2356 - acc: 0.9017 - auc: 0.9660 - val_loss: 0.2646 - val_acc: 0.8975 - val_auc: 0.9566 - 14s/epoch - 54ms/step
Epoch 18/100
264/264 - 14s - loss: 0.2324 - acc: 0.9059 - auc: 0.9668 - val_loss: 0.2671 - val_acc: 0.8922 - val_auc: 0.9594 - 14s/epoch - 55ms/step
Epoch 19/100
264/264 - 14s - loss: 0.2298 - acc: 0.9065 - auc: 0.9678 - val_loss: 0.2603 - val_acc: 0.8943 - val_auc: 0.9584 - 14s/epoch - 54ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2264 - acc: 0.9091 - auc: 0.9686 - val_loss: 0.2864 - val_acc: 0.8805 - val_auc: 0.9481 - 15s/epoch - 55ms/step
Epoch 21/100
264/264 - 14s - loss: 0.2244 - acc: 0.9097 - auc: 0.9691 - val_loss: 0.2784 - val_acc: 0.8826 - val_auc: 0.9530 - 14s/epoch - 55ms/step
Epoch 22/100
264/264 - 14s - loss: 0.2217 - acc: 0.9097 - auc: 0.9699 - val_loss: 0.2696 - val_acc: 0.8879 - val_auc: 0.9549 - 14s/epoch - 54ms/step
Epoch 23/100
264/264 - 14s - loss: 0.2196 - acc: 0.9082 - auc: 0.9707 - val_loss: 0.2938 - val_acc: 0.8847 - val_auc: 0.9448 - 14s/epoch - 55ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
30/30 - 1s - 698ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.89      0.86      0.88       390
           1       0.90      0.93      0.91       547

    accuracy                           0.90       937
   macro avg       0.90      0.89      0.90       937
weighted avg       0.90      0.90      0.90       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.45      0.55      0.49       390
           1       0.62      0.52      0.56       547

    accuracy                           0.53       937
   macro avg       0.53      0.53      0.53       937
weighted avg       0.55      0.53      0.53       937

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3163 - acc: 0.8611 - auc: 0.9374 - val_loss: 0.3250 - val_acc: 0.8666 - val_auc: 0.9359 - 17s/epoch - 65ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2782 - acc: 0.8868 - auc: 0.9504 - val_loss: 0.3293 - val_acc: 0.8634 - val_auc: 0.9380 - 15s/epoch - 55ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2711 - acc: 0.8922 - auc: 0.9529 - val_loss: 0.3082 - val_acc: 0.8741 - val_auc: 0.9404 - 15s/epoch - 56ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2655 - acc: 0.8916 - auc: 0.9550 - val_loss: 0.3090 - val_acc: 0.8698 - val_auc: 0.9446 - 15s/epoch - 55ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2658 - acc: 0.8929 - auc: 0.9553 - val_loss: 0.3220 - val_acc: 0.8709 - val_auc: 0.9323 - 15s/epoch - 55ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2563 - acc: 0.8980 - auc: 0.9590 - val_loss: 0.3110 - val_acc: 0.8698 - val_auc: 0.9388 - 15s/epoch - 55ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2553 - acc: 0.8965 - auc: 0.9594 - val_loss: 0.3302 - val_acc: 0.8677 - val_auc: 0.9465 - 15s/epoch - 55ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2524 - acc: 0.9005 - auc: 0.9607 - val_loss: 0.3131 - val_acc: 0.8687 - val_auc: 0.9419 - 15s/epoch - 56ms/step
Epoch 9/100
264/264 - 15s - loss: 0.2519 - acc: 0.8957 - auc: 0.9606 - val_loss: 0.2981 - val_acc: 0.8741 - val_auc: 0.9485 - 15s/epoch - 55ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2503 - acc: 0.8995 - auc: 0.9611 - val_loss: 0.2937 - val_acc: 0.8719 - val_auc: 0.9477 - 15s/epoch - 57ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2464 - acc: 0.9005 - auc: 0.9625 - val_loss: 0.2933 - val_acc: 0.8709 - val_auc: 0.9498 - 15s/epoch - 55ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2434 - acc: 0.9027 - auc: 0.9638 - val_loss: 0.2847 - val_acc: 0.8698 - val_auc: 0.9506 - 15s/epoch - 55ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2416 - acc: 0.8991 - auc: 0.9647 - val_loss: 0.3065 - val_acc: 0.8677 - val_auc: 0.9470 - 15s/epoch - 55ms/step
Epoch 14/100
264/264 - 15s - loss: 0.2373 - acc: 0.9053 - auc: 0.9655 - val_loss: 0.2891 - val_acc: 0.8751 - val_auc: 0.9506 - 15s/epoch - 55ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2348 - acc: 0.9053 - auc: 0.9661 - val_loss: 0.2911 - val_acc: 0.8730 - val_auc: 0.9521 - 14s/epoch - 55ms/step
Epoch 16/100
264/264 - 14s - loss: 0.2321 - acc: 0.9057 - auc: 0.9668 - val_loss: 0.2766 - val_acc: 0.8805 - val_auc: 0.9520 - 14s/epoch - 55ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2309 - acc: 0.9049 - auc: 0.9672 - val_loss: 0.2857 - val_acc: 0.8666 - val_auc: 0.9510 - 15s/epoch - 55ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2244 - acc: 0.9081 - auc: 0.9693 - val_loss: 0.2822 - val_acc: 0.8730 - val_auc: 0.9514 - 15s/epoch - 56ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2273 - acc: 0.9069 - auc: 0.9684 - val_loss: 0.2781 - val_acc: 0.8847 - val_auc: 0.9552 - 15s/epoch - 57ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2228 - acc: 0.9093 - auc: 0.9694 - val_loss: 0.2764 - val_acc: 0.8751 - val_auc: 0.9528 - 15s/epoch - 55ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2175 - acc: 0.9134 - auc: 0.9713 - val_loss: 0.2743 - val_acc: 0.8858 - val_auc: 0.9553 - 15s/epoch - 55ms/step
Epoch 22/100
264/264 - 14s - loss: 0.2166 - acc: 0.9127 - auc: 0.9716 - val_loss: 0.2909 - val_acc: 0.8858 - val_auc: 0.9548 - 14s/epoch - 55ms/step
Epoch 23/100
264/264 - 15s - loss: 0.2117 - acc: 0.9141 - auc: 0.9726 - val_loss: 0.3040 - val_acc: 0.8719 - val_auc: 0.9462 - 15s/epoch - 55ms/step
Epoch 24/100
264/264 - 15s - loss: 0.2076 - acc: 0.9145 - auc: 0.9732 - val_loss: 0.2779 - val_acc: 0.8901 - val_auc: 0.9563 - 15s/epoch - 55ms/step
Epoch 25/100
264/264 - 14s - loss: 0.2060 - acc: 0.9178 - auc: 0.9741 - val_loss: 0.2659 - val_acc: 0.8975 - val_auc: 0.9571 - 14s/epoch - 55ms/step
Epoch 26/100
264/264 - 14s - loss: 0.2044 - acc: 0.9169 - auc: 0.9746 - val_loss: 0.2773 - val_acc: 0.8901 - val_auc: 0.9550 - 14s/epoch - 54ms/step
Epoch 27/100
264/264 - 14s - loss: 0.1965 - acc: 0.9218 - auc: 0.9766 - val_loss: 0.2867 - val_acc: 0.8815 - val_auc: 0.9547 - 14s/epoch - 54ms/step
Epoch 28/100
264/264 - 15s - loss: 0.1895 - acc: 0.9230 - auc: 0.9783 - val_loss: 0.2693 - val_acc: 0.8890 - val_auc: 0.9582 - 15s/epoch - 57ms/step
Epoch 29/100
264/264 - 15s - loss: 0.1884 - acc: 0.9215 - auc: 0.9785 - val_loss: 0.2950 - val_acc: 0.8773 - val_auc: 0.9538 - 15s/epoch - 57ms/step
Epoch 30/100
264/264 - 15s - loss: 0.1795 - acc: 0.9292 - auc: 0.9801 - val_loss: 0.3065 - val_acc: 0.8858 - val_auc: 0.9509 - 15s/epoch - 56ms/step
Epoch 31/100
264/264 - 15s - loss: 0.1767 - acc: 0.9275 - auc: 0.9816 - val_loss: 0.3110 - val_acc: 0.8773 - val_auc: 0.9494 - 15s/epoch - 57ms/step
Epoch 32/100
264/264 - 15s - loss: 0.1732 - acc: 0.9317 - auc: 0.9820 - val_loss: 0.3223 - val_acc: 0.8751 - val_auc: 0.9474 - 15s/epoch - 56ms/step
Epoch 33/100
264/264 - 15s - loss: 0.1647 - acc: 0.9339 - auc: 0.9840 - val_loss: 0.2976 - val_acc: 0.8911 - val_auc: 0.9516 - 15s/epoch - 56ms/step
Epoch 34/100
264/264 - 15s - loss: 0.1580 - acc: 0.9377 - auc: 0.9850 - val_loss: 0.2976 - val_acc: 0.8858 - val_auc: 0.9523 - 15s/epoch - 56ms/step
Epoch 35/100
264/264 - 15s - loss: 0.1589 - acc: 0.9365 - auc: 0.9847 - val_loss: 0.3181 - val_acc: 0.8709 - val_auc: 0.9430 - 15s/epoch - 56ms/step
Epoch 36/100
264/264 - 15s - loss: 0.1492 - acc: 0.9409 - auc: 0.9866 - val_loss: 0.2978 - val_acc: 0.8826 - val_auc: 0.9560 - 15s/epoch - 56ms/step
Epoch 37/100
264/264 - 15s - loss: 0.1365 - acc: 0.9453 - auc: 0.9888 - val_loss: 0.3256 - val_acc: 0.8741 - val_auc: 0.9487 - 15s/epoch - 56ms/step
Epoch 38/100
264/264 - 15s - loss: 0.1345 - acc: 0.9446 - auc: 0.9888 - val_loss: 0.3449 - val_acc: 0.8858 - val_auc: 0.9444 - 15s/epoch - 56ms/step
Early stopping epoch: 37
******Evaluating TEST set*********
30/30 - 1s - 743ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.91      0.82      0.86       390
           1       0.88      0.94      0.91       547

    accuracy                           0.89       937
   macro avg       0.89      0.88      0.88       937
weighted avg       0.89      0.89      0.89       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.53      0.46       390
           1       0.57      0.46      0.51       547

    accuracy                           0.49       937
   macro avg       0.49      0.49      0.48       937
weighted avg       0.51      0.49      0.49       937

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3163 - acc: 0.8634 - auc: 0.9378 - val_loss: 0.2848 - val_acc: 0.8783 - val_auc: 0.9503 - 17s/epoch - 64ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2845 - acc: 0.8861 - auc: 0.9480 - val_loss: 0.2731 - val_acc: 0.8826 - val_auc: 0.9542 - 15s/epoch - 56ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2778 - acc: 0.8865 - auc: 0.9512 - val_loss: 0.2794 - val_acc: 0.8869 - val_auc: 0.9523 - 15s/epoch - 56ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2691 - acc: 0.8941 - auc: 0.9544 - val_loss: 0.2699 - val_acc: 0.8794 - val_auc: 0.9539 - 15s/epoch - 57ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2610 - acc: 0.8960 - auc: 0.9576 - val_loss: 0.2671 - val_acc: 0.8922 - val_auc: 0.9563 - 15s/epoch - 56ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2649 - acc: 0.8934 - auc: 0.9560 - val_loss: 0.2606 - val_acc: 0.8869 - val_auc: 0.9573 - 15s/epoch - 56ms/step
Epoch 7/100
264/264 - 14s - loss: 0.2619 - acc: 0.8963 - auc: 0.9571 - val_loss: 0.2637 - val_acc: 0.8943 - val_auc: 0.9576 - 14s/epoch - 55ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2568 - acc: 0.8970 - auc: 0.9589 - val_loss: 0.2632 - val_acc: 0.8922 - val_auc: 0.9586 - 15s/epoch - 55ms/step
Epoch 9/100
264/264 - 14s - loss: 0.2546 - acc: 0.8969 - auc: 0.9589 - val_loss: 0.2520 - val_acc: 0.8933 - val_auc: 0.9615 - 14s/epoch - 55ms/step
Epoch 10/100
264/264 - 14s - loss: 0.2523 - acc: 0.8982 - auc: 0.9605 - val_loss: 0.2646 - val_acc: 0.8975 - val_auc: 0.9594 - 14s/epoch - 54ms/step
Epoch 11/100
264/264 - 14s - loss: 0.2488 - acc: 0.8959 - auc: 0.9613 - val_loss: 0.2564 - val_acc: 0.8890 - val_auc: 0.9594 - 14s/epoch - 54ms/step
Epoch 12/100
264/264 - 14s - loss: 0.2474 - acc: 0.8994 - auc: 0.9622 - val_loss: 0.2502 - val_acc: 0.8986 - val_auc: 0.9602 - 14s/epoch - 54ms/step
Epoch 13/100
264/264 - 14s - loss: 0.2445 - acc: 0.8993 - auc: 0.9631 - val_loss: 0.2540 - val_acc: 0.9061 - val_auc: 0.9592 - 14s/epoch - 55ms/step
Epoch 14/100
264/264 - 14s - loss: 0.2437 - acc: 0.9023 - auc: 0.9634 - val_loss: 0.2546 - val_acc: 0.8997 - val_auc: 0.9604 - 14s/epoch - 55ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2437 - acc: 0.9014 - auc: 0.9634 - val_loss: 0.2511 - val_acc: 0.9039 - val_auc: 0.9599 - 14s/epoch - 55ms/step
Epoch 16/100
264/264 - 14s - loss: 0.2396 - acc: 0.9021 - auc: 0.9644 - val_loss: 0.2507 - val_acc: 0.9007 - val_auc: 0.9610 - 14s/epoch - 54ms/step
Epoch 17/100
264/264 - 14s - loss: 0.2368 - acc: 0.9016 - auc: 0.9656 - val_loss: 0.2505 - val_acc: 0.9018 - val_auc: 0.9611 - 14s/epoch - 54ms/step
Epoch 18/100
264/264 - 14s - loss: 0.2354 - acc: 0.9056 - auc: 0.9663 - val_loss: 0.2564 - val_acc: 0.8997 - val_auc: 0.9600 - 14s/epoch - 54ms/step
Epoch 19/100
264/264 - 14s - loss: 0.2324 - acc: 0.9052 - auc: 0.9666 - val_loss: 0.2536 - val_acc: 0.8901 - val_auc: 0.9604 - 14s/epoch - 54ms/step
Early stopping epoch: 18
******Evaluating TEST set*********
30/30 - 1s - 692ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.89      0.84      0.87       390
           1       0.89      0.93      0.91       547

    accuracy                           0.89       937
   macro avg       0.89      0.89      0.89       937
weighted avg       0.89      0.89      0.89       937

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.46      0.42       390
           1       0.56      0.50      0.53       547

    accuracy                           0.48       937
   macro avg       0.48      0.48      0.48       937
weighted avg       0.49      0.48      0.48       937

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3211 - acc: 0.8602 - auc: 0.9355 - val_loss: 0.2641 - val_acc: 0.8985 - val_auc: 0.9576 - 17s/epoch - 64ms/step
Epoch 2/100
264/264 - 14s - loss: 0.2894 - acc: 0.8801 - auc: 0.9462 - val_loss: 0.2471 - val_acc: 0.9049 - val_auc: 0.9617 - 14s/epoch - 54ms/step
Epoch 3/100
264/264 - 14s - loss: 0.2785 - acc: 0.8870 - auc: 0.9498 - val_loss: 0.2471 - val_acc: 0.8953 - val_auc: 0.9629 - 14s/epoch - 54ms/step
Epoch 4/100
264/264 - 14s - loss: 0.2759 - acc: 0.8896 - auc: 0.9511 - val_loss: 0.2446 - val_acc: 0.8953 - val_auc: 0.9636 - 14s/epoch - 54ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2677 - acc: 0.8920 - auc: 0.9539 - val_loss: 0.2371 - val_acc: 0.9103 - val_auc: 0.9662 - 15s/epoch - 56ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2636 - acc: 0.8965 - auc: 0.9563 - val_loss: 0.2652 - val_acc: 0.8996 - val_auc: 0.9580 - 15s/epoch - 57ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2596 - acc: 0.8963 - auc: 0.9572 - val_loss: 0.2305 - val_acc: 0.9049 - val_auc: 0.9684 - 15s/epoch - 56ms/step
Epoch 8/100
264/264 - 14s - loss: 0.2587 - acc: 0.8943 - auc: 0.9576 - val_loss: 0.2388 - val_acc: 0.9028 - val_auc: 0.9664 - 14s/epoch - 55ms/step
Epoch 9/100
264/264 - 14s - loss: 0.2566 - acc: 0.8972 - auc: 0.9585 - val_loss: 0.2587 - val_acc: 0.8921 - val_auc: 0.9612 - 14s/epoch - 55ms/step
Epoch 10/100
264/264 - 14s - loss: 0.2546 - acc: 0.8971 - auc: 0.9598 - val_loss: 0.2346 - val_acc: 0.9006 - val_auc: 0.9679 - 14s/epoch - 55ms/step
Epoch 11/100
264/264 - 14s - loss: 0.2496 - acc: 0.9007 - auc: 0.9610 - val_loss: 0.2282 - val_acc: 0.9038 - val_auc: 0.9684 - 14s/epoch - 54ms/step
Epoch 12/100
264/264 - 14s - loss: 0.2517 - acc: 0.8967 - auc: 0.9608 - val_loss: 0.2338 - val_acc: 0.8974 - val_auc: 0.9683 - 14s/epoch - 54ms/step
Epoch 13/100
264/264 - 14s - loss: 0.2467 - acc: 0.8980 - auc: 0.9625 - val_loss: 0.2399 - val_acc: 0.9028 - val_auc: 0.9682 - 14s/epoch - 54ms/step
Epoch 14/100
264/264 - 14s - loss: 0.2444 - acc: 0.8995 - auc: 0.9631 - val_loss: 0.2278 - val_acc: 0.9071 - val_auc: 0.9691 - 14s/epoch - 54ms/step
Epoch 15/100
264/264 - 14s - loss: 0.2407 - acc: 0.9043 - auc: 0.9645 - val_loss: 0.2443 - val_acc: 0.8964 - val_auc: 0.9653 - 14s/epoch - 55ms/step
Epoch 16/100
264/264 - 15s - loss: 0.2392 - acc: 0.9028 - auc: 0.9653 - val_loss: 0.2276 - val_acc: 0.9049 - val_auc: 0.9687 - 15s/epoch - 55ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2346 - acc: 0.9037 - auc: 0.9661 - val_loss: 0.2340 - val_acc: 0.8985 - val_auc: 0.9674 - 15s/epoch - 55ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2334 - acc: 0.9048 - auc: 0.9671 - val_loss: 0.2421 - val_acc: 0.8974 - val_auc: 0.9656 - 15s/epoch - 56ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2289 - acc: 0.9068 - auc: 0.9679 - val_loss: 0.2302 - val_acc: 0.9006 - val_auc: 0.9689 - 15s/epoch - 56ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2271 - acc: 0.9079 - auc: 0.9685 - val_loss: 0.2365 - val_acc: 0.8996 - val_auc: 0.9668 - 15s/epoch - 56ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2232 - acc: 0.9105 - auc: 0.9693 - val_loss: 0.2432 - val_acc: 0.8985 - val_auc: 0.9638 - 15s/epoch - 56ms/step
Epoch 22/100
264/264 - 15s - loss: 0.2175 - acc: 0.9094 - auc: 0.9711 - val_loss: 0.2454 - val_acc: 0.8900 - val_auc: 0.9635 - 15s/epoch - 56ms/step
Epoch 23/100
264/264 - 15s - loss: 0.2180 - acc: 0.9131 - auc: 0.9710 - val_loss: 0.2548 - val_acc: 0.8889 - val_auc: 0.9608 - 15s/epoch - 56ms/step
Epoch 24/100
264/264 - 15s - loss: 0.2124 - acc: 0.9151 - auc: 0.9720 - val_loss: 0.2447 - val_acc: 0.8964 - val_auc: 0.9633 - 15s/epoch - 56ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
30/30 - 1s - 721ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.89       390
           1       0.90      0.94      0.92       546

    accuracy                           0.91       936
   macro avg       0.91      0.90      0.90       936
weighted avg       0.91      0.91      0.91       936

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.40      0.47      0.43       390
           1       0.57      0.49      0.53       546

    accuracy                           0.49       936
   macro avg       0.48      0.48      0.48       936
weighted avg       0.50      0.49      0.49       936

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
264/264 - 17s - loss: 0.3264 - acc: 0.8592 - auc: 0.9338 - val_loss: 0.2628 - val_acc: 0.9006 - val_auc: 0.9560 - 17s/epoch - 64ms/step
Epoch 2/100
264/264 - 15s - loss: 0.2871 - acc: 0.8832 - auc: 0.9471 - val_loss: 0.2422 - val_acc: 0.9113 - val_auc: 0.9629 - 15s/epoch - 56ms/step
Epoch 3/100
264/264 - 15s - loss: 0.2760 - acc: 0.8866 - auc: 0.9508 - val_loss: 0.2408 - val_acc: 0.9049 - val_auc: 0.9623 - 15s/epoch - 56ms/step
Epoch 4/100
264/264 - 15s - loss: 0.2770 - acc: 0.8866 - auc: 0.9519 - val_loss: 0.2381 - val_acc: 0.9145 - val_auc: 0.9641 - 15s/epoch - 56ms/step
Epoch 5/100
264/264 - 15s - loss: 0.2641 - acc: 0.8912 - auc: 0.9562 - val_loss: 0.2443 - val_acc: 0.9060 - val_auc: 0.9620 - 15s/epoch - 56ms/step
Epoch 6/100
264/264 - 15s - loss: 0.2644 - acc: 0.8918 - auc: 0.9562 - val_loss: 0.2447 - val_acc: 0.9060 - val_auc: 0.9624 - 15s/epoch - 56ms/step
Epoch 7/100
264/264 - 15s - loss: 0.2606 - acc: 0.8916 - auc: 0.9577 - val_loss: 0.2439 - val_acc: 0.9006 - val_auc: 0.9608 - 15s/epoch - 56ms/step
Epoch 8/100
264/264 - 15s - loss: 0.2586 - acc: 0.8955 - auc: 0.9581 - val_loss: 0.2306 - val_acc: 0.9103 - val_auc: 0.9652 - 15s/epoch - 56ms/step
Epoch 9/100
264/264 - 15s - loss: 0.2585 - acc: 0.8953 - auc: 0.9582 - val_loss: 0.2316 - val_acc: 0.9167 - val_auc: 0.9652 - 15s/epoch - 56ms/step
Epoch 10/100
264/264 - 15s - loss: 0.2561 - acc: 0.8977 - auc: 0.9591 - val_loss: 0.2367 - val_acc: 0.9071 - val_auc: 0.9602 - 15s/epoch - 56ms/step
Epoch 11/100
264/264 - 15s - loss: 0.2511 - acc: 0.8979 - auc: 0.9611 - val_loss: 0.2319 - val_acc: 0.9103 - val_auc: 0.9659 - 15s/epoch - 56ms/step
Epoch 12/100
264/264 - 15s - loss: 0.2521 - acc: 0.8979 - auc: 0.9606 - val_loss: 0.2303 - val_acc: 0.9092 - val_auc: 0.9653 - 15s/epoch - 56ms/step
Epoch 13/100
264/264 - 15s - loss: 0.2510 - acc: 0.8975 - auc: 0.9614 - val_loss: 0.2331 - val_acc: 0.9167 - val_auc: 0.9671 - 15s/epoch - 56ms/step
Epoch 14/100
264/264 - 15s - loss: 0.2464 - acc: 0.9007 - auc: 0.9628 - val_loss: 0.2295 - val_acc: 0.9103 - val_auc: 0.9663 - 15s/epoch - 56ms/step
Epoch 15/100
264/264 - 15s - loss: 0.2430 - acc: 0.9031 - auc: 0.9641 - val_loss: 0.2373 - val_acc: 0.9135 - val_auc: 0.9654 - 15s/epoch - 56ms/step
Epoch 16/100
264/264 - 15s - loss: 0.2466 - acc: 0.9012 - auc: 0.9626 - val_loss: 0.2337 - val_acc: 0.9145 - val_auc: 0.9651 - 15s/epoch - 56ms/step
Epoch 17/100
264/264 - 15s - loss: 0.2395 - acc: 0.9039 - auc: 0.9652 - val_loss: 0.2257 - val_acc: 0.9113 - val_auc: 0.9676 - 15s/epoch - 55ms/step
Epoch 18/100
264/264 - 15s - loss: 0.2386 - acc: 0.9018 - auc: 0.9654 - val_loss: 0.2288 - val_acc: 0.9113 - val_auc: 0.9669 - 15s/epoch - 56ms/step
Epoch 19/100
264/264 - 15s - loss: 0.2357 - acc: 0.9048 - auc: 0.9664 - val_loss: 0.2329 - val_acc: 0.9145 - val_auc: 0.9654 - 15s/epoch - 56ms/step
Epoch 20/100
264/264 - 15s - loss: 0.2357 - acc: 0.9004 - auc: 0.9664 - val_loss: 0.2410 - val_acc: 0.9103 - val_auc: 0.9647 - 15s/epoch - 56ms/step
Epoch 21/100
264/264 - 15s - loss: 0.2344 - acc: 0.9041 - auc: 0.9668 - val_loss: 0.2250 - val_acc: 0.9113 - val_auc: 0.9679 - 15s/epoch - 56ms/step
Epoch 22/100
264/264 - 15s - loss: 0.2263 - acc: 0.9071 - auc: 0.9692 - val_loss: 0.2282 - val_acc: 0.9081 - val_auc: 0.9669 - 15s/epoch - 56ms/step
Epoch 23/100
264/264 - 15s - loss: 0.2248 - acc: 0.9064 - auc: 0.9697 - val_loss: 0.2245 - val_acc: 0.9167 - val_auc: 0.9680 - 15s/epoch - 56ms/step
Epoch 24/100
264/264 - 15s - loss: 0.2214 - acc: 0.9090 - auc: 0.9703 - val_loss: 0.2306 - val_acc: 0.9092 - val_auc: 0.9657 - 15s/epoch - 56ms/step
Epoch 25/100
264/264 - 15s - loss: 0.2184 - acc: 0.9100 - auc: 0.9715 - val_loss: 0.2244 - val_acc: 0.9103 - val_auc: 0.9686 - 15s/epoch - 56ms/step
Epoch 26/100
264/264 - 15s - loss: 0.2134 - acc: 0.9119 - auc: 0.9724 - val_loss: 0.2505 - val_acc: 0.9049 - val_auc: 0.9596 - 15s/epoch - 56ms/step
Epoch 27/100
264/264 - 15s - loss: 0.2064 - acc: 0.9151 - auc: 0.9744 - val_loss: 0.2462 - val_acc: 0.9006 - val_auc: 0.9625 - 15s/epoch - 56ms/step
Epoch 28/100
264/264 - 15s - loss: 0.2040 - acc: 0.9192 - auc: 0.9749 - val_loss: 0.2358 - val_acc: 0.9113 - val_auc: 0.9650 - 15s/epoch - 56ms/step
Epoch 29/100
264/264 - 15s - loss: 0.1945 - acc: 0.9214 - auc: 0.9768 - val_loss: 0.2428 - val_acc: 0.9049 - val_auc: 0.9644 - 15s/epoch - 56ms/step
Epoch 30/100
264/264 - 15s - loss: 0.1910 - acc: 0.9222 - auc: 0.9780 - val_loss: 0.2411 - val_acc: 0.9092 - val_auc: 0.9616 - 15s/epoch - 56ms/step
Epoch 31/100
264/264 - 15s - loss: 0.1856 - acc: 0.9236 - auc: 0.9792 - val_loss: 0.2502 - val_acc: 0.9092 - val_auc: 0.9619 - 15s/epoch - 56ms/step
Epoch 32/100
264/264 - 15s - loss: 0.1828 - acc: 0.9296 - auc: 0.9797 - val_loss: 0.2458 - val_acc: 0.9060 - val_auc: 0.9630 - 15s/epoch - 56ms/step
Epoch 33/100
264/264 - 15s - loss: 0.1720 - acc: 0.9311 - auc: 0.9815 - val_loss: 0.2483 - val_acc: 0.9092 - val_auc: 0.9635 - 15s/epoch - 55ms/step
Epoch 34/100
264/264 - 15s - loss: 0.1640 - acc: 0.9343 - auc: 0.9834 - val_loss: 0.2543 - val_acc: 0.9017 - val_auc: 0.9618 - 15s/epoch - 56ms/step
Epoch 35/100
264/264 - 14s - loss: 0.1598 - acc: 0.9358 - auc: 0.9843 - val_loss: 0.2685 - val_acc: 0.8996 - val_auc: 0.9605 - 14s/epoch - 55ms/step
Early stopping epoch: 34
******Evaluating TEST set*********
30/30 - 1s - 773ms/epoch - 26ms/step
              precision    recall  f1-score   support

           0       0.91      0.87      0.89       390
           1       0.91      0.94      0.92       546

    accuracy                           0.91       936
   macro avg       0.91      0.90      0.91       936
weighted avg       0.91      0.91      0.91       936

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.52      0.47       390
           1       0.59      0.48      0.53       546

    accuracy                           0.50       936
   macro avg       0.50      0.50      0.50       936
weighted avg       0.52      0.50      0.50       936

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8992] IC [0.8919, 0.9066]
Mean Recall[0.8927] IC [0.8845, 0.9008]
Mean F1[0.8955] IC [0.8878, 0.9032]
Median Accuracy[0.9002]
Median Recall[0.8939]
Median F1[0.8966]
********************txid297246********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.60      0.65      0.62       130
           1       0.95      0.94      0.94       877

    accuracy                           0.90      1007
   macro avg       0.77      0.79      0.78      1007
weighted avg       0.90      0.90      0.90      1007

Predicted  0.0  1.0   All
True                     
0           84   46   130
1           56  821   877
All        140  867  1007
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 16s - loss: 0.3312 - acc: 0.8563 - auc: 0.9314 - val_loss: 0.2886 - val_acc: 0.8813 - val_auc: 0.9510 - 16s/epoch - 62ms/step
Epoch 2/100
258/258 - 13s - loss: 0.3008 - acc: 0.8775 - auc: 0.9422 - val_loss: 0.2745 - val_acc: 0.8922 - val_auc: 0.9547 - 13s/epoch - 52ms/step
Epoch 3/100
258/258 - 13s - loss: 0.2925 - acc: 0.8820 - auc: 0.9455 - val_loss: 0.2739 - val_acc: 0.8943 - val_auc: 0.9543 - 13s/epoch - 52ms/step
Epoch 4/100
258/258 - 13s - loss: 0.2857 - acc: 0.8838 - auc: 0.9471 - val_loss: 0.2633 - val_acc: 0.9009 - val_auc: 0.9589 - 13s/epoch - 52ms/step
Epoch 5/100
258/258 - 13s - loss: 0.2823 - acc: 0.8842 - auc: 0.9489 - val_loss: 0.2598 - val_acc: 0.8987 - val_auc: 0.9591 - 13s/epoch - 52ms/step
Epoch 6/100
258/258 - 13s - loss: 0.2773 - acc: 0.8907 - auc: 0.9503 - val_loss: 0.2832 - val_acc: 0.8747 - val_auc: 0.9533 - 13s/epoch - 52ms/step
Epoch 7/100
258/258 - 13s - loss: 0.2710 - acc: 0.8887 - auc: 0.9535 - val_loss: 0.2524 - val_acc: 0.8998 - val_auc: 0.9606 - 13s/epoch - 52ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2705 - acc: 0.8900 - auc: 0.9538 - val_loss: 0.2559 - val_acc: 0.8932 - val_auc: 0.9595 - 14s/epoch - 53ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2681 - acc: 0.8902 - auc: 0.9550 - val_loss: 0.2641 - val_acc: 0.8834 - val_auc: 0.9584 - 14s/epoch - 53ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2671 - acc: 0.8865 - auc: 0.9557 - val_loss: 0.2556 - val_acc: 0.8922 - val_auc: 0.9599 - 14s/epoch - 53ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2626 - acc: 0.8930 - auc: 0.9572 - val_loss: 0.2553 - val_acc: 0.8889 - val_auc: 0.9599 - 14s/epoch - 53ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2618 - acc: 0.8929 - auc: 0.9568 - val_loss: 0.2512 - val_acc: 0.8900 - val_auc: 0.9617 - 14s/epoch - 53ms/step
Epoch 13/100
258/258 - 14s - loss: 0.2559 - acc: 0.8958 - auc: 0.9591 - val_loss: 0.2661 - val_acc: 0.8943 - val_auc: 0.9588 - 14s/epoch - 53ms/step
Epoch 14/100
258/258 - 14s - loss: 0.2554 - acc: 0.8961 - auc: 0.9589 - val_loss: 0.2623 - val_acc: 0.8867 - val_auc: 0.9582 - 14s/epoch - 53ms/step
Epoch 15/100
258/258 - 14s - loss: 0.2526 - acc: 0.8993 - auc: 0.9601 - val_loss: 0.2575 - val_acc: 0.8911 - val_auc: 0.9609 - 14s/epoch - 54ms/step
Epoch 16/100
258/258 - 14s - loss: 0.2496 - acc: 0.8997 - auc: 0.9616 - val_loss: 0.2528 - val_acc: 0.8976 - val_auc: 0.9609 - 14s/epoch - 54ms/step
Epoch 17/100
258/258 - 14s - loss: 0.2450 - acc: 0.9002 - auc: 0.9629 - val_loss: 0.2595 - val_acc: 0.8878 - val_auc: 0.9592 - 14s/epoch - 55ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2450 - acc: 0.9026 - auc: 0.9626 - val_loss: 0.2716 - val_acc: 0.8845 - val_auc: 0.9561 - 14s/epoch - 54ms/step
Epoch 19/100
258/258 - 14s - loss: 0.2410 - acc: 0.9050 - auc: 0.9639 - val_loss: 0.2495 - val_acc: 0.8965 - val_auc: 0.9624 - 14s/epoch - 55ms/step
Epoch 20/100
258/258 - 14s - loss: 0.2397 - acc: 0.9031 - auc: 0.9648 - val_loss: 0.2550 - val_acc: 0.8856 - val_auc: 0.9611 - 14s/epoch - 55ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2331 - acc: 0.9070 - auc: 0.9663 - val_loss: 0.2569 - val_acc: 0.8922 - val_auc: 0.9605 - 14s/epoch - 55ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2351 - acc: 0.9021 - auc: 0.9666 - val_loss: 0.2673 - val_acc: 0.8900 - val_auc: 0.9559 - 14s/epoch - 55ms/step
Epoch 23/100
258/258 - 14s - loss: 0.2320 - acc: 0.9050 - auc: 0.9675 - val_loss: 0.2683 - val_acc: 0.8900 - val_auc: 0.9573 - 14s/epoch - 55ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2304 - acc: 0.9066 - auc: 0.9673 - val_loss: 0.2711 - val_acc: 0.8791 - val_auc: 0.9577 - 14s/epoch - 55ms/step
Epoch 25/100
258/258 - 14s - loss: 0.2234 - acc: 0.9080 - auc: 0.9692 - val_loss: 0.2723 - val_acc: 0.8932 - val_auc: 0.9542 - 14s/epoch - 56ms/step
Epoch 26/100
258/258 - 14s - loss: 0.2211 - acc: 0.9102 - auc: 0.9696 - val_loss: 0.2812 - val_acc: 0.8932 - val_auc: 0.9563 - 14s/epoch - 56ms/step
Epoch 27/100
258/258 - 14s - loss: 0.2150 - acc: 0.9130 - auc: 0.9718 - val_loss: 0.2652 - val_acc: 0.8889 - val_auc: 0.9602 - 14s/epoch - 56ms/step
Epoch 28/100
258/258 - 14s - loss: 0.2076 - acc: 0.9147 - auc: 0.9735 - val_loss: 0.2904 - val_acc: 0.8791 - val_auc: 0.9514 - 14s/epoch - 56ms/step
Epoch 29/100
258/258 - 14s - loss: 0.2091 - acc: 0.9123 - auc: 0.9733 - val_loss: 0.2856 - val_acc: 0.8932 - val_auc: 0.9552 - 14s/epoch - 56ms/step
Early stopping epoch: 28
******Evaluating TEST set*********
29/29 - 1s - 718ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.89      0.86      0.87       386
           1       0.90      0.92      0.91       532

    accuracy                           0.90       918
   macro avg       0.90      0.89      0.89       918
weighted avg       0.90      0.90      0.90       918

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.43      0.51      0.47       386
           1       0.59      0.52      0.55       532

    accuracy                           0.52       918
   macro avg       0.51      0.51      0.51       918
weighted avg       0.53      0.52      0.52       918

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3318 - acc: 0.8580 - auc: 0.9312 - val_loss: 0.2879 - val_acc: 0.8813 - val_auc: 0.9518 - 17s/epoch - 65ms/step
Epoch 2/100
258/258 - 14s - loss: 0.2964 - acc: 0.8804 - auc: 0.9432 - val_loss: 0.2742 - val_acc: 0.8878 - val_auc: 0.9557 - 14s/epoch - 56ms/step
Epoch 3/100
258/258 - 14s - loss: 0.2883 - acc: 0.8844 - auc: 0.9464 - val_loss: 0.2834 - val_acc: 0.8769 - val_auc: 0.9517 - 14s/epoch - 56ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2822 - acc: 0.8900 - auc: 0.9484 - val_loss: 0.2674 - val_acc: 0.8922 - val_auc: 0.9573 - 14s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2761 - acc: 0.8884 - auc: 0.9507 - val_loss: 0.2572 - val_acc: 0.8932 - val_auc: 0.9601 - 14s/epoch - 55ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2731 - acc: 0.8918 - auc: 0.9522 - val_loss: 0.2874 - val_acc: 0.8813 - val_auc: 0.9489 - 14s/epoch - 55ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2709 - acc: 0.8902 - auc: 0.9535 - val_loss: 0.2621 - val_acc: 0.8922 - val_auc: 0.9602 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2659 - acc: 0.8951 - auc: 0.9553 - val_loss: 0.2644 - val_acc: 0.8889 - val_auc: 0.9589 - 14s/epoch - 55ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2681 - acc: 0.8906 - auc: 0.9544 - val_loss: 0.2564 - val_acc: 0.8932 - val_auc: 0.9615 - 14s/epoch - 55ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2647 - acc: 0.8933 - auc: 0.9558 - val_loss: 0.2518 - val_acc: 0.8954 - val_auc: 0.9626 - 14s/epoch - 56ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2627 - acc: 0.8945 - auc: 0.9567 - val_loss: 0.2521 - val_acc: 0.8943 - val_auc: 0.9624 - 14s/epoch - 56ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2593 - acc: 0.8965 - auc: 0.9575 - val_loss: 0.2543 - val_acc: 0.8922 - val_auc: 0.9615 - 14s/epoch - 56ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2584 - acc: 0.8952 - auc: 0.9577 - val_loss: 0.2516 - val_acc: 0.8932 - val_auc: 0.9632 - 15s/epoch - 56ms/step
Epoch 14/100
258/258 - 14s - loss: 0.2575 - acc: 0.8962 - auc: 0.9586 - val_loss: 0.2612 - val_acc: 0.8932 - val_auc: 0.9611 - 14s/epoch - 56ms/step
Epoch 15/100
258/258 - 14s - loss: 0.2537 - acc: 0.8967 - auc: 0.9598 - val_loss: 0.2541 - val_acc: 0.9031 - val_auc: 0.9617 - 14s/epoch - 56ms/step
Epoch 16/100
258/258 - 14s - loss: 0.2546 - acc: 0.8970 - auc: 0.9596 - val_loss: 0.2472 - val_acc: 0.9020 - val_auc: 0.9638 - 14s/epoch - 56ms/step
Epoch 17/100
258/258 - 14s - loss: 0.2514 - acc: 0.8977 - auc: 0.9605 - val_loss: 0.2479 - val_acc: 0.8976 - val_auc: 0.9642 - 14s/epoch - 56ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2503 - acc: 0.8993 - auc: 0.9609 - val_loss: 0.2467 - val_acc: 0.9009 - val_auc: 0.9648 - 14s/epoch - 56ms/step
Epoch 19/100
258/258 - 14s - loss: 0.2445 - acc: 0.9010 - auc: 0.9632 - val_loss: 0.2578 - val_acc: 0.8922 - val_auc: 0.9632 - 14s/epoch - 56ms/step
Epoch 20/100
258/258 - 14s - loss: 0.2411 - acc: 0.9026 - auc: 0.9633 - val_loss: 0.2568 - val_acc: 0.8954 - val_auc: 0.9609 - 14s/epoch - 55ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2405 - acc: 0.9047 - auc: 0.9642 - val_loss: 0.2447 - val_acc: 0.8954 - val_auc: 0.9646 - 14s/epoch - 55ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2363 - acc: 0.9061 - auc: 0.9655 - val_loss: 0.2517 - val_acc: 0.8889 - val_auc: 0.9623 - 14s/epoch - 55ms/step
Epoch 23/100
258/258 - 14s - loss: 0.2350 - acc: 0.9034 - auc: 0.9659 - val_loss: 0.2646 - val_acc: 0.8900 - val_auc: 0.9567 - 14s/epoch - 56ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2288 - acc: 0.9048 - auc: 0.9677 - val_loss: 0.2558 - val_acc: 0.8954 - val_auc: 0.9626 - 14s/epoch - 55ms/step
Epoch 25/100
258/258 - 14s - loss: 0.2256 - acc: 0.9107 - auc: 0.9686 - val_loss: 0.2781 - val_acc: 0.8845 - val_auc: 0.9540 - 14s/epoch - 56ms/step
Epoch 26/100
258/258 - 14s - loss: 0.2269 - acc: 0.9090 - auc: 0.9680 - val_loss: 0.2537 - val_acc: 0.8922 - val_auc: 0.9634 - 14s/epoch - 55ms/step
Epoch 27/100
258/258 - 14s - loss: 0.2202 - acc: 0.9124 - auc: 0.9701 - val_loss: 0.2636 - val_acc: 0.8867 - val_auc: 0.9611 - 14s/epoch - 55ms/step
Epoch 28/100
258/258 - 14s - loss: 0.2165 - acc: 0.9095 - auc: 0.9714 - val_loss: 0.2732 - val_acc: 0.8954 - val_auc: 0.9606 - 14s/epoch - 55ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
29/29 - 1s - 689ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.89      0.87      0.88       386
           1       0.91      0.92      0.92       532

    accuracy                           0.90       918
   macro avg       0.90      0.90      0.90       918
weighted avg       0.90      0.90      0.90       918

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.52      0.47       386
           1       0.58      0.48      0.53       532

    accuracy                           0.50       918
   macro avg       0.50      0.50      0.50       918
weighted avg       0.51      0.50      0.50       918

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3229 - acc: 0.8634 - auc: 0.9346 - val_loss: 0.3413 - val_acc: 0.8451 - val_auc: 0.9273 - 17s/epoch - 65ms/step
Epoch 2/100
258/258 - 14s - loss: 0.2905 - acc: 0.8850 - auc: 0.9457 - val_loss: 0.3476 - val_acc: 0.8506 - val_auc: 0.9349 - 14s/epoch - 56ms/step
Epoch 3/100
258/258 - 15s - loss: 0.2816 - acc: 0.8900 - auc: 0.9490 - val_loss: 0.3088 - val_acc: 0.8659 - val_auc: 0.9451 - 15s/epoch - 56ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2780 - acc: 0.8860 - auc: 0.9506 - val_loss: 0.3034 - val_acc: 0.8757 - val_auc: 0.9447 - 14s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2700 - acc: 0.8901 - auc: 0.9541 - val_loss: 0.3028 - val_acc: 0.8735 - val_auc: 0.9453 - 14s/epoch - 56ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2698 - acc: 0.8917 - auc: 0.9534 - val_loss: 0.3069 - val_acc: 0.8680 - val_auc: 0.9436 - 14s/epoch - 55ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2674 - acc: 0.8929 - auc: 0.9549 - val_loss: 0.3011 - val_acc: 0.8680 - val_auc: 0.9433 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2657 - acc: 0.8939 - auc: 0.9552 - val_loss: 0.3117 - val_acc: 0.8648 - val_auc: 0.9404 - 14s/epoch - 56ms/step
Epoch 9/100
258/258 - 15s - loss: 0.2637 - acc: 0.8951 - auc: 0.9569 - val_loss: 0.2857 - val_acc: 0.8800 - val_auc: 0.9516 - 15s/epoch - 56ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2613 - acc: 0.8941 - auc: 0.9571 - val_loss: 0.2908 - val_acc: 0.8779 - val_auc: 0.9501 - 14s/epoch - 56ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2592 - acc: 0.8949 - auc: 0.9578 - val_loss: 0.2915 - val_acc: 0.8779 - val_auc: 0.9497 - 14s/epoch - 56ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2558 - acc: 0.8957 - auc: 0.9590 - val_loss: 0.2901 - val_acc: 0.8702 - val_auc: 0.9488 - 14s/epoch - 56ms/step
Epoch 13/100
258/258 - 14s - loss: 0.2537 - acc: 0.8980 - auc: 0.9599 - val_loss: 0.2865 - val_acc: 0.8691 - val_auc: 0.9507 - 14s/epoch - 56ms/step
Epoch 14/100
258/258 - 15s - loss: 0.2515 - acc: 0.8968 - auc: 0.9609 - val_loss: 0.2852 - val_acc: 0.8779 - val_auc: 0.9519 - 15s/epoch - 56ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2506 - acc: 0.8963 - auc: 0.9610 - val_loss: 0.2831 - val_acc: 0.8779 - val_auc: 0.9514 - 15s/epoch - 56ms/step
Epoch 16/100
258/258 - 15s - loss: 0.2495 - acc: 0.8988 - auc: 0.9614 - val_loss: 0.3191 - val_acc: 0.8659 - val_auc: 0.9452 - 15s/epoch - 57ms/step
Epoch 17/100
258/258 - 15s - loss: 0.2469 - acc: 0.9001 - auc: 0.9627 - val_loss: 0.2809 - val_acc: 0.8702 - val_auc: 0.9538 - 15s/epoch - 56ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2413 - acc: 0.9018 - auc: 0.9645 - val_loss: 0.3048 - val_acc: 0.8680 - val_auc: 0.9461 - 14s/epoch - 56ms/step
Epoch 19/100
258/258 - 14s - loss: 0.2439 - acc: 0.9014 - auc: 0.9635 - val_loss: 0.2741 - val_acc: 0.8757 - val_auc: 0.9558 - 14s/epoch - 56ms/step
Epoch 20/100
258/258 - 14s - loss: 0.2372 - acc: 0.9010 - auc: 0.9656 - val_loss: 0.2732 - val_acc: 0.8790 - val_auc: 0.9540 - 14s/epoch - 55ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2355 - acc: 0.9060 - auc: 0.9659 - val_loss: 0.3210 - val_acc: 0.8702 - val_auc: 0.9399 - 14s/epoch - 56ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2316 - acc: 0.9060 - auc: 0.9672 - val_loss: 0.2922 - val_acc: 0.8779 - val_auc: 0.9486 - 14s/epoch - 56ms/step
Epoch 23/100
258/258 - 14s - loss: 0.2274 - acc: 0.9066 - auc: 0.9685 - val_loss: 0.2980 - val_acc: 0.8691 - val_auc: 0.9470 - 14s/epoch - 56ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2250 - acc: 0.9096 - auc: 0.9689 - val_loss: 0.3050 - val_acc: 0.8691 - val_auc: 0.9419 - 14s/epoch - 56ms/step
Epoch 25/100
258/258 - 15s - loss: 0.2180 - acc: 0.9100 - auc: 0.9709 - val_loss: 0.2957 - val_acc: 0.8757 - val_auc: 0.9490 - 15s/epoch - 56ms/step
Epoch 26/100
258/258 - 14s - loss: 0.2163 - acc: 0.9107 - auc: 0.9715 - val_loss: 0.3054 - val_acc: 0.8670 - val_auc: 0.9445 - 14s/epoch - 56ms/step
Epoch 27/100
258/258 - 14s - loss: 0.2075 - acc: 0.9152 - auc: 0.9742 - val_loss: 0.2918 - val_acc: 0.8735 - val_auc: 0.9497 - 14s/epoch - 56ms/step
Epoch 28/100
258/258 - 15s - loss: 0.2035 - acc: 0.9175 - auc: 0.9747 - val_loss: 0.3094 - val_acc: 0.8779 - val_auc: 0.9432 - 15s/epoch - 56ms/step
Epoch 29/100
258/258 - 15s - loss: 0.2019 - acc: 0.9194 - auc: 0.9755 - val_loss: 0.3004 - val_acc: 0.8735 - val_auc: 0.9450 - 15s/epoch - 56ms/step
Early stopping epoch: 28
******Evaluating TEST set*********
29/29 - 1s - 689ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.89      0.81      0.84       385
           1       0.87      0.93      0.90       532

    accuracy                           0.88       917
   macro avg       0.88      0.87      0.87       917
weighted avg       0.88      0.88      0.87       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.50      0.46       385
           1       0.58      0.50      0.54       532

    accuracy                           0.50       917
   macro avg       0.50      0.50      0.50       917
weighted avg       0.52      0.50      0.51       917

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3301 - acc: 0.8552 - auc: 0.9315 - val_loss: 0.2961 - val_acc: 0.8757 - val_auc: 0.9435 - 17s/epoch - 65ms/step
Epoch 2/100
258/258 - 15s - loss: 0.3016 - acc: 0.8749 - auc: 0.9421 - val_loss: 0.2947 - val_acc: 0.8833 - val_auc: 0.9459 - 15s/epoch - 56ms/step
Epoch 3/100
258/258 - 15s - loss: 0.2929 - acc: 0.8832 - auc: 0.9446 - val_loss: 0.2845 - val_acc: 0.8779 - val_auc: 0.9488 - 15s/epoch - 56ms/step
Epoch 4/100
258/258 - 15s - loss: 0.2842 - acc: 0.8856 - auc: 0.9481 - val_loss: 0.2787 - val_acc: 0.8768 - val_auc: 0.9522 - 15s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2791 - acc: 0.8877 - auc: 0.9494 - val_loss: 0.2930 - val_acc: 0.8899 - val_auc: 0.9489 - 14s/epoch - 56ms/step
Epoch 6/100
258/258 - 15s - loss: 0.2779 - acc: 0.8863 - auc: 0.9508 - val_loss: 0.2682 - val_acc: 0.8866 - val_auc: 0.9557 - 15s/epoch - 56ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2744 - acc: 0.8892 - auc: 0.9518 - val_loss: 0.2749 - val_acc: 0.8833 - val_auc: 0.9550 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2730 - acc: 0.8907 - auc: 0.9522 - val_loss: 0.2629 - val_acc: 0.8888 - val_auc: 0.9572 - 14s/epoch - 56ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2685 - acc: 0.8917 - auc: 0.9539 - val_loss: 0.2688 - val_acc: 0.8899 - val_auc: 0.9565 - 14s/epoch - 56ms/step
Epoch 10/100
258/258 - 15s - loss: 0.2639 - acc: 0.8940 - auc: 0.9563 - val_loss: 0.2605 - val_acc: 0.8975 - val_auc: 0.9576 - 15s/epoch - 56ms/step
Epoch 11/100
258/258 - 15s - loss: 0.2652 - acc: 0.8932 - auc: 0.9557 - val_loss: 0.2616 - val_acc: 0.8899 - val_auc: 0.9591 - 15s/epoch - 56ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2616 - acc: 0.8935 - auc: 0.9570 - val_loss: 0.2572 - val_acc: 0.8909 - val_auc: 0.9596 - 14s/epoch - 56ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2593 - acc: 0.8967 - auc: 0.9578 - val_loss: 0.2676 - val_acc: 0.8833 - val_auc: 0.9578 - 15s/epoch - 56ms/step
Epoch 14/100
258/258 - 14s - loss: 0.2574 - acc: 0.8962 - auc: 0.9576 - val_loss: 0.2681 - val_acc: 0.8844 - val_auc: 0.9577 - 14s/epoch - 56ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2593 - acc: 0.8962 - auc: 0.9577 - val_loss: 0.2565 - val_acc: 0.8931 - val_auc: 0.9604 - 15s/epoch - 57ms/step
Epoch 16/100
258/258 - 14s - loss: 0.2531 - acc: 0.8957 - auc: 0.9596 - val_loss: 0.2612 - val_acc: 0.8866 - val_auc: 0.9584 - 14s/epoch - 56ms/step
Epoch 17/100
258/258 - 15s - loss: 0.2515 - acc: 0.8968 - auc: 0.9604 - val_loss: 0.2649 - val_acc: 0.8920 - val_auc: 0.9584 - 15s/epoch - 56ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2468 - acc: 0.9007 - auc: 0.9620 - val_loss: 0.2653 - val_acc: 0.8899 - val_auc: 0.9581 - 14s/epoch - 56ms/step
Epoch 19/100
258/258 - 15s - loss: 0.2477 - acc: 0.8965 - auc: 0.9622 - val_loss: 0.2550 - val_acc: 0.8888 - val_auc: 0.9600 - 15s/epoch - 56ms/step
Epoch 20/100
258/258 - 15s - loss: 0.2442 - acc: 0.8998 - auc: 0.9632 - val_loss: 0.2556 - val_acc: 0.8975 - val_auc: 0.9601 - 15s/epoch - 57ms/step
Epoch 21/100
258/258 - 15s - loss: 0.2410 - acc: 0.9024 - auc: 0.9643 - val_loss: 0.2554 - val_acc: 0.8931 - val_auc: 0.9607 - 15s/epoch - 56ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2396 - acc: 0.9024 - auc: 0.9646 - val_loss: 0.2530 - val_acc: 0.8986 - val_auc: 0.9602 - 14s/epoch - 56ms/step
Epoch 23/100
258/258 - 15s - loss: 0.2343 - acc: 0.9042 - auc: 0.9663 - val_loss: 0.2583 - val_acc: 0.8909 - val_auc: 0.9604 - 15s/epoch - 57ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2317 - acc: 0.9050 - auc: 0.9674 - val_loss: 0.2704 - val_acc: 0.8877 - val_auc: 0.9590 - 14s/epoch - 56ms/step
Epoch 25/100
258/258 - 15s - loss: 0.2262 - acc: 0.9095 - auc: 0.9686 - val_loss: 0.2538 - val_acc: 0.8953 - val_auc: 0.9620 - 15s/epoch - 57ms/step
Epoch 26/100
258/258 - 15s - loss: 0.2246 - acc: 0.9084 - auc: 0.9689 - val_loss: 0.2476 - val_acc: 0.9019 - val_auc: 0.9629 - 15s/epoch - 57ms/step
Epoch 27/100
258/258 - 15s - loss: 0.2213 - acc: 0.9089 - auc: 0.9700 - val_loss: 0.2530 - val_acc: 0.8920 - val_auc: 0.9617 - 15s/epoch - 57ms/step
Epoch 28/100
258/258 - 15s - loss: 0.2150 - acc: 0.9131 - auc: 0.9718 - val_loss: 0.2659 - val_acc: 0.8920 - val_auc: 0.9583 - 15s/epoch - 56ms/step
Epoch 29/100
258/258 - 15s - loss: 0.2139 - acc: 0.9133 - auc: 0.9722 - val_loss: 0.2491 - val_acc: 0.8986 - val_auc: 0.9643 - 15s/epoch - 57ms/step
Epoch 30/100
258/258 - 15s - loss: 0.2111 - acc: 0.9150 - auc: 0.9727 - val_loss: 0.2547 - val_acc: 0.8942 - val_auc: 0.9637 - 15s/epoch - 56ms/step
Epoch 31/100
258/258 - 15s - loss: 0.2026 - acc: 0.9164 - auc: 0.9749 - val_loss: 0.2731 - val_acc: 0.8909 - val_auc: 0.9570 - 15s/epoch - 57ms/step
Epoch 32/100
258/258 - 15s - loss: 0.1965 - acc: 0.9204 - auc: 0.9765 - val_loss: 0.2589 - val_acc: 0.8975 - val_auc: 0.9603 - 15s/epoch - 56ms/step
Epoch 33/100
258/258 - 15s - loss: 0.1921 - acc: 0.9234 - auc: 0.9773 - val_loss: 0.2937 - val_acc: 0.8844 - val_auc: 0.9515 - 15s/epoch - 56ms/step
Epoch 34/100
258/258 - 14s - loss: 0.1877 - acc: 0.9221 - auc: 0.9791 - val_loss: 0.2746 - val_acc: 0.8986 - val_auc: 0.9581 - 14s/epoch - 56ms/step
Epoch 35/100
258/258 - 14s - loss: 0.1806 - acc: 0.9268 - auc: 0.9804 - val_loss: 0.2806 - val_acc: 0.8888 - val_auc: 0.9588 - 14s/epoch - 56ms/step
Epoch 36/100
258/258 - 14s - loss: 0.1729 - acc: 0.9294 - auc: 0.9821 - val_loss: 0.2733 - val_acc: 0.8986 - val_auc: 0.9576 - 14s/epoch - 56ms/step
Epoch 37/100
258/258 - 14s - loss: 0.1696 - acc: 0.9310 - auc: 0.9825 - val_loss: 0.2912 - val_acc: 0.8877 - val_auc: 0.9515 - 14s/epoch - 55ms/step
Epoch 38/100
258/258 - 14s - loss: 0.1635 - acc: 0.9358 - auc: 0.9838 - val_loss: 0.3089 - val_acc: 0.8909 - val_auc: 0.9474 - 14s/epoch - 56ms/step
Epoch 39/100
258/258 - 14s - loss: 0.1591 - acc: 0.9375 - auc: 0.9846 - val_loss: 0.2975 - val_acc: 0.8888 - val_auc: 0.9532 - 14s/epoch - 56ms/step
Early stopping epoch: 38
******Evaluating TEST set*********
29/29 - 1s - 721ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.88      0.88      0.88       385
           1       0.91      0.91      0.91       532

    accuracy                           0.90       917
   macro avg       0.90      0.90      0.90       917
weighted avg       0.90      0.90      0.90       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.41      0.48      0.44       385
           1       0.57      0.50      0.53       532

    accuracy                           0.49       917
   macro avg       0.49      0.49      0.49       917
weighted avg       0.50      0.49      0.49       917

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3311 - acc: 0.8566 - auc: 0.9319 - val_loss: 0.3061 - val_acc: 0.8800 - val_auc: 0.9400 - 17s/epoch - 66ms/step
Epoch 2/100
258/258 - 15s - loss: 0.2987 - acc: 0.8785 - auc: 0.9426 - val_loss: 0.3120 - val_acc: 0.8691 - val_auc: 0.9399 - 15s/epoch - 57ms/step
Epoch 3/100
258/258 - 15s - loss: 0.2842 - acc: 0.8881 - auc: 0.9482 - val_loss: 0.2966 - val_acc: 0.8822 - val_auc: 0.9464 - 15s/epoch - 57ms/step
Epoch 4/100
258/258 - 15s - loss: 0.2775 - acc: 0.8882 - auc: 0.9507 - val_loss: 0.2866 - val_acc: 0.8724 - val_auc: 0.9504 - 15s/epoch - 57ms/step
Epoch 5/100
258/258 - 15s - loss: 0.2723 - acc: 0.8912 - auc: 0.9524 - val_loss: 0.2945 - val_acc: 0.8877 - val_auc: 0.9505 - 15s/epoch - 57ms/step
Epoch 6/100
258/258 - 15s - loss: 0.2692 - acc: 0.8925 - auc: 0.9541 - val_loss: 0.2745 - val_acc: 0.8822 - val_auc: 0.9547 - 15s/epoch - 57ms/step
Epoch 7/100
258/258 - 15s - loss: 0.2692 - acc: 0.8929 - auc: 0.9541 - val_loss: 0.2779 - val_acc: 0.8822 - val_auc: 0.9530 - 15s/epoch - 56ms/step
Epoch 8/100
258/258 - 15s - loss: 0.2691 - acc: 0.8923 - auc: 0.9543 - val_loss: 0.2882 - val_acc: 0.8790 - val_auc: 0.9510 - 15s/epoch - 57ms/step
Epoch 9/100
258/258 - 15s - loss: 0.2638 - acc: 0.8934 - auc: 0.9563 - val_loss: 0.2774 - val_acc: 0.8844 - val_auc: 0.9553 - 15s/epoch - 57ms/step
Epoch 10/100
258/258 - 15s - loss: 0.2628 - acc: 0.8927 - auc: 0.9566 - val_loss: 0.2847 - val_acc: 0.8866 - val_auc: 0.9514 - 15s/epoch - 56ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2603 - acc: 0.8955 - auc: 0.9577 - val_loss: 0.2795 - val_acc: 0.8822 - val_auc: 0.9524 - 14s/epoch - 56ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2564 - acc: 0.8938 - auc: 0.9588 - val_loss: 0.2748 - val_acc: 0.8790 - val_auc: 0.9548 - 14s/epoch - 56ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2555 - acc: 0.8988 - auc: 0.9590 - val_loss: 0.2943 - val_acc: 0.8822 - val_auc: 0.9482 - 15s/epoch - 56ms/step
Epoch 14/100
258/258 - 14s - loss: 0.2508 - acc: 0.8955 - auc: 0.9612 - val_loss: 0.2757 - val_acc: 0.8855 - val_auc: 0.9539 - 14s/epoch - 56ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2496 - acc: 0.8972 - auc: 0.9611 - val_loss: 0.2782 - val_acc: 0.8877 - val_auc: 0.9542 - 15s/epoch - 56ms/step
Epoch 16/100
258/258 - 14s - loss: 0.2464 - acc: 0.8976 - auc: 0.9625 - val_loss: 0.2776 - val_acc: 0.8844 - val_auc: 0.9539 - 14s/epoch - 55ms/step
Epoch 17/100
258/258 - 14s - loss: 0.2419 - acc: 0.9002 - auc: 0.9639 - val_loss: 0.2747 - val_acc: 0.8899 - val_auc: 0.9535 - 14s/epoch - 55ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2429 - acc: 0.9021 - auc: 0.9640 - val_loss: 0.2814 - val_acc: 0.8844 - val_auc: 0.9523 - 14s/epoch - 55ms/step
Epoch 19/100
258/258 - 14s - loss: 0.2361 - acc: 0.9051 - auc: 0.9657 - val_loss: 0.2817 - val_acc: 0.8920 - val_auc: 0.9537 - 14s/epoch - 56ms/step
Early stopping epoch: 18
******Evaluating TEST set*********
29/29 - 1s - 697ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.87      0.86      0.86       386
           1       0.90      0.90      0.90       531

    accuracy                           0.88       917
   macro avg       0.88      0.88      0.88       917
weighted avg       0.88      0.88      0.88       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.43      0.49      0.46       386
           1       0.59      0.53      0.56       531

    accuracy                           0.51       917
   macro avg       0.51      0.51      0.51       917
weighted avg       0.52      0.51      0.52       917

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3295 - acc: 0.8584 - auc: 0.9321 - val_loss: 0.2606 - val_acc: 0.8942 - val_auc: 0.9559 - 17s/epoch - 65ms/step
Epoch 2/100
258/258 - 14s - loss: 0.2995 - acc: 0.8780 - auc: 0.9424 - val_loss: 0.2500 - val_acc: 0.8931 - val_auc: 0.9601 - 14s/epoch - 56ms/step
Epoch 3/100
258/258 - 14s - loss: 0.2860 - acc: 0.8853 - auc: 0.9474 - val_loss: 0.2430 - val_acc: 0.8975 - val_auc: 0.9629 - 14s/epoch - 56ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2825 - acc: 0.8849 - auc: 0.9490 - val_loss: 0.2439 - val_acc: 0.8986 - val_auc: 0.9633 - 14s/epoch - 55ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2804 - acc: 0.8856 - auc: 0.9501 - val_loss: 0.2391 - val_acc: 0.9062 - val_auc: 0.9644 - 14s/epoch - 55ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2803 - acc: 0.8879 - auc: 0.9495 - val_loss: 0.2434 - val_acc: 0.9051 - val_auc: 0.9650 - 14s/epoch - 55ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2736 - acc: 0.8871 - auc: 0.9527 - val_loss: 0.2434 - val_acc: 0.9019 - val_auc: 0.9654 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2728 - acc: 0.8906 - auc: 0.9529 - val_loss: 0.2377 - val_acc: 0.9084 - val_auc: 0.9644 - 14s/epoch - 55ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2686 - acc: 0.8921 - auc: 0.9547 - val_loss: 0.2449 - val_acc: 0.9095 - val_auc: 0.9665 - 14s/epoch - 56ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2657 - acc: 0.8911 - auc: 0.9561 - val_loss: 0.2327 - val_acc: 0.9117 - val_auc: 0.9666 - 14s/epoch - 56ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2669 - acc: 0.8946 - auc: 0.9550 - val_loss: 0.2308 - val_acc: 0.9029 - val_auc: 0.9673 - 14s/epoch - 55ms/step
Epoch 12/100
258/258 - 15s - loss: 0.2596 - acc: 0.8934 - auc: 0.9581 - val_loss: 0.2462 - val_acc: 0.8986 - val_auc: 0.9637 - 15s/epoch - 57ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2620 - acc: 0.8940 - auc: 0.9570 - val_loss: 0.2286 - val_acc: 0.9073 - val_auc: 0.9682 - 15s/epoch - 57ms/step
Epoch 14/100
258/258 - 14s - loss: 0.2587 - acc: 0.8934 - auc: 0.9581 - val_loss: 0.2265 - val_acc: 0.9029 - val_auc: 0.9680 - 14s/epoch - 56ms/step
Epoch 15/100
258/258 - 14s - loss: 0.2541 - acc: 0.8997 - auc: 0.9597 - val_loss: 0.2300 - val_acc: 0.9062 - val_auc: 0.9672 - 14s/epoch - 55ms/step
Epoch 16/100
258/258 - 15s - loss: 0.2520 - acc: 0.8969 - auc: 0.9605 - val_loss: 0.2418 - val_acc: 0.9008 - val_auc: 0.9677 - 15s/epoch - 57ms/step
Epoch 17/100
258/258 - 15s - loss: 0.2506 - acc: 0.8990 - auc: 0.9612 - val_loss: 0.2928 - val_acc: 0.8855 - val_auc: 0.9516 - 15s/epoch - 57ms/step
Epoch 18/100
258/258 - 15s - loss: 0.2511 - acc: 0.8976 - auc: 0.9609 - val_loss: 0.2394 - val_acc: 0.8986 - val_auc: 0.9653 - 15s/epoch - 57ms/step
Epoch 19/100
258/258 - 15s - loss: 0.2483 - acc: 0.8998 - auc: 0.9621 - val_loss: 0.2420 - val_acc: 0.8997 - val_auc: 0.9627 - 15s/epoch - 57ms/step
Epoch 20/100
258/258 - 15s - loss: 0.2422 - acc: 0.9005 - auc: 0.9637 - val_loss: 0.2351 - val_acc: 0.9008 - val_auc: 0.9672 - 15s/epoch - 56ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2406 - acc: 0.9047 - auc: 0.9645 - val_loss: 0.2387 - val_acc: 0.9051 - val_auc: 0.9644 - 14s/epoch - 56ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2360 - acc: 0.9059 - auc: 0.9655 - val_loss: 0.2323 - val_acc: 0.9106 - val_auc: 0.9667 - 14s/epoch - 56ms/step
Epoch 23/100
258/258 - 14s - loss: 0.2316 - acc: 0.9042 - auc: 0.9667 - val_loss: 0.2406 - val_acc: 0.8975 - val_auc: 0.9639 - 14s/epoch - 55ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
29/29 - 1s - 693ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.86      0.89       386
           1       0.90      0.94      0.92       531

    accuracy                           0.91       917
   macro avg       0.91      0.90      0.90       917
weighted avg       0.91      0.91      0.91       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.44      0.55      0.49       386
           1       0.60      0.50      0.54       531

    accuracy                           0.52       917
   macro avg       0.52      0.52      0.52       917
weighted avg       0.54      0.52      0.52       917

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3235 - acc: 0.8579 - auc: 0.9349 - val_loss: 0.3570 - val_acc: 0.8539 - val_auc: 0.9183 - 17s/epoch - 66ms/step
Epoch 2/100
258/258 - 15s - loss: 0.2932 - acc: 0.8830 - auc: 0.9457 - val_loss: 0.3389 - val_acc: 0.8582 - val_auc: 0.9254 - 15s/epoch - 57ms/step
Epoch 3/100
258/258 - 15s - loss: 0.2858 - acc: 0.8863 - auc: 0.9477 - val_loss: 0.3198 - val_acc: 0.8844 - val_auc: 0.9328 - 15s/epoch - 57ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2760 - acc: 0.8878 - auc: 0.9517 - val_loss: 0.3353 - val_acc: 0.8670 - val_auc: 0.9274 - 14s/epoch - 55ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2725 - acc: 0.8916 - auc: 0.9530 - val_loss: 0.3083 - val_acc: 0.8866 - val_auc: 0.9346 - 14s/epoch - 56ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2690 - acc: 0.8924 - auc: 0.9544 - val_loss: 0.3162 - val_acc: 0.8811 - val_auc: 0.9333 - 14s/epoch - 55ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2688 - acc: 0.8915 - auc: 0.9545 - val_loss: 0.2988 - val_acc: 0.8909 - val_auc: 0.9397 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2656 - acc: 0.8929 - auc: 0.9555 - val_loss: 0.3089 - val_acc: 0.8942 - val_auc: 0.9383 - 14s/epoch - 56ms/step
Epoch 9/100
258/258 - 15s - loss: 0.2595 - acc: 0.8949 - auc: 0.9575 - val_loss: 0.2934 - val_acc: 0.8909 - val_auc: 0.9425 - 15s/epoch - 57ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2645 - acc: 0.8927 - auc: 0.9568 - val_loss: 0.2970 - val_acc: 0.8888 - val_auc: 0.9419 - 14s/epoch - 56ms/step
Epoch 11/100
258/258 - 15s - loss: 0.2579 - acc: 0.8952 - auc: 0.9585 - val_loss: 0.2996 - val_acc: 0.8920 - val_auc: 0.9415 - 15s/epoch - 57ms/step
Epoch 12/100
258/258 - 15s - loss: 0.2549 - acc: 0.8944 - auc: 0.9595 - val_loss: 0.3069 - val_acc: 0.8779 - val_auc: 0.9390 - 15s/epoch - 57ms/step
Epoch 13/100
258/258 - 14s - loss: 0.2548 - acc: 0.8967 - auc: 0.9590 - val_loss: 0.3177 - val_acc: 0.8757 - val_auc: 0.9399 - 14s/epoch - 56ms/step
Epoch 14/100
258/258 - 15s - loss: 0.2519 - acc: 0.8969 - auc: 0.9608 - val_loss: 0.3010 - val_acc: 0.8909 - val_auc: 0.9448 - 15s/epoch - 57ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2510 - acc: 0.8976 - auc: 0.9606 - val_loss: 0.3018 - val_acc: 0.8757 - val_auc: 0.9424 - 15s/epoch - 58ms/step
Epoch 16/100
258/258 - 14s - loss: 0.2470 - acc: 0.8975 - auc: 0.9624 - val_loss: 0.3113 - val_acc: 0.8855 - val_auc: 0.9445 - 14s/epoch - 55ms/step
Epoch 17/100
258/258 - 14s - loss: 0.2445 - acc: 0.8991 - auc: 0.9634 - val_loss: 0.2978 - val_acc: 0.8779 - val_auc: 0.9461 - 14s/epoch - 56ms/step
Epoch 18/100
258/258 - 14s - loss: 0.2434 - acc: 0.9024 - auc: 0.9632 - val_loss: 0.3027 - val_acc: 0.8779 - val_auc: 0.9446 - 14s/epoch - 55ms/step
Epoch 19/100
258/258 - 14s - loss: 0.2424 - acc: 0.8987 - auc: 0.9637 - val_loss: 0.2901 - val_acc: 0.8877 - val_auc: 0.9457 - 14s/epoch - 55ms/step
Epoch 20/100
258/258 - 14s - loss: 0.2354 - acc: 0.9044 - auc: 0.9656 - val_loss: 0.2970 - val_acc: 0.8899 - val_auc: 0.9442 - 14s/epoch - 55ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2338 - acc: 0.9053 - auc: 0.9669 - val_loss: 0.3032 - val_acc: 0.8746 - val_auc: 0.9438 - 14s/epoch - 56ms/step
Epoch 22/100
258/258 - 15s - loss: 0.2292 - acc: 0.9070 - auc: 0.9672 - val_loss: 0.3093 - val_acc: 0.8855 - val_auc: 0.9443 - 15s/epoch - 57ms/step
Epoch 23/100
258/258 - 14s - loss: 0.2280 - acc: 0.9065 - auc: 0.9683 - val_loss: 0.3254 - val_acc: 0.8768 - val_auc: 0.9426 - 14s/epoch - 56ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2226 - acc: 0.9104 - auc: 0.9694 - val_loss: 0.3067 - val_acc: 0.8855 - val_auc: 0.9481 - 14s/epoch - 56ms/step
Epoch 25/100
258/258 - 14s - loss: 0.2214 - acc: 0.9105 - auc: 0.9699 - val_loss: 0.3078 - val_acc: 0.8833 - val_auc: 0.9445 - 14s/epoch - 56ms/step
Epoch 26/100
258/258 - 15s - loss: 0.2185 - acc: 0.9122 - auc: 0.9704 - val_loss: 0.3156 - val_acc: 0.8800 - val_auc: 0.9439 - 15s/epoch - 57ms/step
Epoch 27/100
258/258 - 15s - loss: 0.2096 - acc: 0.9168 - auc: 0.9735 - val_loss: 0.3197 - val_acc: 0.8822 - val_auc: 0.9398 - 15s/epoch - 57ms/step
Epoch 28/100
258/258 - 15s - loss: 0.2040 - acc: 0.9190 - auc: 0.9743 - val_loss: 0.3281 - val_acc: 0.8724 - val_auc: 0.9402 - 15s/epoch - 56ms/step
Epoch 29/100
258/258 - 14s - loss: 0.2006 - acc: 0.9194 - auc: 0.9754 - val_loss: 0.3270 - val_acc: 0.8702 - val_auc: 0.9430 - 14s/epoch - 56ms/step
Epoch 30/100
258/258 - 15s - loss: 0.1911 - acc: 0.9226 - auc: 0.9774 - val_loss: 0.3321 - val_acc: 0.8811 - val_auc: 0.9379 - 15s/epoch - 57ms/step
Epoch 31/100
258/258 - 15s - loss: 0.1926 - acc: 0.9225 - auc: 0.9765 - val_loss: 0.3084 - val_acc: 0.8877 - val_auc: 0.9426 - 15s/epoch - 57ms/step
Epoch 32/100
258/258 - 15s - loss: 0.1794 - acc: 0.9286 - auc: 0.9799 - val_loss: 0.3427 - val_acc: 0.8746 - val_auc: 0.9349 - 15s/epoch - 57ms/step
Epoch 33/100
258/258 - 15s - loss: 0.1729 - acc: 0.9329 - auc: 0.9810 - val_loss: 0.3475 - val_acc: 0.8768 - val_auc: 0.9361 - 15s/epoch - 56ms/step
Epoch 34/100
258/258 - 14s - loss: 0.1696 - acc: 0.9347 - auc: 0.9822 - val_loss: 0.3567 - val_acc: 0.8746 - val_auc: 0.9296 - 14s/epoch - 56ms/step
Early stopping epoch: 33
******Evaluating TEST set*********
29/29 - 1s - 716ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.91      0.81      0.86       386
           1       0.87      0.94      0.90       531

    accuracy                           0.89       917
   macro avg       0.89      0.88      0.88       917
weighted avg       0.89      0.89      0.88       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.44      0.54      0.49       386
           1       0.60      0.50      0.55       531

    accuracy                           0.52       917
   macro avg       0.52      0.52      0.52       917
weighted avg       0.53      0.52      0.52       917

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3270 - acc: 0.8600 - auc: 0.9333 - val_loss: 0.2848 - val_acc: 0.8855 - val_auc: 0.9485 - 17s/epoch - 64ms/step
Epoch 2/100
258/258 - 14s - loss: 0.2934 - acc: 0.8796 - auc: 0.9453 - val_loss: 0.2777 - val_acc: 0.8942 - val_auc: 0.9513 - 14s/epoch - 56ms/step
Epoch 3/100
258/258 - 14s - loss: 0.2858 - acc: 0.8831 - auc: 0.9481 - val_loss: 0.2781 - val_acc: 0.8899 - val_auc: 0.9511 - 14s/epoch - 55ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2820 - acc: 0.8856 - auc: 0.9490 - val_loss: 0.2717 - val_acc: 0.8877 - val_auc: 0.9557 - 14s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2769 - acc: 0.8879 - auc: 0.9511 - val_loss: 0.2856 - val_acc: 0.8833 - val_auc: 0.9481 - 14s/epoch - 55ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2743 - acc: 0.8879 - auc: 0.9517 - val_loss: 0.2772 - val_acc: 0.8866 - val_auc: 0.9534 - 14s/epoch - 56ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2685 - acc: 0.8917 - auc: 0.9543 - val_loss: 0.2773 - val_acc: 0.8909 - val_auc: 0.9548 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2662 - acc: 0.8915 - auc: 0.9551 - val_loss: 0.2760 - val_acc: 0.8942 - val_auc: 0.9552 - 14s/epoch - 55ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2642 - acc: 0.8922 - auc: 0.9557 - val_loss: 0.2730 - val_acc: 0.8888 - val_auc: 0.9555 - 14s/epoch - 56ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2603 - acc: 0.8940 - auc: 0.9576 - val_loss: 0.2778 - val_acc: 0.8866 - val_auc: 0.9535 - 14s/epoch - 55ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2591 - acc: 0.8959 - auc: 0.9580 - val_loss: 0.2796 - val_acc: 0.8811 - val_auc: 0.9534 - 14s/epoch - 56ms/step
Epoch 12/100
258/258 - 15s - loss: 0.2552 - acc: 0.8973 - auc: 0.9592 - val_loss: 0.2775 - val_acc: 0.8931 - val_auc: 0.9545 - 15s/epoch - 57ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2535 - acc: 0.8962 - auc: 0.9601 - val_loss: 0.2717 - val_acc: 0.8909 - val_auc: 0.9546 - 15s/epoch - 57ms/step
Epoch 14/100
258/258 - 15s - loss: 0.2522 - acc: 0.8941 - auc: 0.9606 - val_loss: 0.2741 - val_acc: 0.8877 - val_auc: 0.9559 - 15s/epoch - 58ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2486 - acc: 0.8993 - auc: 0.9612 - val_loss: 0.2897 - val_acc: 0.8964 - val_auc: 0.9544 - 15s/epoch - 57ms/step
Epoch 16/100
258/258 - 15s - loss: 0.2483 - acc: 0.9010 - auc: 0.9614 - val_loss: 0.3012 - val_acc: 0.8779 - val_auc: 0.9430 - 15s/epoch - 57ms/step
Epoch 17/100
258/258 - 14s - loss: 0.2438 - acc: 0.9005 - auc: 0.9635 - val_loss: 0.2956 - val_acc: 0.8899 - val_auc: 0.9461 - 14s/epoch - 56ms/step
Epoch 18/100
258/258 - 15s - loss: 0.2420 - acc: 0.9010 - auc: 0.9636 - val_loss: 0.3241 - val_acc: 0.8746 - val_auc: 0.9455 - 15s/epoch - 57ms/step
Epoch 19/100
258/258 - 15s - loss: 0.2376 - acc: 0.9038 - auc: 0.9652 - val_loss: 0.2806 - val_acc: 0.8855 - val_auc: 0.9546 - 15s/epoch - 57ms/step
Epoch 20/100
258/258 - 15s - loss: 0.2328 - acc: 0.9054 - auc: 0.9669 - val_loss: 0.2763 - val_acc: 0.8866 - val_auc: 0.9556 - 15s/epoch - 57ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2298 - acc: 0.9084 - auc: 0.9669 - val_loss: 0.2854 - val_acc: 0.8909 - val_auc: 0.9527 - 14s/epoch - 56ms/step
Epoch 22/100
258/258 - 15s - loss: 0.2260 - acc: 0.9073 - auc: 0.9686 - val_loss: 0.2889 - val_acc: 0.8877 - val_auc: 0.9529 - 15s/epoch - 57ms/step
Epoch 23/100
258/258 - 15s - loss: 0.2214 - acc: 0.9096 - auc: 0.9700 - val_loss: 0.2833 - val_acc: 0.8811 - val_auc: 0.9531 - 15s/epoch - 56ms/step
Epoch 24/100
258/258 - 15s - loss: 0.2158 - acc: 0.9116 - auc: 0.9717 - val_loss: 0.2914 - val_acc: 0.8877 - val_auc: 0.9531 - 15s/epoch - 57ms/step
Early stopping epoch: 23
******Evaluating TEST set*********
29/29 - 1s - 700ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.89      0.83      0.86       386
           1       0.88      0.93      0.91       531

    accuracy                           0.89       917
   macro avg       0.89      0.88      0.88       917
weighted avg       0.89      0.89      0.89       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.45      0.41       386
           1       0.54      0.47      0.50       531

    accuracy                           0.46       917
   macro avg       0.46      0.46      0.45       917
weighted avg       0.47      0.46      0.46       917

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3273 - acc: 0.8601 - auc: 0.9335 - val_loss: 0.3225 - val_acc: 0.8648 - val_auc: 0.9309 - 17s/epoch - 67ms/step
Epoch 2/100
258/258 - 15s - loss: 0.2971 - acc: 0.8773 - auc: 0.9438 - val_loss: 0.3163 - val_acc: 0.8713 - val_auc: 0.9350 - 15s/epoch - 56ms/step
Epoch 3/100
258/258 - 14s - loss: 0.2890 - acc: 0.8837 - auc: 0.9467 - val_loss: 0.3241 - val_acc: 0.8757 - val_auc: 0.9339 - 14s/epoch - 56ms/step
Epoch 4/100
258/258 - 15s - loss: 0.2790 - acc: 0.8855 - auc: 0.9504 - val_loss: 0.3016 - val_acc: 0.8811 - val_auc: 0.9382 - 15s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2759 - acc: 0.8888 - auc: 0.9517 - val_loss: 0.2927 - val_acc: 0.8822 - val_auc: 0.9408 - 14s/epoch - 56ms/step
Epoch 6/100
258/258 - 15s - loss: 0.2690 - acc: 0.8919 - auc: 0.9541 - val_loss: 0.2942 - val_acc: 0.8768 - val_auc: 0.9407 - 15s/epoch - 56ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2694 - acc: 0.8922 - auc: 0.9543 - val_loss: 0.2933 - val_acc: 0.8866 - val_auc: 0.9407 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2623 - acc: 0.8952 - auc: 0.9564 - val_loss: 0.2920 - val_acc: 0.8800 - val_auc: 0.9448 - 14s/epoch - 56ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2620 - acc: 0.8923 - auc: 0.9570 - val_loss: 0.2943 - val_acc: 0.8790 - val_auc: 0.9453 - 14s/epoch - 56ms/step
Epoch 10/100
258/258 - 15s - loss: 0.2612 - acc: 0.8957 - auc: 0.9571 - val_loss: 0.2846 - val_acc: 0.8768 - val_auc: 0.9461 - 15s/epoch - 56ms/step
Epoch 11/100
258/258 - 15s - loss: 0.2561 - acc: 0.8952 - auc: 0.9590 - val_loss: 0.2968 - val_acc: 0.8746 - val_auc: 0.9435 - 15s/epoch - 57ms/step
Epoch 12/100
258/258 - 15s - loss: 0.2557 - acc: 0.8962 - auc: 0.9593 - val_loss: 0.2898 - val_acc: 0.8768 - val_auc: 0.9462 - 15s/epoch - 58ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2561 - acc: 0.8953 - auc: 0.9595 - val_loss: 0.2820 - val_acc: 0.8779 - val_auc: 0.9500 - 15s/epoch - 57ms/step
Epoch 14/100
258/258 - 15s - loss: 0.2516 - acc: 0.8965 - auc: 0.9609 - val_loss: 0.2877 - val_acc: 0.8833 - val_auc: 0.9464 - 15s/epoch - 56ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2483 - acc: 0.8993 - auc: 0.9623 - val_loss: 0.3009 - val_acc: 0.8833 - val_auc: 0.9449 - 15s/epoch - 57ms/step
Epoch 16/100
258/258 - 15s - loss: 0.2497 - acc: 0.8975 - auc: 0.9615 - val_loss: 0.2904 - val_acc: 0.8822 - val_auc: 0.9487 - 15s/epoch - 57ms/step
Epoch 17/100
258/258 - 15s - loss: 0.2433 - acc: 0.9010 - auc: 0.9635 - val_loss: 0.2944 - val_acc: 0.8790 - val_auc: 0.9441 - 15s/epoch - 57ms/step
Epoch 18/100
258/258 - 15s - loss: 0.2431 - acc: 0.9022 - auc: 0.9634 - val_loss: 0.2830 - val_acc: 0.8735 - val_auc: 0.9502 - 15s/epoch - 57ms/step
Epoch 19/100
258/258 - 15s - loss: 0.2395 - acc: 0.9026 - auc: 0.9645 - val_loss: 0.2848 - val_acc: 0.8822 - val_auc: 0.9492 - 15s/epoch - 56ms/step
Epoch 20/100
258/258 - 15s - loss: 0.2367 - acc: 0.9021 - auc: 0.9655 - val_loss: 0.2892 - val_acc: 0.8746 - val_auc: 0.9472 - 15s/epoch - 56ms/step
Epoch 21/100
258/258 - 14s - loss: 0.2325 - acc: 0.9067 - auc: 0.9671 - val_loss: 0.2939 - val_acc: 0.8757 - val_auc: 0.9455 - 14s/epoch - 56ms/step
Epoch 22/100
258/258 - 15s - loss: 0.2324 - acc: 0.9044 - auc: 0.9672 - val_loss: 0.2864 - val_acc: 0.8822 - val_auc: 0.9482 - 15s/epoch - 57ms/step
Epoch 23/100
258/258 - 15s - loss: 0.2274 - acc: 0.9066 - auc: 0.9681 - val_loss: 0.3013 - val_acc: 0.8779 - val_auc: 0.9465 - 15s/epoch - 57ms/step
Epoch 24/100
258/258 - 15s - loss: 0.2234 - acc: 0.9076 - auc: 0.9697 - val_loss: 0.2953 - val_acc: 0.8702 - val_auc: 0.9469 - 15s/epoch - 57ms/step
Epoch 25/100
258/258 - 15s - loss: 0.2176 - acc: 0.9114 - auc: 0.9713 - val_loss: 0.2995 - val_acc: 0.8779 - val_auc: 0.9428 - 15s/epoch - 56ms/step
Epoch 26/100
258/258 - 14s - loss: 0.2219 - acc: 0.9089 - auc: 0.9700 - val_loss: 0.2955 - val_acc: 0.8757 - val_auc: 0.9463 - 14s/epoch - 56ms/step
Epoch 27/100
258/258 - 14s - loss: 0.2144 - acc: 0.9104 - auc: 0.9722 - val_loss: 0.2910 - val_acc: 0.8844 - val_auc: 0.9496 - 14s/epoch - 55ms/step
Epoch 28/100
258/258 - 14s - loss: 0.2092 - acc: 0.9130 - auc: 0.9734 - val_loss: 0.3033 - val_acc: 0.8855 - val_auc: 0.9465 - 14s/epoch - 55ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
29/29 - 1s - 698ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.87      0.82      0.84       386
           1       0.87      0.91      0.89       531

    accuracy                           0.87       917
   macro avg       0.87      0.87      0.87       917
weighted avg       0.87      0.87      0.87       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.44      0.49      0.47       386
           1       0.60      0.54      0.57       531

    accuracy                           0.52       917
   macro avg       0.52      0.52      0.52       917
weighted avg       0.53      0.52      0.52       917

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
258/258 - 17s - loss: 0.3352 - acc: 0.8573 - auc: 0.9291 - val_loss: 0.2715 - val_acc: 0.8899 - val_auc: 0.9529 - 17s/epoch - 65ms/step
Epoch 2/100
258/258 - 14s - loss: 0.3021 - acc: 0.8739 - auc: 0.9417 - val_loss: 0.2569 - val_acc: 0.8986 - val_auc: 0.9573 - 14s/epoch - 56ms/step
Epoch 3/100
258/258 - 14s - loss: 0.2900 - acc: 0.8832 - auc: 0.9465 - val_loss: 0.2627 - val_acc: 0.8997 - val_auc: 0.9611 - 14s/epoch - 56ms/step
Epoch 4/100
258/258 - 14s - loss: 0.2870 - acc: 0.8873 - auc: 0.9474 - val_loss: 0.2487 - val_acc: 0.9008 - val_auc: 0.9631 - 14s/epoch - 56ms/step
Epoch 5/100
258/258 - 14s - loss: 0.2787 - acc: 0.8878 - auc: 0.9504 - val_loss: 0.2420 - val_acc: 0.9062 - val_auc: 0.9632 - 14s/epoch - 56ms/step
Epoch 6/100
258/258 - 14s - loss: 0.2777 - acc: 0.8872 - auc: 0.9505 - val_loss: 0.2416 - val_acc: 0.9062 - val_auc: 0.9641 - 14s/epoch - 55ms/step
Epoch 7/100
258/258 - 14s - loss: 0.2711 - acc: 0.8913 - auc: 0.9536 - val_loss: 0.2345 - val_acc: 0.9062 - val_auc: 0.9661 - 14s/epoch - 56ms/step
Epoch 8/100
258/258 - 14s - loss: 0.2717 - acc: 0.8906 - auc: 0.9529 - val_loss: 0.2371 - val_acc: 0.9040 - val_auc: 0.9651 - 14s/epoch - 55ms/step
Epoch 9/100
258/258 - 14s - loss: 0.2677 - acc: 0.8904 - auc: 0.9556 - val_loss: 0.2540 - val_acc: 0.9019 - val_auc: 0.9600 - 14s/epoch - 56ms/step
Epoch 10/100
258/258 - 14s - loss: 0.2709 - acc: 0.8906 - auc: 0.9537 - val_loss: 0.2359 - val_acc: 0.9073 - val_auc: 0.9662 - 14s/epoch - 56ms/step
Epoch 11/100
258/258 - 14s - loss: 0.2654 - acc: 0.8902 - auc: 0.9563 - val_loss: 0.2394 - val_acc: 0.9084 - val_auc: 0.9638 - 14s/epoch - 55ms/step
Epoch 12/100
258/258 - 14s - loss: 0.2638 - acc: 0.8933 - auc: 0.9565 - val_loss: 0.2348 - val_acc: 0.9117 - val_auc: 0.9667 - 14s/epoch - 56ms/step
Epoch 13/100
258/258 - 15s - loss: 0.2592 - acc: 0.8940 - auc: 0.9582 - val_loss: 0.2432 - val_acc: 0.9019 - val_auc: 0.9636 - 15s/epoch - 56ms/step
Epoch 14/100
258/258 - 24s - loss: 0.2627 - acc: 0.8921 - auc: 0.9570 - val_loss: 0.2380 - val_acc: 0.9073 - val_auc: 0.9646 - 24s/epoch - 94ms/step
Epoch 15/100
258/258 - 15s - loss: 0.2570 - acc: 0.8940 - auc: 0.9592 - val_loss: 0.2305 - val_acc: 0.9040 - val_auc: 0.9653 - 15s/epoch - 57ms/step
Epoch 16/100
258/258 - 16s - loss: 0.2554 - acc: 0.8951 - auc: 0.9594 - val_loss: 0.2314 - val_acc: 0.9051 - val_auc: 0.9680 - 16s/epoch - 62ms/step
Epoch 17/100
258/258 - 15s - loss: 0.2531 - acc: 0.8951 - auc: 0.9604 - val_loss: 0.2620 - val_acc: 0.8920 - val_auc: 0.9574 - 15s/epoch - 57ms/step
Epoch 18/100
258/258 - 15s - loss: 0.2512 - acc: 0.8946 - auc: 0.9610 - val_loss: 0.2336 - val_acc: 0.9106 - val_auc: 0.9654 - 15s/epoch - 57ms/step
Epoch 19/100
258/258 - 15s - loss: 0.2489 - acc: 0.8969 - auc: 0.9615 - val_loss: 0.2369 - val_acc: 0.8964 - val_auc: 0.9660 - 15s/epoch - 56ms/step
Epoch 20/100
258/258 - 14s - loss: 0.2404 - acc: 0.9019 - auc: 0.9644 - val_loss: 0.2284 - val_acc: 0.9073 - val_auc: 0.9674 - 14s/epoch - 56ms/step
Epoch 21/100
258/258 - 15s - loss: 0.2382 - acc: 0.9026 - auc: 0.9649 - val_loss: 0.2422 - val_acc: 0.8997 - val_auc: 0.9618 - 15s/epoch - 56ms/step
Epoch 22/100
258/258 - 14s - loss: 0.2345 - acc: 0.9048 - auc: 0.9663 - val_loss: 0.2380 - val_acc: 0.9029 - val_auc: 0.9630 - 14s/epoch - 55ms/step
Epoch 23/100
258/258 - 19s - loss: 0.2373 - acc: 0.9021 - auc: 0.9656 - val_loss: 0.2357 - val_acc: 0.9019 - val_auc: 0.9658 - 19s/epoch - 74ms/step
Epoch 24/100
258/258 - 14s - loss: 0.2305 - acc: 0.9054 - auc: 0.9675 - val_loss: 0.2408 - val_acc: 0.9019 - val_auc: 0.9608 - 14s/epoch - 55ms/step
Epoch 25/100
258/258 - 14s - loss: 0.2240 - acc: 0.9111 - auc: 0.9688 - val_loss: 0.2278 - val_acc: 0.9106 - val_auc: 0.9676 - 14s/epoch - 55ms/step
Epoch 26/100
258/258 - 14s - loss: 0.2187 - acc: 0.9134 - auc: 0.9709 - val_loss: 0.2566 - val_acc: 0.8931 - val_auc: 0.9600 - 14s/epoch - 56ms/step
Early stopping epoch: 25
******Evaluating TEST set*********
29/29 - 2s - 2s/epoch - 52ms/step
              precision    recall  f1-score   support

           0       0.92      0.84      0.88       386
           1       0.89      0.95      0.92       531

    accuracy                           0.91       917
   macro avg       0.91      0.90      0.90       917
weighted avg       0.91      0.91      0.90       917

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.42      0.52      0.46       386
           1       0.58      0.47      0.52       531

    accuracy                           0.49       917
   macro avg       0.50      0.50      0.49       917
weighted avg       0.51      0.49      0.50       917

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.8915] IC [0.8846, 0.8984]
Mean Recall[0.8850] IC [0.8774, 0.8927]
Mean F1[0.8878] IC [0.8805, 0.8950]
Median Accuracy[0.8921]
Median Recall[0.8861]
Median F1[0.8885]
********************txid169963********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.83      0.65      0.73       172
           1       0.94      0.98      0.96      1031

    accuracy                           0.93      1203
   macro avg       0.89      0.81      0.84      1203
weighted avg       0.93      0.93      0.93      1203

Predicted  0.0   1.0   All
True                      
0          111    61   172
1           23  1008  1031
All        134  1069  1203
**************************************************
fold 0
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 146, 1, 64)        5824      
                                                                 
 lambda (Lambda)             (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention (SelfAttenti  ((None, 1024),           2560      
 on)                          (None, 16, 146))                   
                                                                 
 dense (Dense)               (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 19s - loss: 0.3161 - acc: 0.8608 - auc: 0.9370 - val_loss: 0.2550 - val_acc: 0.8971 - val_auc: 0.9592 - 19s/epoch - 67ms/step
Epoch 2/100
282/282 - 15s - loss: 0.2736 - acc: 0.8892 - auc: 0.9520 - val_loss: 0.2354 - val_acc: 0.9051 - val_auc: 0.9658 - 15s/epoch - 54ms/step
Epoch 3/100
282/282 - 15s - loss: 0.2692 - acc: 0.8925 - auc: 0.9532 - val_loss: 0.2358 - val_acc: 0.9071 - val_auc: 0.9657 - 15s/epoch - 54ms/step
Epoch 4/100
282/282 - 15s - loss: 0.2613 - acc: 0.8962 - auc: 0.9565 - val_loss: 0.2318 - val_acc: 0.8991 - val_auc: 0.9669 - 15s/epoch - 55ms/step
Epoch 5/100
282/282 - 15s - loss: 0.2561 - acc: 0.8985 - auc: 0.9586 - val_loss: 0.2280 - val_acc: 0.9061 - val_auc: 0.9686 - 15s/epoch - 55ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2544 - acc: 0.8979 - auc: 0.9593 - val_loss: 0.2249 - val_acc: 0.9071 - val_auc: 0.9696 - 16s/epoch - 55ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2538 - acc: 0.8991 - auc: 0.9591 - val_loss: 0.2427 - val_acc: 0.8971 - val_auc: 0.9662 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2501 - acc: 0.9001 - auc: 0.9609 - val_loss: 0.2256 - val_acc: 0.9091 - val_auc: 0.9698 - 16s/epoch - 55ms/step
Epoch 9/100
282/282 - 16s - loss: 0.2472 - acc: 0.9002 - auc: 0.9616 - val_loss: 0.2166 - val_acc: 0.9101 - val_auc: 0.9712 - 16s/epoch - 55ms/step
Epoch 10/100
282/282 - 16s - loss: 0.2471 - acc: 0.9007 - auc: 0.9613 - val_loss: 0.2125 - val_acc: 0.9131 - val_auc: 0.9726 - 16s/epoch - 57ms/step
Epoch 11/100
282/282 - 15s - loss: 0.2413 - acc: 0.9019 - auc: 0.9642 - val_loss: 0.2121 - val_acc: 0.9121 - val_auc: 0.9713 - 15s/epoch - 55ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2421 - acc: 0.9007 - auc: 0.9638 - val_loss: 0.2177 - val_acc: 0.9101 - val_auc: 0.9726 - 16s/epoch - 57ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2406 - acc: 0.9025 - auc: 0.9642 - val_loss: 0.2220 - val_acc: 0.9101 - val_auc: 0.9707 - 16s/epoch - 55ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2374 - acc: 0.9034 - auc: 0.9647 - val_loss: 0.2148 - val_acc: 0.9081 - val_auc: 0.9715 - 16s/epoch - 55ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2393 - acc: 0.9001 - auc: 0.9650 - val_loss: 0.2170 - val_acc: 0.9131 - val_auc: 0.9722 - 16s/epoch - 55ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2361 - acc: 0.9031 - auc: 0.9653 - val_loss: 0.2111 - val_acc: 0.9131 - val_auc: 0.9729 - 16s/epoch - 57ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2300 - acc: 0.9065 - auc: 0.9671 - val_loss: 0.2109 - val_acc: 0.9121 - val_auc: 0.9732 - 16s/epoch - 56ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2282 - acc: 0.9075 - auc: 0.9675 - val_loss: 0.2078 - val_acc: 0.9181 - val_auc: 0.9724 - 16s/epoch - 56ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2237 - acc: 0.9111 - auc: 0.9689 - val_loss: 0.2129 - val_acc: 0.9201 - val_auc: 0.9721 - 16s/epoch - 55ms/step
Epoch 20/100
282/282 - 15s - loss: 0.2217 - acc: 0.9091 - auc: 0.9696 - val_loss: 0.2165 - val_acc: 0.9061 - val_auc: 0.9706 - 15s/epoch - 55ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2203 - acc: 0.9087 - auc: 0.9703 - val_loss: 0.2079 - val_acc: 0.9221 - val_auc: 0.9738 - 16s/epoch - 55ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2140 - acc: 0.9144 - auc: 0.9721 - val_loss: 0.2256 - val_acc: 0.9111 - val_auc: 0.9692 - 16s/epoch - 56ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2134 - acc: 0.9137 - auc: 0.9719 - val_loss: 0.2159 - val_acc: 0.9201 - val_auc: 0.9711 - 16s/epoch - 56ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2055 - acc: 0.9159 - auc: 0.9744 - val_loss: 0.2205 - val_acc: 0.9161 - val_auc: 0.9690 - 16s/epoch - 56ms/step
Epoch 25/100
282/282 - 16s - loss: 0.2029 - acc: 0.9198 - auc: 0.9749 - val_loss: 0.2214 - val_acc: 0.9101 - val_auc: 0.9697 - 16s/epoch - 55ms/step
Epoch 26/100
282/282 - 15s - loss: 0.1981 - acc: 0.9189 - auc: 0.9760 - val_loss: 0.2242 - val_acc: 0.9151 - val_auc: 0.9679 - 15s/epoch - 55ms/step
Epoch 27/100
282/282 - 16s - loss: 0.1928 - acc: 0.9224 - auc: 0.9775 - val_loss: 0.2342 - val_acc: 0.9061 - val_auc: 0.9668 - 16s/epoch - 56ms/step
Epoch 28/100
282/282 - 16s - loss: 0.1904 - acc: 0.9236 - auc: 0.9777 - val_loss: 0.2278 - val_acc: 0.9141 - val_auc: 0.9684 - 16s/epoch - 56ms/step
Epoch 29/100
282/282 - 16s - loss: 0.1852 - acc: 0.9249 - auc: 0.9792 - val_loss: 0.2409 - val_acc: 0.9021 - val_auc: 0.9644 - 16s/epoch - 56ms/step
Epoch 30/100
282/282 - 16s - loss: 0.1786 - acc: 0.9301 - auc: 0.9800 - val_loss: 0.2342 - val_acc: 0.9091 - val_auc: 0.9693 - 16s/epoch - 56ms/step
Epoch 31/100
282/282 - 16s - loss: 0.1747 - acc: 0.9295 - auc: 0.9812 - val_loss: 0.2399 - val_acc: 0.9071 - val_auc: 0.9640 - 16s/epoch - 55ms/step
Early stopping epoch: 30
******Evaluating TEST set*********
32/32 - 1s - 768ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.90      0.90      0.90       391
           1       0.94      0.93      0.94       610

    accuracy                           0.92      1001
   macro avg       0.92      0.92      0.92      1001
weighted avg       0.92      0.92      0.92      1001

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.49      0.44       391
           1       0.61      0.51      0.56       610

    accuracy                           0.50      1001
   macro avg       0.50      0.50      0.50      1001
weighted avg       0.53      0.50      0.51      1001

______________________________________________________
fold 1
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_1 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_1 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_1 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_1 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3094 - acc: 0.8671 - auc: 0.9402 - val_loss: 0.2967 - val_acc: 0.8781 - val_auc: 0.9412 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2759 - acc: 0.8878 - auc: 0.9515 - val_loss: 0.2710 - val_acc: 0.8951 - val_auc: 0.9518 - 16s/epoch - 55ms/step
Epoch 3/100
282/282 - 15s - loss: 0.2622 - acc: 0.8945 - auc: 0.9560 - val_loss: 0.2738 - val_acc: 0.8951 - val_auc: 0.9516 - 15s/epoch - 55ms/step
Epoch 4/100
282/282 - 15s - loss: 0.2575 - acc: 0.8971 - auc: 0.9580 - val_loss: 0.2813 - val_acc: 0.8921 - val_auc: 0.9496 - 15s/epoch - 55ms/step
Epoch 5/100
282/282 - 15s - loss: 0.2542 - acc: 0.8988 - auc: 0.9587 - val_loss: 0.2603 - val_acc: 0.9011 - val_auc: 0.9580 - 15s/epoch - 55ms/step
Epoch 6/100
282/282 - 15s - loss: 0.2511 - acc: 0.8978 - auc: 0.9602 - val_loss: 0.2643 - val_acc: 0.9001 - val_auc: 0.9551 - 15s/epoch - 55ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2515 - acc: 0.8984 - auc: 0.9606 - val_loss: 0.2595 - val_acc: 0.9021 - val_auc: 0.9577 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 15s - loss: 0.2487 - acc: 0.9001 - auc: 0.9609 - val_loss: 0.2561 - val_acc: 0.8971 - val_auc: 0.9581 - 15s/epoch - 55ms/step
Epoch 9/100
282/282 - 15s - loss: 0.2453 - acc: 0.9025 - auc: 0.9620 - val_loss: 0.2615 - val_acc: 0.8991 - val_auc: 0.9583 - 15s/epoch - 55ms/step
Epoch 10/100
282/282 - 15s - loss: 0.2437 - acc: 0.9012 - auc: 0.9627 - val_loss: 0.2614 - val_acc: 0.8971 - val_auc: 0.9581 - 15s/epoch - 54ms/step
Epoch 11/100
282/282 - 15s - loss: 0.2411 - acc: 0.9012 - auc: 0.9633 - val_loss: 0.2571 - val_acc: 0.8961 - val_auc: 0.9584 - 15s/epoch - 55ms/step
Epoch 12/100
282/282 - 15s - loss: 0.2412 - acc: 0.9018 - auc: 0.9635 - val_loss: 0.2585 - val_acc: 0.8931 - val_auc: 0.9581 - 15s/epoch - 55ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2352 - acc: 0.9056 - auc: 0.9649 - val_loss: 0.2531 - val_acc: 0.9001 - val_auc: 0.9616 - 16s/epoch - 55ms/step
Epoch 14/100
282/282 - 15s - loss: 0.2338 - acc: 0.9080 - auc: 0.9659 - val_loss: 0.2631 - val_acc: 0.8891 - val_auc: 0.9573 - 15s/epoch - 55ms/step
Epoch 15/100
282/282 - 15s - loss: 0.2341 - acc: 0.9056 - auc: 0.9653 - val_loss: 0.2664 - val_acc: 0.8901 - val_auc: 0.9568 - 15s/epoch - 55ms/step
Epoch 16/100
282/282 - 15s - loss: 0.2334 - acc: 0.9046 - auc: 0.9658 - val_loss: 0.2531 - val_acc: 0.8981 - val_auc: 0.9609 - 15s/epoch - 55ms/step
Epoch 17/100
282/282 - 15s - loss: 0.2288 - acc: 0.9047 - auc: 0.9669 - val_loss: 0.2505 - val_acc: 0.8941 - val_auc: 0.9605 - 15s/epoch - 54ms/step
Epoch 18/100
282/282 - 15s - loss: 0.2262 - acc: 0.9101 - auc: 0.9674 - val_loss: 0.2584 - val_acc: 0.8921 - val_auc: 0.9593 - 15s/epoch - 54ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2235 - acc: 0.9117 - auc: 0.9681 - val_loss: 0.2471 - val_acc: 0.9051 - val_auc: 0.9616 - 16s/epoch - 55ms/step
Epoch 20/100
282/282 - 15s - loss: 0.2218 - acc: 0.9088 - auc: 0.9687 - val_loss: 0.2440 - val_acc: 0.9051 - val_auc: 0.9615 - 15s/epoch - 54ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2176 - acc: 0.9129 - auc: 0.9703 - val_loss: 0.2494 - val_acc: 0.8991 - val_auc: 0.9619 - 16s/epoch - 56ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2161 - acc: 0.9146 - auc: 0.9702 - val_loss: 0.2531 - val_acc: 0.9011 - val_auc: 0.9611 - 16s/epoch - 55ms/step
Epoch 23/100
282/282 - 15s - loss: 0.2136 - acc: 0.9134 - auc: 0.9715 - val_loss: 0.2452 - val_acc: 0.9001 - val_auc: 0.9617 - 15s/epoch - 54ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2076 - acc: 0.9170 - auc: 0.9727 - val_loss: 0.2506 - val_acc: 0.9011 - val_auc: 0.9600 - 16s/epoch - 56ms/step
Epoch 25/100
282/282 - 16s - loss: 0.2073 - acc: 0.9169 - auc: 0.9731 - val_loss: 0.2574 - val_acc: 0.9031 - val_auc: 0.9578 - 16s/epoch - 56ms/step
Epoch 26/100
282/282 - 16s - loss: 0.1996 - acc: 0.9210 - auc: 0.9752 - val_loss: 0.2461 - val_acc: 0.9121 - val_auc: 0.9629 - 16s/epoch - 56ms/step
Epoch 27/100
282/282 - 15s - loss: 0.1987 - acc: 0.9189 - auc: 0.9753 - val_loss: 0.2549 - val_acc: 0.9041 - val_auc: 0.9610 - 15s/epoch - 54ms/step
Epoch 28/100
282/282 - 22s - loss: 0.1941 - acc: 0.9226 - auc: 0.9765 - val_loss: 0.2605 - val_acc: 0.8961 - val_auc: 0.9612 - 22s/epoch - 76ms/step
Epoch 29/100
282/282 - 15s - loss: 0.1899 - acc: 0.9244 - auc: 0.9772 - val_loss: 0.2732 - val_acc: 0.9091 - val_auc: 0.9576 - 15s/epoch - 54ms/step
Epoch 30/100
282/282 - 16s - loss: 0.1822 - acc: 0.9282 - auc: 0.9792 - val_loss: 0.2581 - val_acc: 0.9001 - val_auc: 0.9596 - 16s/epoch - 56ms/step
Epoch 31/100
282/282 - 15s - loss: 0.1801 - acc: 0.9280 - auc: 0.9796 - val_loss: 0.2712 - val_acc: 0.8961 - val_auc: 0.9568 - 15s/epoch - 55ms/step
Epoch 32/100
282/282 - 15s - loss: 0.1757 - acc: 0.9316 - auc: 0.9811 - val_loss: 0.2558 - val_acc: 0.9031 - val_auc: 0.9612 - 15s/epoch - 55ms/step
Epoch 33/100
282/282 - 15s - loss: 0.1725 - acc: 0.9340 - auc: 0.9817 - val_loss: 0.2824 - val_acc: 0.8951 - val_auc: 0.9540 - 15s/epoch - 55ms/step
Epoch 34/100
282/282 - 15s - loss: 0.1642 - acc: 0.9359 - auc: 0.9827 - val_loss: 0.2755 - val_acc: 0.8961 - val_auc: 0.9580 - 15s/epoch - 55ms/step
Epoch 35/100
282/282 - 15s - loss: 0.1584 - acc: 0.9362 - auc: 0.9835 - val_loss: 0.2819 - val_acc: 0.9011 - val_auc: 0.9529 - 15s/epoch - 55ms/step
Epoch 36/100
282/282 - 16s - loss: 0.1560 - acc: 0.9421 - auc: 0.9848 - val_loss: 0.2866 - val_acc: 0.8941 - val_auc: 0.9533 - 16s/epoch - 55ms/step
Early stopping epoch: 35
******Evaluating TEST set*********
32/32 - 2s - 2s/epoch - 60ms/step
              precision    recall  f1-score   support

           0       0.92      0.85      0.88       391
           1       0.91      0.95      0.93       610

    accuracy                           0.91      1001
   macro avg       0.91      0.90      0.91      1001
weighted avg       0.91      0.91      0.91      1001

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.51      0.43       391
           1       0.59      0.46      0.52       610

    accuracy                           0.48      1001
   macro avg       0.49      0.49      0.48      1001
weighted avg       0.51      0.48      0.49      1001

______________________________________________________
fold 2
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_2 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_2 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_2 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_2 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3166 - acc: 0.8635 - auc: 0.9372 - val_loss: 0.2695 - val_acc: 0.8861 - val_auc: 0.9536 - 18s/epoch - 65ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2810 - acc: 0.8851 - auc: 0.9494 - val_loss: 0.2560 - val_acc: 0.8961 - val_auc: 0.9589 - 16s/epoch - 56ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2710 - acc: 0.8911 - auc: 0.9521 - val_loss: 0.2511 - val_acc: 0.9051 - val_auc: 0.9593 - 16s/epoch - 56ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2641 - acc: 0.8948 - auc: 0.9549 - val_loss: 0.2345 - val_acc: 0.9101 - val_auc: 0.9646 - 16s/epoch - 57ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2587 - acc: 0.8971 - auc: 0.9573 - val_loss: 0.2392 - val_acc: 0.9061 - val_auc: 0.9633 - 16s/epoch - 56ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2529 - acc: 0.8990 - auc: 0.9593 - val_loss: 0.2361 - val_acc: 0.9111 - val_auc: 0.9629 - 16s/epoch - 56ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2535 - acc: 0.8959 - auc: 0.9595 - val_loss: 0.2459 - val_acc: 0.8971 - val_auc: 0.9643 - 16s/epoch - 56ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2506 - acc: 0.8989 - auc: 0.9606 - val_loss: 0.2371 - val_acc: 0.9041 - val_auc: 0.9646 - 16s/epoch - 57ms/step
Epoch 9/100
282/282 - 16s - loss: 0.2482 - acc: 0.9014 - auc: 0.9613 - val_loss: 0.2317 - val_acc: 0.9131 - val_auc: 0.9658 - 16s/epoch - 56ms/step
Epoch 10/100
282/282 - 16s - loss: 0.2466 - acc: 0.9015 - auc: 0.9621 - val_loss: 0.2332 - val_acc: 0.9011 - val_auc: 0.9663 - 16s/epoch - 55ms/step
Epoch 11/100
282/282 - 16s - loss: 0.2447 - acc: 0.9000 - auc: 0.9627 - val_loss: 0.2359 - val_acc: 0.9141 - val_auc: 0.9645 - 16s/epoch - 57ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2424 - acc: 0.9009 - auc: 0.9637 - val_loss: 0.2313 - val_acc: 0.9091 - val_auc: 0.9656 - 16s/epoch - 56ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2425 - acc: 0.9019 - auc: 0.9631 - val_loss: 0.2286 - val_acc: 0.9161 - val_auc: 0.9675 - 16s/epoch - 57ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2381 - acc: 0.9037 - auc: 0.9647 - val_loss: 0.2283 - val_acc: 0.9021 - val_auc: 0.9680 - 16s/epoch - 57ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2359 - acc: 0.9049 - auc: 0.9650 - val_loss: 0.2311 - val_acc: 0.9031 - val_auc: 0.9677 - 16s/epoch - 56ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2350 - acc: 0.9055 - auc: 0.9656 - val_loss: 0.2271 - val_acc: 0.9041 - val_auc: 0.9688 - 16s/epoch - 56ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2341 - acc: 0.9044 - auc: 0.9656 - val_loss: 0.2338 - val_acc: 0.8991 - val_auc: 0.9673 - 16s/epoch - 56ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2295 - acc: 0.9098 - auc: 0.9675 - val_loss: 0.2275 - val_acc: 0.9081 - val_auc: 0.9686 - 16s/epoch - 56ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2257 - acc: 0.9081 - auc: 0.9686 - val_loss: 0.2284 - val_acc: 0.9021 - val_auc: 0.9679 - 16s/epoch - 56ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2236 - acc: 0.9102 - auc: 0.9693 - val_loss: 0.2398 - val_acc: 0.8981 - val_auc: 0.9650 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2223 - acc: 0.9094 - auc: 0.9695 - val_loss: 0.2352 - val_acc: 0.9011 - val_auc: 0.9663 - 16s/epoch - 56ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2179 - acc: 0.9119 - auc: 0.9710 - val_loss: 0.2342 - val_acc: 0.9081 - val_auc: 0.9658 - 16s/epoch - 56ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2163 - acc: 0.9138 - auc: 0.9715 - val_loss: 0.2249 - val_acc: 0.9031 - val_auc: 0.9683 - 16s/epoch - 56ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2172 - acc: 0.9131 - auc: 0.9710 - val_loss: 0.2369 - val_acc: 0.9051 - val_auc: 0.9656 - 16s/epoch - 56ms/step
Epoch 25/100
282/282 - 16s - loss: 0.2097 - acc: 0.9151 - auc: 0.9731 - val_loss: 0.2369 - val_acc: 0.9021 - val_auc: 0.9643 - 16s/epoch - 56ms/step
Epoch 26/100
282/282 - 16s - loss: 0.2054 - acc: 0.9200 - auc: 0.9738 - val_loss: 0.2478 - val_acc: 0.8981 - val_auc: 0.9636 - 16s/epoch - 56ms/step
Early stopping epoch: 25
******Evaluating TEST set*********
32/32 - 1s - 745ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.92      0.83      0.87       391
           1       0.90      0.95      0.92       610

    accuracy                           0.90      1001
   macro avg       0.91      0.89      0.90      1001
weighted avg       0.90      0.90      0.90      1001

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.50      0.44       391
           1       0.61      0.50      0.55       610

    accuracy                           0.50      1001
   macro avg       0.50      0.50      0.49      1001
weighted avg       0.52      0.50      0.50      1001

______________________________________________________
fold 3
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_3 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_3 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_3 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_3 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3151 - acc: 0.8643 - auc: 0.9379 - val_loss: 0.2642 - val_acc: 0.8870 - val_auc: 0.9556 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2808 - acc: 0.8893 - auc: 0.9489 - val_loss: 0.2481 - val_acc: 0.8990 - val_auc: 0.9611 - 16s/epoch - 56ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2685 - acc: 0.8918 - auc: 0.9538 - val_loss: 0.2414 - val_acc: 0.9060 - val_auc: 0.9623 - 16s/epoch - 56ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2618 - acc: 0.8924 - auc: 0.9565 - val_loss: 0.2410 - val_acc: 0.9110 - val_auc: 0.9633 - 16s/epoch - 56ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2572 - acc: 0.8980 - auc: 0.9573 - val_loss: 0.2392 - val_acc: 0.9040 - val_auc: 0.9639 - 16s/epoch - 57ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2533 - acc: 0.8980 - auc: 0.9599 - val_loss: 0.2400 - val_acc: 0.8980 - val_auc: 0.9643 - 16s/epoch - 56ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2518 - acc: 0.8981 - auc: 0.9599 - val_loss: 0.2302 - val_acc: 0.9140 - val_auc: 0.9661 - 16s/epoch - 56ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2490 - acc: 0.8999 - auc: 0.9609 - val_loss: 0.2334 - val_acc: 0.9130 - val_auc: 0.9667 - 16s/epoch - 57ms/step
Epoch 9/100
282/282 - 15s - loss: 0.2475 - acc: 0.8993 - auc: 0.9617 - val_loss: 0.2248 - val_acc: 0.9160 - val_auc: 0.9686 - 15s/epoch - 55ms/step
Epoch 10/100
282/282 - 15s - loss: 0.2472 - acc: 0.8997 - auc: 0.9614 - val_loss: 0.2454 - val_acc: 0.8980 - val_auc: 0.9651 - 15s/epoch - 54ms/step
Epoch 11/100
282/282 - 15s - loss: 0.2412 - acc: 0.9014 - auc: 0.9640 - val_loss: 0.2386 - val_acc: 0.9020 - val_auc: 0.9669 - 15s/epoch - 54ms/step
Epoch 12/100
282/282 - 15s - loss: 0.2394 - acc: 0.9036 - auc: 0.9643 - val_loss: 0.2343 - val_acc: 0.9040 - val_auc: 0.9673 - 15s/epoch - 54ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2394 - acc: 0.9025 - auc: 0.9646 - val_loss: 0.2274 - val_acc: 0.9060 - val_auc: 0.9677 - 16s/epoch - 56ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2356 - acc: 0.9030 - auc: 0.9659 - val_loss: 0.2241 - val_acc: 0.9100 - val_auc: 0.9693 - 16s/epoch - 57ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2327 - acc: 0.9067 - auc: 0.9662 - val_loss: 0.2500 - val_acc: 0.8970 - val_auc: 0.9651 - 16s/epoch - 57ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2301 - acc: 0.9074 - auc: 0.9672 - val_loss: 0.2234 - val_acc: 0.9120 - val_auc: 0.9703 - 16s/epoch - 57ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2289 - acc: 0.9051 - auc: 0.9680 - val_loss: 0.2218 - val_acc: 0.9130 - val_auc: 0.9704 - 16s/epoch - 57ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2284 - acc: 0.9070 - auc: 0.9679 - val_loss: 0.2239 - val_acc: 0.9190 - val_auc: 0.9672 - 16s/epoch - 56ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2266 - acc: 0.9066 - auc: 0.9677 - val_loss: 0.2301 - val_acc: 0.9100 - val_auc: 0.9675 - 16s/epoch - 56ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2190 - acc: 0.9131 - auc: 0.9702 - val_loss: 0.2252 - val_acc: 0.9100 - val_auc: 0.9685 - 16s/epoch - 57ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2155 - acc: 0.9133 - auc: 0.9714 - val_loss: 0.2254 - val_acc: 0.9140 - val_auc: 0.9689 - 16s/epoch - 56ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2101 - acc: 0.9154 - auc: 0.9730 - val_loss: 0.2278 - val_acc: 0.9080 - val_auc: 0.9680 - 16s/epoch - 56ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2089 - acc: 0.9151 - auc: 0.9731 - val_loss: 0.2230 - val_acc: 0.9230 - val_auc: 0.9677 - 16s/epoch - 56ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2031 - acc: 0.9175 - auc: 0.9744 - val_loss: 0.2288 - val_acc: 0.9100 - val_auc: 0.9679 - 16s/epoch - 55ms/step
Epoch 25/100
282/282 - 15s - loss: 0.1998 - acc: 0.9185 - auc: 0.9756 - val_loss: 0.2308 - val_acc: 0.9130 - val_auc: 0.9667 - 15s/epoch - 55ms/step
Epoch 26/100
282/282 - 15s - loss: 0.1924 - acc: 0.9235 - auc: 0.9765 - val_loss: 0.2263 - val_acc: 0.9160 - val_auc: 0.9671 - 15s/epoch - 55ms/step
Epoch 27/100
282/282 - 15s - loss: 0.1881 - acc: 0.9241 - auc: 0.9780 - val_loss: 0.2319 - val_acc: 0.9160 - val_auc: 0.9696 - 15s/epoch - 55ms/step
Early stopping epoch: 26
******Evaluating TEST set*********
32/32 - 1s - 727ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.89      0.89      0.89       390
           1       0.93      0.93      0.93       610

    accuracy                           0.91      1000
   macro avg       0.91      0.91      0.91      1000
weighted avg       0.91      0.91      0.91      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.45      0.41       390
           1       0.59      0.51      0.55       610

    accuracy                           0.49      1000
   macro avg       0.48      0.48      0.48      1000
weighted avg       0.51      0.49      0.49      1000

______________________________________________________
fold 4
Model: "model_4"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_4 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_4 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_4 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_4 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3090 - acc: 0.8679 - auc: 0.9397 - val_loss: 0.2914 - val_acc: 0.8840 - val_auc: 0.9464 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2727 - acc: 0.8910 - auc: 0.9518 - val_loss: 0.2918 - val_acc: 0.8790 - val_auc: 0.9507 - 16s/epoch - 55ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2624 - acc: 0.8966 - auc: 0.9558 - val_loss: 0.2799 - val_acc: 0.8920 - val_auc: 0.9536 - 16s/epoch - 56ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2578 - acc: 0.8974 - auc: 0.9575 - val_loss: 0.2744 - val_acc: 0.8860 - val_auc: 0.9536 - 16s/epoch - 55ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2535 - acc: 0.9004 - auc: 0.9591 - val_loss: 0.2708 - val_acc: 0.8850 - val_auc: 0.9556 - 16s/epoch - 56ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2526 - acc: 0.8991 - auc: 0.9590 - val_loss: 0.2759 - val_acc: 0.8860 - val_auc: 0.9541 - 16s/epoch - 55ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2490 - acc: 0.9004 - auc: 0.9610 - val_loss: 0.2772 - val_acc: 0.8900 - val_auc: 0.9528 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2458 - acc: 0.8993 - auc: 0.9616 - val_loss: 0.2741 - val_acc: 0.8910 - val_auc: 0.9542 - 16s/epoch - 55ms/step
Epoch 9/100
282/282 - 16s - loss: 0.2461 - acc: 0.9029 - auc: 0.9616 - val_loss: 0.2698 - val_acc: 0.8800 - val_auc: 0.9559 - 16s/epoch - 56ms/step
Epoch 10/100
282/282 - 16s - loss: 0.2446 - acc: 0.9026 - auc: 0.9623 - val_loss: 0.2706 - val_acc: 0.8920 - val_auc: 0.9549 - 16s/epoch - 55ms/step
Epoch 11/100
282/282 - 16s - loss: 0.2417 - acc: 0.9013 - auc: 0.9640 - val_loss: 0.2657 - val_acc: 0.8840 - val_auc: 0.9559 - 16s/epoch - 55ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2403 - acc: 0.9036 - auc: 0.9634 - val_loss: 0.2637 - val_acc: 0.8820 - val_auc: 0.9583 - 16s/epoch - 56ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2378 - acc: 0.9041 - auc: 0.9651 - val_loss: 0.2671 - val_acc: 0.8830 - val_auc: 0.9568 - 16s/epoch - 55ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2330 - acc: 0.9055 - auc: 0.9664 - val_loss: 0.2719 - val_acc: 0.8880 - val_auc: 0.9557 - 16s/epoch - 56ms/step
Epoch 15/100
282/282 - 15s - loss: 0.2309 - acc: 0.9067 - auc: 0.9668 - val_loss: 0.2693 - val_acc: 0.8910 - val_auc: 0.9559 - 15s/epoch - 55ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2304 - acc: 0.9064 - auc: 0.9671 - val_loss: 0.2571 - val_acc: 0.8950 - val_auc: 0.9606 - 16s/epoch - 55ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2255 - acc: 0.9075 - auc: 0.9688 - val_loss: 0.2623 - val_acc: 0.8920 - val_auc: 0.9576 - 16s/epoch - 55ms/step
Epoch 18/100
282/282 - 15s - loss: 0.2221 - acc: 0.9096 - auc: 0.9690 - val_loss: 0.2913 - val_acc: 0.8880 - val_auc: 0.9524 - 15s/epoch - 55ms/step
Epoch 19/100
282/282 - 15s - loss: 0.2222 - acc: 0.9088 - auc: 0.9694 - val_loss: 0.2652 - val_acc: 0.8930 - val_auc: 0.9591 - 15s/epoch - 55ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2173 - acc: 0.9107 - auc: 0.9706 - val_loss: 0.2684 - val_acc: 0.8890 - val_auc: 0.9551 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 15s - loss: 0.2134 - acc: 0.9134 - auc: 0.9716 - val_loss: 0.2713 - val_acc: 0.8940 - val_auc: 0.9558 - 15s/epoch - 55ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2118 - acc: 0.9141 - auc: 0.9720 - val_loss: 0.2625 - val_acc: 0.8900 - val_auc: 0.9578 - 16s/epoch - 55ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2051 - acc: 0.9165 - auc: 0.9732 - val_loss: 0.2807 - val_acc: 0.8860 - val_auc: 0.9572 - 16s/epoch - 55ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2018 - acc: 0.9166 - auc: 0.9751 - val_loss: 0.3040 - val_acc: 0.8840 - val_auc: 0.9508 - 16s/epoch - 55ms/step
Epoch 25/100
282/282 - 15s - loss: 0.1980 - acc: 0.9194 - auc: 0.9754 - val_loss: 0.2710 - val_acc: 0.8960 - val_auc: 0.9552 - 15s/epoch - 55ms/step
Epoch 26/100
282/282 - 16s - loss: 0.1953 - acc: 0.9221 - auc: 0.9759 - val_loss: 0.2707 - val_acc: 0.8880 - val_auc: 0.9544 - 16s/epoch - 56ms/step
Early stopping epoch: 25
******Evaluating TEST set*********
32/32 - 1s - 795ms/epoch - 25ms/step
              precision    recall  f1-score   support

           0       0.89      0.83      0.86       390
           1       0.90      0.94      0.92       610

    accuracy                           0.90      1000
   macro avg       0.89      0.88      0.89      1000
weighted avg       0.89      0.90      0.89      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.52      0.45       390
           1       0.61      0.49      0.55       610

    accuracy                           0.50      1000
   macro avg       0.50      0.50      0.50      1000
weighted avg       0.53      0.50      0.51      1000

______________________________________________________
fold 5
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_5 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_5 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_5 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_5 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3124 - acc: 0.8668 - auc: 0.9388 - val_loss: 0.2889 - val_acc: 0.8770 - val_auc: 0.9469 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2791 - acc: 0.8865 - auc: 0.9499 - val_loss: 0.2719 - val_acc: 0.8850 - val_auc: 0.9543 - 16s/epoch - 55ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2680 - acc: 0.8928 - auc: 0.9534 - val_loss: 0.2563 - val_acc: 0.9010 - val_auc: 0.9588 - 16s/epoch - 55ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2600 - acc: 0.8986 - auc: 0.9568 - val_loss: 0.2548 - val_acc: 0.9020 - val_auc: 0.9608 - 16s/epoch - 55ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2594 - acc: 0.8956 - auc: 0.9574 - val_loss: 0.2598 - val_acc: 0.9000 - val_auc: 0.9612 - 16s/epoch - 55ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2533 - acc: 0.8997 - auc: 0.9592 - val_loss: 0.2511 - val_acc: 0.9000 - val_auc: 0.9610 - 16s/epoch - 57ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2511 - acc: 0.8997 - auc: 0.9599 - val_loss: 0.2483 - val_acc: 0.9030 - val_auc: 0.9617 - 16s/epoch - 56ms/step
Epoch 8/100
282/282 - 15s - loss: 0.2476 - acc: 0.8983 - auc: 0.9612 - val_loss: 0.2534 - val_acc: 0.8960 - val_auc: 0.9607 - 15s/epoch - 55ms/step
Epoch 9/100
282/282 - 15s - loss: 0.2480 - acc: 0.9013 - auc: 0.9612 - val_loss: 0.2490 - val_acc: 0.9020 - val_auc: 0.9613 - 15s/epoch - 55ms/step
Epoch 10/100
282/282 - 15s - loss: 0.2417 - acc: 0.9044 - auc: 0.9632 - val_loss: 0.2465 - val_acc: 0.9010 - val_auc: 0.9616 - 15s/epoch - 55ms/step
Epoch 11/100
282/282 - 16s - loss: 0.2423 - acc: 0.9010 - auc: 0.9635 - val_loss: 0.2527 - val_acc: 0.9050 - val_auc: 0.9612 - 16s/epoch - 55ms/step
Epoch 12/100
282/282 - 15s - loss: 0.2430 - acc: 0.9031 - auc: 0.9635 - val_loss: 0.2673 - val_acc: 0.8930 - val_auc: 0.9587 - 15s/epoch - 55ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2359 - acc: 0.9057 - auc: 0.9651 - val_loss: 0.2589 - val_acc: 0.8940 - val_auc: 0.9594 - 16s/epoch - 55ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2331 - acc: 0.9064 - auc: 0.9666 - val_loss: 0.2588 - val_acc: 0.8980 - val_auc: 0.9572 - 16s/epoch - 55ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2309 - acc: 0.9051 - auc: 0.9672 - val_loss: 0.2595 - val_acc: 0.8940 - val_auc: 0.9575 - 16s/epoch - 55ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2307 - acc: 0.9054 - auc: 0.9676 - val_loss: 0.2548 - val_acc: 0.9000 - val_auc: 0.9580 - 16s/epoch - 57ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2286 - acc: 0.9084 - auc: 0.9676 - val_loss: 0.2563 - val_acc: 0.8920 - val_auc: 0.9598 - 16s/epoch - 57ms/step
Early stopping epoch: 16
******Evaluating TEST set*********
32/32 - 1s - 763ms/epoch - 24ms/step
              precision    recall  f1-score   support

           0       0.91      0.84      0.87       390
           1       0.90      0.94      0.92       610

    accuracy                           0.90      1000
   macro avg       0.90      0.89      0.90      1000
weighted avg       0.90      0.90      0.90      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.50      0.43       390
           1       0.60      0.48      0.53       610

    accuracy                           0.49      1000
   macro avg       0.49      0.49      0.48      1000
weighted avg       0.52      0.49      0.49      1000

______________________________________________________
fold 6
Model: "model_6"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_6 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_6 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_6 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_6 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3091 - acc: 0.8668 - auc: 0.9404 - val_loss: 0.2986 - val_acc: 0.8810 - val_auc: 0.9435 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2766 - acc: 0.8866 - auc: 0.9513 - val_loss: 0.2947 - val_acc: 0.8830 - val_auc: 0.9453 - 16s/epoch - 55ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2666 - acc: 0.8921 - auc: 0.9545 - val_loss: 0.2728 - val_acc: 0.8900 - val_auc: 0.9503 - 16s/epoch - 55ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2584 - acc: 0.8989 - auc: 0.9566 - val_loss: 0.2756 - val_acc: 0.8850 - val_auc: 0.9527 - 16s/epoch - 56ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2574 - acc: 0.8977 - auc: 0.9579 - val_loss: 0.2879 - val_acc: 0.8790 - val_auc: 0.9494 - 16s/epoch - 55ms/step
Epoch 6/100
282/282 - 15s - loss: 0.2531 - acc: 0.9016 - auc: 0.9590 - val_loss: 0.2756 - val_acc: 0.8940 - val_auc: 0.9526 - 15s/epoch - 55ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2492 - acc: 0.8998 - auc: 0.9605 - val_loss: 0.2704 - val_acc: 0.8910 - val_auc: 0.9544 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 15s - loss: 0.2468 - acc: 0.8997 - auc: 0.9617 - val_loss: 0.2792 - val_acc: 0.8850 - val_auc: 0.9534 - 15s/epoch - 55ms/step
Epoch 9/100
282/282 - 16s - loss: 0.2457 - acc: 0.9025 - auc: 0.9621 - val_loss: 0.2649 - val_acc: 0.8890 - val_auc: 0.9565 - 16s/epoch - 56ms/step
Epoch 10/100
282/282 - 16s - loss: 0.2409 - acc: 0.9049 - auc: 0.9634 - val_loss: 0.2727 - val_acc: 0.8840 - val_auc: 0.9530 - 16s/epoch - 56ms/step
Epoch 11/100
282/282 - 16s - loss: 0.2400 - acc: 0.9044 - auc: 0.9641 - val_loss: 0.2712 - val_acc: 0.8840 - val_auc: 0.9544 - 16s/epoch - 56ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2376 - acc: 0.9059 - auc: 0.9643 - val_loss: 0.2624 - val_acc: 0.8850 - val_auc: 0.9585 - 16s/epoch - 55ms/step
Epoch 13/100
282/282 - 15s - loss: 0.2354 - acc: 0.9051 - auc: 0.9653 - val_loss: 0.2671 - val_acc: 0.8810 - val_auc: 0.9553 - 15s/epoch - 55ms/step
Epoch 14/100
282/282 - 15s - loss: 0.2337 - acc: 0.9038 - auc: 0.9659 - val_loss: 0.2618 - val_acc: 0.8860 - val_auc: 0.9573 - 15s/epoch - 55ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2279 - acc: 0.9094 - auc: 0.9678 - val_loss: 0.2655 - val_acc: 0.8830 - val_auc: 0.9569 - 16s/epoch - 57ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2294 - acc: 0.9077 - auc: 0.9671 - val_loss: 0.2757 - val_acc: 0.8840 - val_auc: 0.9554 - 16s/epoch - 57ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2264 - acc: 0.9074 - auc: 0.9684 - val_loss: 0.2754 - val_acc: 0.8810 - val_auc: 0.9534 - 16s/epoch - 57ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2249 - acc: 0.9121 - auc: 0.9682 - val_loss: 0.2733 - val_acc: 0.8880 - val_auc: 0.9567 - 16s/epoch - 56ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2224 - acc: 0.9094 - auc: 0.9695 - val_loss: 0.2639 - val_acc: 0.8900 - val_auc: 0.9568 - 16s/epoch - 57ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2181 - acc: 0.9129 - auc: 0.9704 - val_loss: 0.2731 - val_acc: 0.8850 - val_auc: 0.9584 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2155 - acc: 0.9113 - auc: 0.9706 - val_loss: 0.2660 - val_acc: 0.8910 - val_auc: 0.9559 - 16s/epoch - 56ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2129 - acc: 0.9148 - auc: 0.9721 - val_loss: 0.2781 - val_acc: 0.8890 - val_auc: 0.9563 - 16s/epoch - 56ms/step
Early stopping epoch: 21
******Evaluating TEST set*********
32/32 - 1s - 752ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.89      0.81      0.85       390
           1       0.88      0.93      0.91       610

    accuracy                           0.89      1000
   macro avg       0.89      0.87      0.88      1000
weighted avg       0.89      0.89      0.88      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.39      0.50      0.44       390
           1       0.61      0.49      0.54       610

    accuracy                           0.49      1000
   macro avg       0.50      0.50      0.49      1000
weighted avg       0.52      0.49      0.50      1000

______________________________________________________
fold 7
Model: "model_7"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_7 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_7 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_7 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_7 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3080 - acc: 0.8685 - auc: 0.9404 - val_loss: 0.2698 - val_acc: 0.8960 - val_auc: 0.9540 - 18s/epoch - 64ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2734 - acc: 0.8883 - auc: 0.9521 - val_loss: 0.2540 - val_acc: 0.9020 - val_auc: 0.9574 - 16s/epoch - 55ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2625 - acc: 0.8927 - auc: 0.9556 - val_loss: 0.2455 - val_acc: 0.9060 - val_auc: 0.9621 - 16s/epoch - 55ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2593 - acc: 0.8950 - auc: 0.9577 - val_loss: 0.2436 - val_acc: 0.9100 - val_auc: 0.9635 - 16s/epoch - 55ms/step
Epoch 5/100
282/282 - 15s - loss: 0.2582 - acc: 0.8977 - auc: 0.9569 - val_loss: 0.2442 - val_acc: 0.9000 - val_auc: 0.9627 - 15s/epoch - 55ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2538 - acc: 0.8986 - auc: 0.9592 - val_loss: 0.2419 - val_acc: 0.9050 - val_auc: 0.9642 - 16s/epoch - 56ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2533 - acc: 0.8983 - auc: 0.9594 - val_loss: 0.2357 - val_acc: 0.9090 - val_auc: 0.9647 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2479 - acc: 0.8987 - auc: 0.9613 - val_loss: 0.2369 - val_acc: 0.9060 - val_auc: 0.9652 - 16s/epoch - 55ms/step
Epoch 9/100
282/282 - 15s - loss: 0.2476 - acc: 0.9006 - auc: 0.9616 - val_loss: 0.2461 - val_acc: 0.9030 - val_auc: 0.9629 - 15s/epoch - 55ms/step
Epoch 10/100
282/282 - 15s - loss: 0.2473 - acc: 0.8985 - auc: 0.9620 - val_loss: 0.2302 - val_acc: 0.9090 - val_auc: 0.9661 - 15s/epoch - 54ms/step
Epoch 11/100
282/282 - 15s - loss: 0.2417 - acc: 0.9021 - auc: 0.9632 - val_loss: 0.2394 - val_acc: 0.9140 - val_auc: 0.9643 - 15s/epoch - 54ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2411 - acc: 0.9011 - auc: 0.9638 - val_loss: 0.2309 - val_acc: 0.9190 - val_auc: 0.9678 - 16s/epoch - 56ms/step
Epoch 13/100
282/282 - 15s - loss: 0.2360 - acc: 0.9038 - auc: 0.9655 - val_loss: 0.2557 - val_acc: 0.8900 - val_auc: 0.9586 - 15s/epoch - 55ms/step
Epoch 14/100
282/282 - 15s - loss: 0.2355 - acc: 0.9034 - auc: 0.9660 - val_loss: 0.2364 - val_acc: 0.9050 - val_auc: 0.9632 - 15s/epoch - 55ms/step
Epoch 15/100
282/282 - 15s - loss: 0.2307 - acc: 0.9073 - auc: 0.9670 - val_loss: 0.2388 - val_acc: 0.9110 - val_auc: 0.9639 - 15s/epoch - 55ms/step
Epoch 16/100
282/282 - 15s - loss: 0.2282 - acc: 0.9067 - auc: 0.9681 - val_loss: 0.2457 - val_acc: 0.9050 - val_auc: 0.9590 - 15s/epoch - 55ms/step
Epoch 17/100
282/282 - 15s - loss: 0.2258 - acc: 0.9088 - auc: 0.9686 - val_loss: 0.2450 - val_acc: 0.9020 - val_auc: 0.9631 - 15s/epoch - 55ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2250 - acc: 0.9076 - auc: 0.9689 - val_loss: 0.2461 - val_acc: 0.9080 - val_auc: 0.9628 - 16s/epoch - 56ms/step
Epoch 19/100
282/282 - 15s - loss: 0.2200 - acc: 0.9111 - auc: 0.9703 - val_loss: 0.2495 - val_acc: 0.8990 - val_auc: 0.9613 - 15s/epoch - 55ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2173 - acc: 0.9143 - auc: 0.9711 - val_loss: 0.2472 - val_acc: 0.9120 - val_auc: 0.9593 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2124 - acc: 0.9170 - auc: 0.9722 - val_loss: 0.2503 - val_acc: 0.9030 - val_auc: 0.9617 - 16s/epoch - 55ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2146 - acc: 0.9126 - auc: 0.9716 - val_loss: 0.2570 - val_acc: 0.9030 - val_auc: 0.9576 - 16s/epoch - 55ms/step
Early stopping epoch: 21
******Evaluating TEST set*********
32/32 - 1s - 728ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.91      0.88      0.89       390
           1       0.92      0.95      0.93       610

    accuracy                           0.92      1000
   macro avg       0.92      0.91      0.91      1000
weighted avg       0.92      0.92      0.92      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.48      0.42       390
           1       0.60      0.50      0.54       610

    accuracy                           0.49      1000
   macro avg       0.49      0.49      0.48      1000
weighted avg       0.51      0.49      0.50      1000

______________________________________________________
fold 8
Model: "model_8"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_8 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_8 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_8 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_8 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 19s - loss: 0.3122 - acc: 0.8678 - auc: 0.9387 - val_loss: 0.2996 - val_acc: 0.8790 - val_auc: 0.9453 - 19s/epoch - 67ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2740 - acc: 0.8927 - auc: 0.9512 - val_loss: 0.2796 - val_acc: 0.8890 - val_auc: 0.9509 - 16s/epoch - 56ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2654 - acc: 0.8948 - auc: 0.9551 - val_loss: 0.2715 - val_acc: 0.8920 - val_auc: 0.9532 - 16s/epoch - 56ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2562 - acc: 0.8991 - auc: 0.9574 - val_loss: 0.2746 - val_acc: 0.8900 - val_auc: 0.9525 - 16s/epoch - 55ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2582 - acc: 0.8957 - auc: 0.9581 - val_loss: 0.2775 - val_acc: 0.8940 - val_auc: 0.9527 - 16s/epoch - 55ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2528 - acc: 0.9000 - auc: 0.9585 - val_loss: 0.2645 - val_acc: 0.8970 - val_auc: 0.9555 - 16s/epoch - 55ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2492 - acc: 0.9003 - auc: 0.9609 - val_loss: 0.2689 - val_acc: 0.8950 - val_auc: 0.9550 - 16s/epoch - 55ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2491 - acc: 0.9024 - auc: 0.9602 - val_loss: 0.2619 - val_acc: 0.8990 - val_auc: 0.9565 - 16s/epoch - 55ms/step
Epoch 9/100
282/282 - 15s - loss: 0.2473 - acc: 0.8993 - auc: 0.9614 - val_loss: 0.2651 - val_acc: 0.8940 - val_auc: 0.9558 - 15s/epoch - 55ms/step
Epoch 10/100
282/282 - 15s - loss: 0.2419 - acc: 0.9041 - auc: 0.9637 - val_loss: 0.2787 - val_acc: 0.8950 - val_auc: 0.9527 - 15s/epoch - 55ms/step
Epoch 11/100
282/282 - 15s - loss: 0.2408 - acc: 0.9041 - auc: 0.9636 - val_loss: 0.2584 - val_acc: 0.8990 - val_auc: 0.9570 - 15s/epoch - 54ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2411 - acc: 0.9036 - auc: 0.9635 - val_loss: 0.2687 - val_acc: 0.8970 - val_auc: 0.9559 - 16s/epoch - 55ms/step
Epoch 13/100
282/282 - 16s - loss: 0.2398 - acc: 0.9041 - auc: 0.9638 - val_loss: 0.2670 - val_acc: 0.8960 - val_auc: 0.9585 - 16s/epoch - 56ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2372 - acc: 0.9066 - auc: 0.9647 - val_loss: 0.2749 - val_acc: 0.8990 - val_auc: 0.9557 - 16s/epoch - 56ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2373 - acc: 0.9031 - auc: 0.9651 - val_loss: 0.2623 - val_acc: 0.8920 - val_auc: 0.9571 - 16s/epoch - 55ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2311 - acc: 0.9066 - auc: 0.9670 - val_loss: 0.2549 - val_acc: 0.9020 - val_auc: 0.9587 - 16s/epoch - 56ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2321 - acc: 0.9074 - auc: 0.9666 - val_loss: 0.2673 - val_acc: 0.8990 - val_auc: 0.9553 - 16s/epoch - 56ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2303 - acc: 0.9074 - auc: 0.9669 - val_loss: 0.2510 - val_acc: 0.8980 - val_auc: 0.9598 - 16s/epoch - 57ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2255 - acc: 0.9068 - auc: 0.9681 - val_loss: 0.2567 - val_acc: 0.8990 - val_auc: 0.9588 - 16s/epoch - 56ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2249 - acc: 0.9103 - auc: 0.9686 - val_loss: 0.2574 - val_acc: 0.8990 - val_auc: 0.9592 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2219 - acc: 0.9100 - auc: 0.9697 - val_loss: 0.2635 - val_acc: 0.8980 - val_auc: 0.9560 - 16s/epoch - 57ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2168 - acc: 0.9118 - auc: 0.9713 - val_loss: 0.2677 - val_acc: 0.8980 - val_auc: 0.9591 - 16s/epoch - 56ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2168 - acc: 0.9130 - auc: 0.9708 - val_loss: 0.2628 - val_acc: 0.9030 - val_auc: 0.9563 - 16s/epoch - 56ms/step
Epoch 24/100
282/282 - 16s - loss: 0.2123 - acc: 0.9154 - auc: 0.9721 - val_loss: 0.2902 - val_acc: 0.8980 - val_auc: 0.9514 - 16s/epoch - 57ms/step
Epoch 25/100
282/282 - 16s - loss: 0.2082 - acc: 0.9141 - auc: 0.9731 - val_loss: 0.2583 - val_acc: 0.8940 - val_auc: 0.9567 - 16s/epoch - 57ms/step
Epoch 26/100
282/282 - 16s - loss: 0.2108 - acc: 0.9147 - auc: 0.9720 - val_loss: 0.2587 - val_acc: 0.9000 - val_auc: 0.9575 - 16s/epoch - 57ms/step
Epoch 27/100
282/282 - 16s - loss: 0.2030 - acc: 0.9201 - auc: 0.9744 - val_loss: 0.2739 - val_acc: 0.8950 - val_auc: 0.9527 - 16s/epoch - 56ms/step
Epoch 28/100
282/282 - 16s - loss: 0.1970 - acc: 0.9221 - auc: 0.9759 - val_loss: 0.2855 - val_acc: 0.8900 - val_auc: 0.9529 - 16s/epoch - 56ms/step
Early stopping epoch: 27
******Evaluating TEST set*********
32/32 - 1s - 735ms/epoch - 23ms/step
              precision    recall  f1-score   support

           0       0.88      0.86      0.87       390
           1       0.91      0.92      0.92       610

    accuracy                           0.90      1000
   macro avg       0.89      0.89      0.89      1000
weighted avg       0.90      0.90      0.90      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.37      0.47      0.41       390
           1       0.59      0.48      0.53       610

    accuracy                           0.48      1000
   macro avg       0.48      0.48      0.47      1000
weighted avg       0.50      0.48      0.48      1000

______________________________________________________
fold 9
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
282/282 - 18s - loss: 0.3093 - acc: 0.8687 - auc: 0.9404 - val_loss: 0.3023 - val_acc: 0.8720 - val_auc: 0.9421 - 18s/epoch - 65ms/step
Epoch 2/100
282/282 - 16s - loss: 0.2754 - acc: 0.8905 - auc: 0.9512 - val_loss: 0.2778 - val_acc: 0.8920 - val_auc: 0.9489 - 16s/epoch - 57ms/step
Epoch 3/100
282/282 - 16s - loss: 0.2634 - acc: 0.8984 - auc: 0.9546 - val_loss: 0.2729 - val_acc: 0.8820 - val_auc: 0.9518 - 16s/epoch - 56ms/step
Epoch 4/100
282/282 - 16s - loss: 0.2569 - acc: 0.8983 - auc: 0.9571 - val_loss: 0.2881 - val_acc: 0.8750 - val_auc: 0.9496 - 16s/epoch - 56ms/step
Epoch 5/100
282/282 - 16s - loss: 0.2543 - acc: 0.8987 - auc: 0.9592 - val_loss: 0.2706 - val_acc: 0.8860 - val_auc: 0.9575 - 16s/epoch - 56ms/step
Epoch 6/100
282/282 - 16s - loss: 0.2525 - acc: 0.9018 - auc: 0.9590 - val_loss: 0.2708 - val_acc: 0.8830 - val_auc: 0.9556 - 16s/epoch - 56ms/step
Epoch 7/100
282/282 - 16s - loss: 0.2473 - acc: 0.9025 - auc: 0.9611 - val_loss: 0.2724 - val_acc: 0.8900 - val_auc: 0.9552 - 16s/epoch - 57ms/step
Epoch 8/100
282/282 - 16s - loss: 0.2476 - acc: 0.8998 - auc: 0.9609 - val_loss: 0.2721 - val_acc: 0.8870 - val_auc: 0.9552 - 16s/epoch - 57ms/step
Epoch 9/100
282/282 - 16s - loss: 0.2440 - acc: 0.9035 - auc: 0.9627 - val_loss: 0.2710 - val_acc: 0.8880 - val_auc: 0.9556 - 16s/epoch - 57ms/step
Epoch 10/100
282/282 - 17s - loss: 0.2431 - acc: 0.9024 - auc: 0.9631 - val_loss: 0.2864 - val_acc: 0.8810 - val_auc: 0.9516 - 17s/epoch - 59ms/step
Epoch 11/100
282/282 - 19s - loss: 0.2398 - acc: 0.9061 - auc: 0.9636 - val_loss: 0.2800 - val_acc: 0.8780 - val_auc: 0.9522 - 19s/epoch - 68ms/step
Epoch 12/100
282/282 - 16s - loss: 0.2364 - acc: 0.9044 - auc: 0.9652 - val_loss: 0.2855 - val_acc: 0.8860 - val_auc: 0.9559 - 16s/epoch - 57ms/step
Epoch 13/100
282/282 - 17s - loss: 0.2345 - acc: 0.9063 - auc: 0.9655 - val_loss: 0.2686 - val_acc: 0.8840 - val_auc: 0.9583 - 17s/epoch - 61ms/step
Epoch 14/100
282/282 - 16s - loss: 0.2319 - acc: 0.9086 - auc: 0.9667 - val_loss: 0.2793 - val_acc: 0.8830 - val_auc: 0.9559 - 16s/epoch - 56ms/step
Epoch 15/100
282/282 - 16s - loss: 0.2275 - acc: 0.9089 - auc: 0.9675 - val_loss: 0.2799 - val_acc: 0.8850 - val_auc: 0.9550 - 16s/epoch - 57ms/step
Epoch 16/100
282/282 - 16s - loss: 0.2262 - acc: 0.9085 - auc: 0.9683 - val_loss: 0.2674 - val_acc: 0.8810 - val_auc: 0.9565 - 16s/epoch - 56ms/step
Epoch 17/100
282/282 - 16s - loss: 0.2243 - acc: 0.9105 - auc: 0.9687 - val_loss: 0.2746 - val_acc: 0.8820 - val_auc: 0.9565 - 16s/epoch - 56ms/step
Epoch 18/100
282/282 - 16s - loss: 0.2227 - acc: 0.9117 - auc: 0.9687 - val_loss: 0.2784 - val_acc: 0.8790 - val_auc: 0.9558 - 16s/epoch - 57ms/step
Epoch 19/100
282/282 - 16s - loss: 0.2176 - acc: 0.9131 - auc: 0.9706 - val_loss: 0.2843 - val_acc: 0.8810 - val_auc: 0.9536 - 16s/epoch - 56ms/step
Epoch 20/100
282/282 - 16s - loss: 0.2183 - acc: 0.9149 - auc: 0.9702 - val_loss: 0.2850 - val_acc: 0.8820 - val_auc: 0.9537 - 16s/epoch - 56ms/step
Epoch 21/100
282/282 - 16s - loss: 0.2129 - acc: 0.9156 - auc: 0.9713 - val_loss: 0.3241 - val_acc: 0.8590 - val_auc: 0.9448 - 16s/epoch - 56ms/step
Epoch 22/100
282/282 - 16s - loss: 0.2095 - acc: 0.9170 - auc: 0.9721 - val_loss: 0.2829 - val_acc: 0.8840 - val_auc: 0.9515 - 16s/epoch - 56ms/step
Epoch 23/100
282/282 - 16s - loss: 0.2031 - acc: 0.9197 - auc: 0.9739 - val_loss: 0.2801 - val_acc: 0.8920 - val_auc: 0.9552 - 16s/epoch - 56ms/step
Early stopping epoch: 22
******Evaluating TEST set*********
32/32 - 1s - 1s/epoch - 42ms/step
              precision    recall  f1-score   support

           0       0.86      0.83      0.85       391
           1       0.90      0.92      0.91       609

    accuracy                           0.88      1000
   macro avg       0.88      0.88      0.88      1000
weighted avg       0.88      0.88      0.88      1000

******Evaluating RANDOM Model*********
              precision    recall  f1-score   support

           0       0.38      0.49      0.43       391
           1       0.60      0.50      0.55       609

    accuracy                           0.49      1000
   macro avg       0.49      0.49      0.49      1000
weighted avg       0.52      0.49      0.50      1000

______________________________________________________
Model: "model_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 150, 6, 3)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 146, 1, 64)        5824      
                                                                 
 lambda_9 (Lambda)           (None, 146, 64)           0         
                                                                 
 lstm (LSTM)                 [(None, 146, 64),         33024     
                              (None, 64),                        
                              (None, 64)]                        
                                                                 
 self_attention_9 (SelfAtten  ((None, 1024),           2560      
 tion)                        (None, 16, 146))                   
                                                                 
 dense_9 (Dense)             (None, 2)                 2050      
                                                                 
=================================================================
Total params: 43,458
Trainable params: 43,458
Non-trainable params: 0
_________________________________________________________________
None
Mean Accuracy[0.9035] IC [0.8959, 0.9112]
Mean Recall[0.8944] IC [0.8853, 0.9034]
Mean F1[0.8977] IC [0.8894, 0.9060]
Median Accuracy[0.9035]
Median Recall[0.8912]
Median F1[0.8970]
********************txid272634********************
0 non-operons were not labeled and 0 operons were not labeled 

Classification report
              precision    recall  f1-score   support

           0       0.90      0.56      0.69       126
           1       0.81      0.97      0.88       246

    accuracy                           0.83       372
   macro avg       0.85      0.76      0.78       372
weighted avg       0.84      0.83      0.82       372

Predicted  0.0  1.0  All
True                    
0           70   56  126
1            8  238  246
All         78  294  372
**************************************************
